{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym.spaces.discrete import Discrete\n",
    "from gym_sokoban.envs import SokobanEnv\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import CnnPolicy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SokobanEnvFixated(SokobanEnv): \n",
    "    def __init__(self):\n",
    "        SokobanEnv.__init__(self,\n",
    "                            dim_room=(10, 10), \n",
    "                            max_steps=100, \n",
    "                            num_boxes=4, \n",
    "                            num_gen_steps=None, \n",
    "                            reset=False)\n",
    "        self.action_space = Discrete(5) # limit to push actions\n",
    "\n",
    "    def reset(self, second_player=False, render_mode='rgb_array'):\n",
    "\n",
    "        self.room_fixed = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "                                    [0, 1, 1, 1, 1, 2, 1, 1, 1, 0],\n",
    "                                    [0, 1, 1, 1, 2, 1, 1, 1, 1, 0],\n",
    "                                    [0, 1, 1, 1, 0, 1, 1, 2, 1, 0],\n",
    "                                    [0, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "        self.room_state = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "                                    [0, 1, 1, 4, 5, 2, 1, 1, 1, 0],\n",
    "                                    [0, 1, 1, 1, 2, 4, 1, 4, 1, 0],\n",
    "                                    [0, 1, 1, 1, 0, 1, 1, 2, 1, 0],\n",
    "                                    [0, 2, 4, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "        self.box_mapping = {(6, 7): (5, 7), (5, 4): (5, 5), (4, 5): (4, 3), (7, 1): (7, 2)}\n",
    "\n",
    "        self.player_position = np.argwhere(self.room_state == 5)[0]\n",
    "        self.num_env_steps = 0\n",
    "        self.reward_last = 0\n",
    "        self.boxes_on_target = 0\n",
    "\n",
    "        starting_observation = self.render(render_mode)\n",
    "        return starting_observation # Close environment after testing\n",
    "\n",
    "def render_state(env, mode=\"rgb_array\"):\n",
    "    \"\"\"Renders the Sokoban environment as image and displays it.\"\"\"\n",
    "    image = env.render(mode)\n",
    "\n",
    "    plt.figure(dpi=200) \n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./tensorboard_logs/log_sokoban_ppo_fixated_env_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roger\\Documents\\Projects\\vm-code-generation-with-knowledge-graph\\venv\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x000001A6B47F8450> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x000001A6B0ED4310>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -9.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -10         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009512777 |\n",
      "|    clip_fraction        | 0.048       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.0753     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.336      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00946    |\n",
      "|    value_loss           | 0.469       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -9.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 33       |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 122      |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -10         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016916178 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.319      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    value_loss           | 0.0461      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -9.74    |\n",
      "| time/              |          |\n",
      "|    fps             | 31       |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 196      |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -9          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021363974 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.37       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0486     |\n",
      "|    value_loss           | 0.0665      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -9       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -9       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -9       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -9.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024604214 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.333      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 0.0767      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -9.53    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 345      |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02370967 |\n",
      "|    clip_fraction        | 0.277      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.53      |\n",
      "|    explained_variance   | 0.126      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.325     |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0484    |\n",
      "|    value_loss           | 0.083      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -9.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 419      |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026671866 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.345      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -9.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 493      |\n",
      "|    total_timesteps | 14336    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025103157 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.321      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    value_loss           | 0.101       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 567      |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022734199 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.304      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0486     |\n",
      "|    value_loss           | 0.09        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.63    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 641      |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022208408 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.317      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    value_loss           | 0.0961      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 715      |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025266705 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.268      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    value_loss           | 0.0844      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 795      |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026924975 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.327      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 0.0768      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 869      |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025983013 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.318      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0506     |\n",
      "|    value_loss           | 0.0761      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.23    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 943      |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024952307 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.308      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    value_loss           | 0.0786      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.19    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 1017     |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 29000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02672909 |\n",
      "|    clip_fraction        | 0.313      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.46      |\n",
      "|    explained_variance   | 0.484      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.324     |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0475    |\n",
      "|    value_loss           | 0.0718     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.21    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 1090     |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 100       |\n",
      "|    mean_reward          | -8        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 31000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0254088 |\n",
      "|    clip_fraction        | 0.308     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.46     |\n",
      "|    explained_variance   | 0.361     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.294    |\n",
      "|    n_updates            | 150       |\n",
      "|    policy_gradient_loss | -0.0497   |\n",
      "|    value_loss           | 0.0768    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.19    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 1164     |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024656728 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.314      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0444     |\n",
      "|    value_loss           | 0.0885      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.19    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 1238     |\n",
      "|    total_timesteps | 34816    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026831117 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.358       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.341      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0507     |\n",
      "|    value_loss           | 0.0803      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 1312     |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026500119 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.296      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 0.0858      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 1386     |\n",
      "|    total_timesteps | 38912    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 39000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027884264 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.325      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0557     |\n",
      "|    value_loss           | 0.0874      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 1460     |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 41000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027750712 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.338      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 0.0937      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 1540     |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 43500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029384498 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.366      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0666     |\n",
      "|    value_loss           | 0.0659      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 1614     |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023156205 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.348       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.332      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 0.0878      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 1688     |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 47500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025981804 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.301       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.289      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 0.0981      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 1762     |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 49500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024894174 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.395       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.297      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0422     |\n",
      "|    value_loss           | 0.0884      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.18    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 1836     |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 51500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024822008 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.477       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.31       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0453     |\n",
      "|    value_loss           | 0.0816      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 1910     |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 53500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027729753 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.432       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.31       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    value_loss           | 0.0829      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 1984     |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024661407 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.335       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.327      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 0.085       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.17    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 2058     |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 57500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025182558 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.314      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0573     |\n",
      "|    value_loss           | 0.0871      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 2132     |\n",
      "|    total_timesteps | 59392    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 59500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022094136 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.297       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.293      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 0.0938      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 2206     |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 61500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024566937 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.284      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0479     |\n",
      "|    value_loss           | 0.0888      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 2280     |\n",
      "|    total_timesteps | 63488    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 63500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027072888 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.238       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.32       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0602     |\n",
      "|    value_loss           | 0.0644      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 2359     |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027160848 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.307      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 0.0809      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 2431     |\n",
      "|    total_timesteps | 67584    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 68000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032466397 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.308      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    value_loss           | 0.0796      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 2503     |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029034102 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.321      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0491     |\n",
      "|    value_loss           | 0.0915      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 2576     |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029228168 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.252       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.324      |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 0.0832      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 2648     |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 74000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02840741 |\n",
      "|    clip_fraction        | 0.361      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.48      |\n",
      "|    explained_variance   | 0.291      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.322     |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0616    |\n",
      "|    value_loss           | 0.0966     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 2722     |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 76000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025023472 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.343      |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 0.0991      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 2796     |\n",
      "|    total_timesteps | 77824    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 78000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02671174 |\n",
      "|    clip_fraction        | 0.329      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.47      |\n",
      "|    explained_variance   | 0.338      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.295     |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0593    |\n",
      "|    value_loss           | 0.0879     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 2870     |\n",
      "|    total_timesteps | 79872    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027593281 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.346      |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 0.088       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=80500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.35    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 2944     |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 82000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028120734 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.337      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 0.0915      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=82500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 82500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 3018     |\n",
      "|    total_timesteps | 83968    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 84000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023968069 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.32       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0578     |\n",
      "|    value_loss           | 0.0802      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=84500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 3098     |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 86500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02715229 |\n",
      "|    clip_fraction        | 0.371      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.49      |\n",
      "|    explained_variance   | 0.23       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.333     |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0639    |\n",
      "|    value_loss           | 0.0812     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 87000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 87500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.45    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 3172     |\n",
      "|    total_timesteps | 88064    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 88500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026180023 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.353      |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 0.0865      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 89000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 89500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 3246     |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026112486 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.388       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.316      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0562     |\n",
      "|    value_loss           | 0.0784      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 91000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 91500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 3320     |\n",
      "|    total_timesteps | 92160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 92500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026096955 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.358      |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0594     |\n",
      "|    value_loss           | 0.0821      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 93000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 93500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 3394     |\n",
      "|    total_timesteps | 94208    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 94500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021490462 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.331      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0422     |\n",
      "|    value_loss           | 0.0878      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 95000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 95500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 3468     |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 96500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025444932 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.327      |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    value_loss           | 0.0887      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 97000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 97500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.19    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 3542     |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 98500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02459275 |\n",
      "|    clip_fraction        | 0.298      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.45      |\n",
      "|    explained_variance   | 0.455      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.35      |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0489    |\n",
      "|    value_loss           | 0.0806     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 99000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=99500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 99500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 3615     |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025608689 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.317      |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.052      |\n",
      "|    value_loss           | 0.0881      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 101000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=101500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 101500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.11    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 3689     |\n",
      "|    total_timesteps | 102400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 102500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024697328 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.262      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0466     |\n",
      "|    value_loss           | 0.106       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=103000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 103000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=103500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 103500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 3764     |\n",
      "|    total_timesteps | 104448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 104500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025668366 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.392       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.298      |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0569     |\n",
      "|    value_loss           | 0.0816      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 105000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 105500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.99    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 3838     |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 106500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024668284 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.314       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.345      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 0.0863      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 107000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 107500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.99    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 3918     |\n",
      "|    total_timesteps | 108544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 109000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027169626 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.391       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.333      |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    value_loss           | 0.0804      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=109500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 109500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 3991     |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 111000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021981828 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.282      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    value_loss           | 0.0815      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=111500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 111500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.87    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 4065     |\n",
      "|    total_timesteps | 112640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 113000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026427966 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.325      |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 0.0973      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=113500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 113500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.99    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 4140     |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 115000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024462357 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.329       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.322      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0602     |\n",
      "|    value_loss           | 0.0744      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=115500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 115500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 4214     |\n",
      "|    total_timesteps | 116736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 117000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025122756 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.306       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.368      |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 0.0869      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=117500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 117500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=118500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.94    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 4288     |\n",
      "|    total_timesteps | 118784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 119000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024626464 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.347      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0478     |\n",
      "|    value_loss           | 0.0946      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=119500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 119500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.95    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 4362     |\n",
      "|    total_timesteps | 120832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 121000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02730656 |\n",
      "|    clip_fraction        | 0.333      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.407      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.274     |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0586    |\n",
      "|    value_loss           | 0.0817     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=121500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 121500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.91    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 4436     |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 123000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027997667 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.311      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 0.0877      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=123500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 123500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 4511     |\n",
      "|    total_timesteps | 124928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 125000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02468308 |\n",
      "|    clip_fraction        | 0.318      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.45      |\n",
      "|    explained_variance   | 0.383      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.3       |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.0471    |\n",
      "|    value_loss           | 0.102      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=125500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 125500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=126500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.78    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 4585     |\n",
      "|    total_timesteps | 126976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 127000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025376078 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.312      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0563     |\n",
      "|    value_loss           | 0.0817      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=127500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 127500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 129000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.84    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 63       |\n",
      "|    time_elapsed    | 4664     |\n",
      "|    total_timesteps | 129024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 129500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023406265 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.329      |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 0.0882      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 131000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.87    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 4738     |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=131500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 131500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024693228 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.322      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0517     |\n",
      "|    value_loss           | 0.0863      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 132000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 132500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 133000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 4812     |\n",
      "|    total_timesteps | 133120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=133500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 133500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02505721 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.43      |\n",
      "|    explained_variance   | 0.448      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.301     |\n",
      "|    n_updates            | 650        |\n",
      "|    policy_gradient_loss | -0.0452    |\n",
      "|    value_loss           | 0.0822     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 134000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=134500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 134500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 135000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 4886     |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=135500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 135500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025565634 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.336      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 0.0839      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 136000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 136500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 137000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.68    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 4960     |\n",
      "|    total_timesteps | 137216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=137500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 137500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02648453 |\n",
      "|    clip_fraction        | 0.345      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.46      |\n",
      "|    explained_variance   | 0.372      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.353     |\n",
      "|    n_updates            | 670        |\n",
      "|    policy_gradient_loss | -0.0656    |\n",
      "|    value_loss           | 0.0756     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 138000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=138500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 138500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 139000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.69    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 5033     |\n",
      "|    total_timesteps | 139264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=139500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 139500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025097113 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.28        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.304      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 0.0834      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 141000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 5107     |\n",
      "|    total_timesteps | 141312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 141500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02601816 |\n",
      "|    clip_fraction        | 0.342      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.43      |\n",
      "|    explained_variance   | 0.384      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.326     |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | -0.0573    |\n",
      "|    value_loss           | 0.0688     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 142000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=142500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 142500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 143000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 5181     |\n",
      "|    total_timesteps | 143360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=143500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 143500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023979522 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.38        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.299      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.049      |\n",
      "|    value_loss           | 0.0812      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 144000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 144500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 145000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.82    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 5255     |\n",
      "|    total_timesteps | 145408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=145500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 145500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022924682 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.266      |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0508     |\n",
      "|    value_loss           | 0.0915      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 146000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=146500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 146500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 147000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 5329     |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=147500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 147500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019363526 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.61        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.294      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    value_loss           | 0.0802      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 148000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=148500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 148500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 149000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=149500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 149500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.66    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 73       |\n",
      "|    time_elapsed    | 5408     |\n",
      "|    total_timesteps | 149504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021956725 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.49        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.277      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 0.0768      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=150500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 151000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=151500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 151500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.57    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 5482     |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 152000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022292048 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.504       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.318      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    value_loss           | 0.076       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=152500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 152500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 153000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=153500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 153500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 75       |\n",
      "|    time_elapsed    | 5556     |\n",
      "|    total_timesteps | 153600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 154000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02123551 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | 0.421      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.283     |\n",
      "|    n_updates            | 750        |\n",
      "|    policy_gradient_loss | -0.0458    |\n",
      "|    value_loss           | 0.0751     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=154500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 154500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=155500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.51    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 5630     |\n",
      "|    total_timesteps | 155648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 156000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024285764 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.292      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 0.0665      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=156500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 156500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 157000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=157500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 157500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.49    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 77       |\n",
      "|    time_elapsed    | 5704     |\n",
      "|    total_timesteps | 157696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 158000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021915236 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.276      |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    value_loss           | 0.0871      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=158500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 158500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 159000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=159500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 159500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.49    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 5778     |\n",
      "|    total_timesteps | 159744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024810586 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.369       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.265      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0393     |\n",
      "|    value_loss           | 0.0973      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 160500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 161000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 161500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.53    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 5852     |\n",
      "|    total_timesteps | 161792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 162000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028289966 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.358      |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0493     |\n",
      "|    value_loss           | 0.0854      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=162500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 162500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 163000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=163500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 163500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 5925     |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 164000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026350211 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.259      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=164500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 164500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 165000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=165500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 165500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.69    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 5999     |\n",
      "|    total_timesteps | 165888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 166000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025898576 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.268      |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0617     |\n",
      "|    value_loss           | 0.0927      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=166500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 166500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 167000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=167500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 167500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.82    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 6073     |\n",
      "|    total_timesteps | 167936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 168000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024963155 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.318      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0574     |\n",
      "|    value_loss           | 0.0964      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=168500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 168500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 169000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=169500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 169500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.81    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 83       |\n",
      "|    time_elapsed    | 6147     |\n",
      "|    total_timesteps | 169984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 170000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025117578 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.306      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0567     |\n",
      "|    value_loss           | 0.0874      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=170500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 171000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=171500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 171500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 172000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 6227     |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=172500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 172500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022855757 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.311      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    value_loss           | 0.0894      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 173000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=173500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 173500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 174000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.63    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 85       |\n",
      "|    time_elapsed    | 6301     |\n",
      "|    total_timesteps | 174080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=174500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 174500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024496567 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.302      |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.042      |\n",
      "|    value_loss           | 0.0876      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 175000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=175500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 175500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 176000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.55    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 6375     |\n",
      "|    total_timesteps | 176128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 176500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024530374 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.274      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    value_loss           | 0.0774      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 177000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=177500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 177500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 178000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.45    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 87       |\n",
      "|    time_elapsed    | 6448     |\n",
      "|    total_timesteps | 178176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=178500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 178500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023665402 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.531       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.311      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0482     |\n",
      "|    value_loss           | 0.0777      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 179000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=179500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 179500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 6522     |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023089258 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.304      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.053      |\n",
      "|    value_loss           | 0.0889      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 181000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 181500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 182000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.68    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 89       |\n",
      "|    time_elapsed    | 6597     |\n",
      "|    total_timesteps | 182272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=182500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 182500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026405606 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.328      |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 0.072       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 183000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=183500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 183500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 184000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 6671     |\n",
      "|    total_timesteps | 184320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=184500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 184500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024942629 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.326      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0456     |\n",
      "|    value_loss           | 0.0887      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 185000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=185500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 185500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 186000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 6744     |\n",
      "|    total_timesteps | 186368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=186500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 186500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024442961 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.285      |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 0.0881      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 187000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=187500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 187500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 188000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.69    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 6819     |\n",
      "|    total_timesteps | 188416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=188500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 188500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022694964 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.454       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.326      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0462     |\n",
      "|    value_loss           | 0.0809      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 189000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=189500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 189500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 6893     |\n",
      "|    total_timesteps | 190464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 190500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02498243 |\n",
      "|    clip_fraction        | 0.333      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | 0.506      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.326     |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.0568    |\n",
      "|    value_loss           | 0.086      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 191000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=191500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 191500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 192500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 6972     |\n",
      "|    total_timesteps | 192512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 193000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024172014 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.515       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.275      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0424     |\n",
      "|    value_loss           | 0.081       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=193500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 193500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 194000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=194500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 194500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 95       |\n",
      "|    time_elapsed    | 7047     |\n",
      "|    total_timesteps | 194560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 195000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021988908 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.548       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.315      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    value_loss           | 0.0751      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=195500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 195500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 196000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=196500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 196500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 7120     |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 197000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023221891 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.309      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0471     |\n",
      "|    value_loss           | 0.0793      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=197500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 197500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 198000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=198500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 198500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 97       |\n",
      "|    time_elapsed    | 7194     |\n",
      "|    total_timesteps | 198656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 199000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024430785 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.474       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.346      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0616     |\n",
      "|    value_loss           | 0.0721      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=199500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 199500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.59    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 7268     |\n",
      "|    total_timesteps | 200704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 201000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024055261 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.286      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    value_loss           | 0.0909      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=201500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 201500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=202000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 202000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=202500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 202500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.59    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 99       |\n",
      "|    time_elapsed    | 7342     |\n",
      "|    total_timesteps | 202752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=203000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 203000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020816196 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.566       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.304      |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0493     |\n",
      "|    value_loss           | 0.0713      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=203500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 203500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 204000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=204500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 204500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 7416     |\n",
      "|    total_timesteps | 204800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 205000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025340889 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.466       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.341      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 0.0844      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=205500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 205500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=206000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 206000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=206500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 206500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.66    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 101      |\n",
      "|    time_elapsed    | 7490     |\n",
      "|    total_timesteps | 206848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=207000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 207000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022726636 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.466       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.283      |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 0.0699      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=207500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 207500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 208000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=208500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 208500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 7564     |\n",
      "|    total_timesteps | 208896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=209000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 209000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025087535 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.273      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0517     |\n",
      "|    value_loss           | 0.0847      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=209500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 209500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.68    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 7638     |\n",
      "|    total_timesteps | 210944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=211000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 211000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028354255 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.536       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.278      |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 0.0707      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=211500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 211500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 212000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=212500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 212500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 7711     |\n",
      "|    total_timesteps | 212992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=213000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 213000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026517864 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.333      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 0.0794      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=213500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 213500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=214000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 214000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=214500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 214500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 215000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 105      |\n",
      "|    time_elapsed    | 7792     |\n",
      "|    total_timesteps | 215040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=215500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 215500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02665504 |\n",
      "|    clip_fraction        | 0.364      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.44      |\n",
      "|    explained_variance   | 0.356      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.317     |\n",
      "|    n_updates            | 1050       |\n",
      "|    policy_gradient_loss | -0.0641    |\n",
      "|    value_loss           | 0.0874     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 216000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=216500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 216500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=217000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 217000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.79    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 7865     |\n",
      "|    total_timesteps | 217088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=217500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 217500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028606424 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.395       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.341      |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0648     |\n",
      "|    value_loss           | 0.0759      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=218000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 218000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=218500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 218500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=219000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 219000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 107      |\n",
      "|    time_elapsed    | 7939     |\n",
      "|    total_timesteps | 219136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=219500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 219500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02479642 |\n",
      "|    clip_fraction        | 0.321      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.535      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.309     |\n",
      "|    n_updates            | 1070       |\n",
      "|    policy_gradient_loss | -0.0547    |\n",
      "|    value_loss           | 0.0923     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 221000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 8013     |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 221500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029262273 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.309      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0487     |\n",
      "|    value_loss           | 0.0938      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=222000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 222000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=222500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 222500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=223000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 223000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 109      |\n",
      "|    time_elapsed    | 8087     |\n",
      "|    total_timesteps | 223232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=223500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 223500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023180265 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.614       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.294      |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 0.0672      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 224000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 224500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 225000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.82    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 8162     |\n",
      "|    total_timesteps | 225280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=225500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 225500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02824818 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.46      |\n",
      "|    explained_variance   | 0.471      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.333     |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.0592    |\n",
      "|    value_loss           | 0.0826     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=226000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 226000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=226500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 226500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=227000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 227000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.91    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 111      |\n",
      "|    time_elapsed    | 8236     |\n",
      "|    total_timesteps | 227328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=227500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 227500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024337023 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.302      |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.0613     |\n",
      "|    value_loss           | 0.0836      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 228000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=228500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 228500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=229000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 229000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.97    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 8310     |\n",
      "|    total_timesteps | 229376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=229500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 229500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02595625 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.45      |\n",
      "|    explained_variance   | 0.384      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.314     |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0635    |\n",
      "|    value_loss           | 0.0771     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 230000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=230500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 230500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=231000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 231000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 8384     |\n",
      "|    total_timesteps | 231424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=231500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 231500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023627693 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.305       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.323      |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | -0.0563     |\n",
      "|    value_loss           | 0.0998      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 232000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=232500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 232500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=233000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 233000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.88    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 8457     |\n",
      "|    total_timesteps | 233472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=233500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 233500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02220643 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.378      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.302     |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.042     |\n",
      "|    value_loss           | 0.0882     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=234000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 234000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=234500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 234500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 235000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=235500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 235500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 115      |\n",
      "|    time_elapsed    | 8535     |\n",
      "|    total_timesteps | 235520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 236000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021842526 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.335      |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 0.0654      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=236500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 236500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=237000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 237000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=237500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 237500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 8608     |\n",
      "|    total_timesteps | 237568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=238000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 238000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021108884 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.335      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0566     |\n",
      "|    value_loss           | 0.0703      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=238500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 238500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=239000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 239000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=239500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 239500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.64    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 117      |\n",
      "|    time_elapsed    | 8680     |\n",
      "|    total_timesteps | 239616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024570085 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.492       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.325      |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 0.0677      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=240500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 240500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 241000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 241500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 8752     |\n",
      "|    total_timesteps | 241664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=242000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 242000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021203892 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.418       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.284      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0362     |\n",
      "|    value_loss           | 0.093       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=242500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 242500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=243000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 243000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=243500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 243500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 119      |\n",
      "|    time_elapsed    | 8824     |\n",
      "|    total_timesteps | 243712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 244000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024823211 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.336      |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | -0.0635     |\n",
      "|    value_loss           | 0.0814      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=244500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 244500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 245000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=245500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 245500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 8896     |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=246000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 246000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024801716 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.558       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.289      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0487     |\n",
      "|    value_loss           | 0.0864      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=246500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 246500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=247000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 247000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=247500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 247500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.74    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 121      |\n",
      "|    time_elapsed    | 8968     |\n",
      "|    total_timesteps | 247808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 248000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023464298 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.302      |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 0.0762      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=248500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 248500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=249000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 249000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=249500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 249500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.83    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 9040     |\n",
      "|    total_timesteps | 249856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 250000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02097278 |\n",
      "|    clip_fraction        | 0.298      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.46      |\n",
      "|    explained_variance   | 0.499      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.365     |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.059     |\n",
      "|    value_loss           | 0.0784     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=250500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 250500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=251000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 251000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=251500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 251500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.94    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 9112     |\n",
      "|    total_timesteps | 251904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 252000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025783509 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.474       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.329      |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 0.0763      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=252500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 252500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=253000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 253000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=253500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 253500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.87    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 9184     |\n",
      "|    total_timesteps | 253952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=254000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 254000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02538928 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.44      |\n",
      "|    explained_variance   | 0.273      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.326     |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0562    |\n",
      "|    value_loss           | 0.0827     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=254500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 254500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 255000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=255500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 255500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.89    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 125      |\n",
      "|    time_elapsed    | 9262     |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 256500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025850814 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.275      |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.0579     |\n",
      "|    value_loss           | 0.0808      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=257000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 257000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=257500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 257500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=258000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 258000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.82    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 9334     |\n",
      "|    total_timesteps | 258048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=258500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 100       |\n",
      "|    mean_reward          | -7        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 258500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0220625 |\n",
      "|    clip_fraction        | 0.284     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.42     |\n",
      "|    explained_variance   | 0.405     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.329    |\n",
      "|    n_updates            | 1260      |\n",
      "|    policy_gradient_loss | -0.0459   |\n",
      "|    value_loss           | 0.0815    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=259000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 259000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=259500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 259500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 260000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.74    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 9406     |\n",
      "|    total_timesteps | 260096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025607629 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.227       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.337      |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 0.0825      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=261000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 261000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=261500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 261500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 262000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.59    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 9478     |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 262500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024260234 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.533       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.34       |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0479     |\n",
      "|    value_loss           | 0.0724      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=263000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 263000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=263500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 263500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 264000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.63    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 129      |\n",
      "|    time_elapsed    | 9550     |\n",
      "|    total_timesteps | 264192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=264500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 264500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023675857 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.302      |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    value_loss           | 0.0831      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 265000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=265500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 265500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=266000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 266000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.56    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 9622     |\n",
      "|    total_timesteps | 266240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=266500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 266500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023435365 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.331      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0473     |\n",
      "|    value_loss           | 0.0914      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=267000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 267000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=267500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 267500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 268000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 131      |\n",
      "|    time_elapsed    | 9694     |\n",
      "|    total_timesteps | 268288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=268500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 268500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021118805 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.582       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.327      |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    value_loss           | 0.0635      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=269000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 269000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=269500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 269500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.45    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 9766     |\n",
      "|    total_timesteps | 270336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 270500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02202776 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.43      |\n",
      "|    explained_variance   | 0.584      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.287     |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0359    |\n",
      "|    value_loss           | 0.0825     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=271000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 271000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=271500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 271500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 272000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.47    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 133      |\n",
      "|    time_elapsed    | 9838     |\n",
      "|    total_timesteps | 272384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 272500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022615261 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.338      |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 0.0775      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=273000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 273000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=273500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 273500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=274000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 274000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.49    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 9910     |\n",
      "|    total_timesteps | 274432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=274500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 274500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02487094 |\n",
      "|    clip_fraction        | 0.308      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.46      |\n",
      "|    explained_variance   | 0.478      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.298     |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.0507    |\n",
      "|    value_loss           | 0.0783     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 275000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=275500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 275500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 276000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 135      |\n",
      "|    time_elapsed    | 9982     |\n",
      "|    total_timesteps | 276480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=276500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 276500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02555893 |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.48      |\n",
      "|    explained_variance   | 0.376      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.351     |\n",
      "|    n_updates            | 1350       |\n",
      "|    policy_gradient_loss | -0.0589    |\n",
      "|    value_loss           | 0.0821     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=277000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 277000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=277500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 277500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=278000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 278000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=278500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 278500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 10060    |\n",
      "|    total_timesteps | 278528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=279000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 279000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02385027 |\n",
      "|    clip_fraction        | 0.342      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.47      |\n",
      "|    explained_variance   | 0.282      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.33      |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0573    |\n",
      "|    value_loss           | 0.1        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=279500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 279500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.82    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 10132    |\n",
      "|    total_timesteps | 280576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=281000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 281000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022248361 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.315      |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 0.0886      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=281500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 281500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 282000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 282500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.78    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 10204    |\n",
      "|    total_timesteps | 282624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=283000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 283000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024684088 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.47        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.292      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 0.081       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=283500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 283500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 284000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=284500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 284500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 139      |\n",
      "|    time_elapsed    | 10276    |\n",
      "|    total_timesteps | 284672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 285000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02548237 |\n",
      "|    clip_fraction        | 0.32       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.44      |\n",
      "|    explained_variance   | 0.431      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.329     |\n",
      "|    n_updates            | 1390       |\n",
      "|    policy_gradient_loss | -0.0537    |\n",
      "|    value_loss           | 0.0806     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=285500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 285500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=286000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 286000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=286500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 286500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.64    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 10348    |\n",
      "|    total_timesteps | 286720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=287000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 287000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02733502 |\n",
      "|    clip_fraction        | 0.315      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.43      |\n",
      "|    explained_variance   | 0.452      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.293     |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0611    |\n",
      "|    value_loss           | 0.0734     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=287500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 287500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 288000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 288500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.57    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 141      |\n",
      "|    time_elapsed    | 10420    |\n",
      "|    total_timesteps | 288768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=289000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 289000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02361273 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.44      |\n",
      "|    explained_variance   | 0.486      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.314     |\n",
      "|    n_updates            | 1410       |\n",
      "|    policy_gradient_loss | -0.0515    |\n",
      "|    value_loss           | 0.0707     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=289500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 289500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.66    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 10492    |\n",
      "|    total_timesteps | 290816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=291000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 291000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024992973 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.334      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 0.0852      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=291500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 291500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 292000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=292500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 292500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 143      |\n",
      "|    time_elapsed    | 10564    |\n",
      "|    total_timesteps | 292864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=293000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 293000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028676838 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.352       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.358      |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 0.0799      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=293500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 293500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=294000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 294000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=294500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 294500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 10636    |\n",
      "|    total_timesteps | 294912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 295000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023074832 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.297      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.06       |\n",
      "|    value_loss           | 0.075       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=295500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 295500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 296000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=296500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 296500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 145      |\n",
      "|    time_elapsed    | 10708    |\n",
      "|    total_timesteps | 296960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=297000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 297000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019796975 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.441       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.32       |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    value_loss           | 0.088       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=297500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 297500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=298000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 298000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=298500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 298500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=299000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 299000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 10786    |\n",
      "|    total_timesteps | 299008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=299500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 299500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020475581 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.494       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.241      |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    value_loss           | 0.0895      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=301000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 301000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 10858    |\n",
      "|    total_timesteps | 301056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=301500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 301500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024904765 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.472       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.295      |\n",
      "|    n_updates            | 1470        |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    value_loss           | 0.0863      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=302000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 302000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 302500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=303000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 303000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 10930    |\n",
      "|    total_timesteps | 303104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=303500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 303500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022957642 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.482       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.31       |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.045      |\n",
      "|    value_loss           | 0.0893      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 304000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=304500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 304500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 305000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.59    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 149      |\n",
      "|    time_elapsed    | 11002    |\n",
      "|    total_timesteps | 305152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=305500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 305500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020939454 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.573       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.333      |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | -0.0491     |\n",
      "|    value_loss           | 0.0627      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=306000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 306000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=306500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 306500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=307000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 307000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 11074    |\n",
      "|    total_timesteps | 307200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=307500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 307500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027216244 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.31       |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0666     |\n",
      "|    value_loss           | 0.0865      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 308000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=308500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 308500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=309000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 309000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 151      |\n",
      "|    time_elapsed    | 11147    |\n",
      "|    total_timesteps | 309248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=309500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 309500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025876025 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.383      |\n",
      "|    n_updates            | 1510        |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 0.0762      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=311000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 311000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 11219    |\n",
      "|    total_timesteps | 311296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=311500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 311500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025323719 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.325      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 0.0752      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 312000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=312500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 312500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=313000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 313000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.83    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 153      |\n",
      "|    time_elapsed    | 11291    |\n",
      "|    total_timesteps | 313344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=313500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 313500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025091413 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.339      |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.0604     |\n",
      "|    value_loss           | 0.0702      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=314000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 314000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=314500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 314500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 315000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 11363    |\n",
      "|    total_timesteps | 315392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=315500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 315500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023460494 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.442       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.374      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 0.0604      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 316000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=316500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 316500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=317000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 317000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.85    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 155      |\n",
      "|    time_elapsed    | 11435    |\n",
      "|    total_timesteps | 317440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=317500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 317500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024625946 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.298      |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | -0.0565     |\n",
      "|    value_loss           | 0.0963      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=318000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 318000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=318500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 318500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=319000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 319000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.78    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 11507    |\n",
      "|    total_timesteps | 319488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=319500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 319500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027130459 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.325      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0651     |\n",
      "|    value_loss           | 0.0704      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 320000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 320500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=321000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 321000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=321500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 321500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 11585    |\n",
      "|    total_timesteps | 321536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 322000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025478572 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.559       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.349      |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    value_loss           | 0.0704      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=322500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 322500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=323000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 323000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=323500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 323500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 11657    |\n",
      "|    total_timesteps | 323584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 324000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02501081 |\n",
      "|    clip_fraction        | 0.298      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.46      |\n",
      "|    explained_variance   | 0.377      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.291     |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0478    |\n",
      "|    value_loss           | 0.0959     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=324500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 324500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=325000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 325000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=325500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 325500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 159      |\n",
      "|    time_elapsed    | 11729    |\n",
      "|    total_timesteps | 325632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=326000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 326000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026830543 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.343       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.342      |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.0641     |\n",
      "|    value_loss           | 0.0809      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=326500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 326500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=327000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 327000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=327500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 327500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 11802    |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 328000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021884594 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.384       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.322      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 0.0804      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=328500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 328500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=329000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 329000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=329500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 329500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.68    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 161      |\n",
      "|    time_elapsed    | 11874    |\n",
      "|    total_timesteps | 329728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 330000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023759373 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.399       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.354      |\n",
      "|    n_updates            | 1610        |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 0.0812      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=330500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 330500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=331000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 331000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=331500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 331500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 11946    |\n",
      "|    total_timesteps | 331776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 332000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024677204 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.308      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0589     |\n",
      "|    value_loss           | 0.0947      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=332500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 332500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=333000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 333000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=333500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 333500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.74    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 163      |\n",
      "|    time_elapsed    | 12018    |\n",
      "|    total_timesteps | 333824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=334000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 334000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02501684 |\n",
      "|    clip_fraction        | 0.3        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.45      |\n",
      "|    explained_variance   | 0.471      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.331     |\n",
      "|    n_updates            | 1630       |\n",
      "|    policy_gradient_loss | -0.0536    |\n",
      "|    value_loss           | 0.078      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=334500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 334500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=335000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 335000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=335500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 335500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.69    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 12091    |\n",
      "|    total_timesteps | 335872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 336000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022991832 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.365      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 0.0699      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=336500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 336500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=337000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 337000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=337500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 337500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 165      |\n",
      "|    time_elapsed    | 12163    |\n",
      "|    total_timesteps | 337920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=338000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 338000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024115887 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.334      |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 0.0965      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=338500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 338500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=339000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 339000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=339500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 339500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 12235    |\n",
      "|    total_timesteps | 339968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 340000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02150479 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.468      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.352     |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0571    |\n",
      "|    value_loss           | 0.0665     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=340500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 340500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=341000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 341000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=341500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 341500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 342000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 12314    |\n",
      "|    total_timesteps | 342016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 342500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024788106 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.384      |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | -0.067      |\n",
      "|    value_loss           | 0.0779      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=343000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 343000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=343500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 343500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 344000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 12386    |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=344500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 344500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025781028 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.317       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.257      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0485     |\n",
      "|    value_loss           | 0.0732      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 345000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=345500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 345500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=346000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 346000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 169      |\n",
      "|    time_elapsed    | 12458    |\n",
      "|    total_timesteps | 346112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=346500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 346500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023473928 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.328      |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | -0.0506     |\n",
      "|    value_loss           | 0.0711      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=347000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 347000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=347500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 347500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 348000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 12530    |\n",
      "|    total_timesteps | 348160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=348500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 348500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022817712 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.325      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 0.0628      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=349000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 349000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=349500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 349500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.74    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 171      |\n",
      "|    time_elapsed    | 12602    |\n",
      "|    total_timesteps | 350208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 350500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024839351 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.329      |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 0.0834      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=351000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 351000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=351500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 351500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 352000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 12675    |\n",
      "|    total_timesteps | 352256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 352500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02455153 |\n",
      "|    clip_fraction        | 0.278      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.437      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.3       |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0457    |\n",
      "|    value_loss           | 0.07       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=353000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 353000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=353500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 353500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=354000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 354000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 173      |\n",
      "|    time_elapsed    | 12749    |\n",
      "|    total_timesteps | 354304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=354500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 354500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025774743 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.339      |\n",
      "|    n_updates            | 1730        |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 0.0843      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=355000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 355000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=355500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 355500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 356000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 12821    |\n",
      "|    total_timesteps | 356352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=356500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 356500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024076605 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.323      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 0.079       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=357000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 357000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=357500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 357500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=358000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 358000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 175      |\n",
      "|    time_elapsed    | 12893    |\n",
      "|    total_timesteps | 358400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=358500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 358500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023511453 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.331      |\n",
      "|    n_updates            | 1750        |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 0.0712      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=359000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 359000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=359500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 359500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.64    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 12965    |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 360500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021152735 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.343      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    value_loss           | 0.0804      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=361000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 361000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=361500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 361500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 362000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.61    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 177      |\n",
      "|    time_elapsed    | 13037    |\n",
      "|    total_timesteps | 362496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 362500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021436404 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.529       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.312      |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    value_loss           | 0.0927      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=363000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 363000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=363500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 363500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 364000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=364500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 364500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.49    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 13115    |\n",
      "|    total_timesteps | 364544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=365000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 365000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021017443 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.294      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    value_loss           | 0.0864      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=365500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 365500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=366000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 366000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=366500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 366500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.53    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 179      |\n",
      "|    time_elapsed    | 13188    |\n",
      "|    total_timesteps | 366592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=367000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 367000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026614536 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.31       |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 0.0845      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=367500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 367500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 368000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 368500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.9     |\n",
      "|    ep_rew_mean     | -7.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 13260    |\n",
      "|    total_timesteps | 368640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=369000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 369000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023321457 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.249       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.28        |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    value_loss           | 0.187       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=369500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 369500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.9     |\n",
      "|    ep_rew_mean     | -7.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 181      |\n",
      "|    time_elapsed    | 13332    |\n",
      "|    total_timesteps | 370688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=371000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 100      |\n",
      "|    mean_reward          | -7       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 371000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.021192 |\n",
      "|    clip_fraction        | 0.279    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.46    |\n",
      "|    explained_variance   | 0.11     |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | -0.3     |\n",
      "|    n_updates            | 1810     |\n",
      "|    policy_gradient_loss | -0.0459  |\n",
      "|    value_loss           | 0.0873   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=371500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 371500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 372000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=372500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 372500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.9     |\n",
      "|    ep_rew_mean     | -7.56    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 13404    |\n",
      "|    total_timesteps | 372736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=373000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 373000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022448204 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.312       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.288      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 0.0835      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=373500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 373500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=374000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 374000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=374500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 374500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.9     |\n",
      "|    ep_rew_mean     | -7.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 183      |\n",
      "|    time_elapsed    | 13476    |\n",
      "|    total_timesteps | 374784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 375000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022370597 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.328      |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 0.0717      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=375500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 375500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 376000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=376500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 376500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 13549    |\n",
      "|    total_timesteps | 376832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=377000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 377000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020951696 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.395       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.309      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 0.0815      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=377500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 377500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=378000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 378000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=378500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 378500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -7.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 185      |\n",
      "|    time_elapsed    | 13621    |\n",
      "|    total_timesteps | 378880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=379000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 379000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026947862 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.301      |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 0.0711      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=379500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 379500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=380500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.9     |\n",
      "|    ep_rew_mean     | -7.49    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 13695    |\n",
      "|    total_timesteps | 380928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=381000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 381000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027912738 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.34        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.311      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0475     |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=381500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 381500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=382000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 382000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=382500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 382500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.7     |\n",
      "|    ep_rew_mean     | -7.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 187      |\n",
      "|    time_elapsed    | 13769    |\n",
      "|    total_timesteps | 382976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 383000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030492049 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.287      |\n",
      "|    n_updates            | 1870        |\n",
      "|    policy_gradient_loss | -0.0475     |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=383500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 383500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 384000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 384500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=385000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 385000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.6     |\n",
      "|    ep_rew_mean     | -7.23    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 13849    |\n",
      "|    total_timesteps | 385024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=385500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 385500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019390179 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.123       |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0359     |\n",
      "|    value_loss           | 0.274       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=386000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 386000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=386500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 386500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=387000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 387000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.3     |\n",
      "|    ep_rew_mean     | -7.12    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 189      |\n",
      "|    time_elapsed    | 13924    |\n",
      "|    total_timesteps | 387072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=387500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 387500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022487136 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.338      |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | -0.0485     |\n",
      "|    value_loss           | 0.242       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=388000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 388000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=388500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 388500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=389000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 389000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.2     |\n",
      "|    ep_rew_mean     | -6.99    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 13982    |\n",
      "|    total_timesteps | 389120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=389500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 389500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024098635 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.307      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=390500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=391000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 391000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99       |\n",
      "|    ep_rew_mean     | -6.95    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 191      |\n",
      "|    time_elapsed    | 14041    |\n",
      "|    total_timesteps | 391168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=391500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 391500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023319677 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.26       |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | -0.0454     |\n",
      "|    value_loss           | 0.304       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 392000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=392500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 392500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=393000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 393000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.7     |\n",
      "|    ep_rew_mean     | -6.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 14115    |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=393500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 393500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018531919 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.277      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    value_loss           | 0.393       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=394000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 394000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=394500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 394500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=395000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 395000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.2     |\n",
      "|    ep_rew_mean     | -6.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 193      |\n",
      "|    time_elapsed    | 14189    |\n",
      "|    total_timesteps | 395264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=395500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 395500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021099202 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.233      |\n",
      "|    n_updates            | 1930        |\n",
      "|    policy_gradient_loss | -0.0365     |\n",
      "|    value_loss           | 0.497       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 396000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=396500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 396500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=397000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 397000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.4     |\n",
      "|    ep_rew_mean     | -5.81    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 14263    |\n",
      "|    total_timesteps | 397312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=397500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 397500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027762575 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0164      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0374     |\n",
      "|    value_loss           | 0.551       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=398000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 398000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=398500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 398500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=399000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 399000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.1     |\n",
      "|    ep_rew_mean     | -5.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 195      |\n",
      "|    time_elapsed    | 14337    |\n",
      "|    total_timesteps | 399360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=399500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 399500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026873432 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.241      |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    value_loss           | 0.351       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=401000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 401000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.5     |\n",
      "|    ep_rew_mean     | -5.11    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 14412    |\n",
      "|    total_timesteps | 401408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=401500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 401500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028031435 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.154      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 0.357       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=402000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 402000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=402500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 402500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 403000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.9     |\n",
      "|    ep_rew_mean     | -4.88    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 197      |\n",
      "|    time_elapsed    | 14470    |\n",
      "|    total_timesteps | 403456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 403500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024419427 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.112      |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | -0.0405     |\n",
      "|    value_loss           | 0.625       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 404000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=404500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 404500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 405000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=405500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 405500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.3     |\n",
      "|    ep_rew_mean     | -3.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 14531    |\n",
      "|    total_timesteps | 405504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=406000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 406000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026241772 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.193       |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0489     |\n",
      "|    value_loss           | 0.459       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=406500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 406500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=407000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 407000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=407500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 407500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.5     |\n",
      "|    ep_rew_mean     | -3.87    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 199      |\n",
      "|    time_elapsed    | 14589    |\n",
      "|    total_timesteps | 407552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 408000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025474563 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.556       |\n",
      "|    n_updates            | 1990        |\n",
      "|    policy_gradient_loss | -0.0486     |\n",
      "|    value_loss           | 0.519       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=408500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 408500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=409000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 409000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=409500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 409500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.9     |\n",
      "|    ep_rew_mean     | -2.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 14648    |\n",
      "|    total_timesteps | 409600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 410000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031808972 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0951     |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.037      |\n",
      "|    value_loss           | 0.696       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=410500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 410500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=411000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 411000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=411500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 411500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84       |\n",
      "|    ep_rew_mean     | -0.751   |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 201      |\n",
      "|    time_elapsed    | 14706    |\n",
      "|    total_timesteps | 411648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 412000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022035703 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.49        |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    value_loss           | 1.43        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=412500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 412500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=413000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 413000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=413500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 413500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.2     |\n",
      "|    ep_rew_mean     | -0.589   |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 14764    |\n",
      "|    total_timesteps | 413696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=414000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 414000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022075053 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.22       |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    value_loss           | 0.644       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=414500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 414500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=415000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 415000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=415500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 415500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 82.7     |\n",
      "|    ep_rew_mean     | 0.254    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 203      |\n",
      "|    time_elapsed    | 14823    |\n",
      "|    total_timesteps | 415744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 416000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023352392 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.119       |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | -0.0372     |\n",
      "|    value_loss           | 0.894       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=416500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 416500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=417000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 417000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=417500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 417500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.1     |\n",
      "|    ep_rew_mean     | -0.151   |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 14882    |\n",
      "|    total_timesteps | 417792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=418000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 418000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02665239 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.32      |\n",
      "|    explained_variance   | 0.898      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.132     |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0496    |\n",
      "|    value_loss           | 0.429      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=418500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 418500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=419000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 419000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=419500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 419500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.8     |\n",
      "|    ep_rew_mean     | -0.911   |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 205      |\n",
      "|    time_elapsed    | 14940    |\n",
      "|    total_timesteps | 419840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 420000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023190018 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.124      |\n",
      "|    n_updates            | 2050        |\n",
      "|    policy_gradient_loss | -0.0363     |\n",
      "|    value_loss           | 0.612       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=420500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 420500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=421000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 421000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=421500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 421500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.5     |\n",
      "|    ep_rew_mean     | -0.823   |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 14999    |\n",
      "|    total_timesteps | 421888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=422000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 422000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02471903 |\n",
      "|    clip_fraction        | 0.317      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.34      |\n",
      "|    explained_variance   | 0.894      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.131     |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.048     |\n",
      "|    value_loss           | 0.479      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=422500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 422500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 423000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 423500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.1     |\n",
      "|    ep_rew_mean     | -1.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 207      |\n",
      "|    time_elapsed    | 15074    |\n",
      "|    total_timesteps | 423936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 424000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028929641 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.245      |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 0.42        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=424500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 424500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=425000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 425000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=425500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 425500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 82.1     |\n",
      "|    ep_rew_mean     | -0.483   |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 15132    |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=426000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 426000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025337182 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.12        |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 0.516       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=426500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 426500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=427000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 427000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=427500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 427500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=428000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 428000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 79.7     |\n",
      "|    ep_rew_mean     | 0.827    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 209      |\n",
      "|    time_elapsed    | 15193    |\n",
      "|    total_timesteps | 428032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=428500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 428500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020181691 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0295      |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 1.05        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=429000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 429000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=429500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 429500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 430000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 79.7     |\n",
      "|    ep_rew_mean     | 0.908    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 15252    |\n",
      "|    total_timesteps | 430080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=430500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 430500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024263825 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.109      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    value_loss           | 0.658       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=431000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 431000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=431500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 431500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 432000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77       |\n",
      "|    ep_rew_mean     | 2.2      |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 211      |\n",
      "|    time_elapsed    | 15327    |\n",
      "|    total_timesteps | 432128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 432500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025592878 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0956     |\n",
      "|    n_updates            | 2110        |\n",
      "|    policy_gradient_loss | -0.038      |\n",
      "|    value_loss           | 0.707       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=433000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 433000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=433500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 433500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=434000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 434000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81.4     |\n",
      "|    ep_rew_mean     | 0.572    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 15386    |\n",
      "|    total_timesteps | 434176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=434500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 434500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027672684 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.24       |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 0.341       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 435000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=435500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 435500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 436000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 85.7     |\n",
      "|    ep_rew_mean     | -1.37    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 213      |\n",
      "|    time_elapsed    | 15460    |\n",
      "|    total_timesteps | 436224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=436500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 436500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025032401 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.259      |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 0.392       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=437000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 437000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=437500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 437500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=438000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 438000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.5     |\n",
      "|    ep_rew_mean     | -1.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 15535    |\n",
      "|    total_timesteps | 438272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=438500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 438500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023898376 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.143      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    value_loss           | 0.596       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=439000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 439000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=439500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 439500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 440000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.5     |\n",
      "|    ep_rew_mean     | -1.92    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 215      |\n",
      "|    time_elapsed    | 15610    |\n",
      "|    total_timesteps | 440320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 440500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020605735 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.196      |\n",
      "|    n_updates            | 2150        |\n",
      "|    policy_gradient_loss | -0.0454     |\n",
      "|    value_loss           | 0.503       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=441000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 441000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=441500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 441500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=442000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 442000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89       |\n",
      "|    ep_rew_mean     | -1.53    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 15668    |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=442500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 442500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024175586 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0243     |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0409     |\n",
      "|    value_loss           | 0.604       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=443000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 443000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 443500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 444000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.8     |\n",
      "|    ep_rew_mean     | 0.412    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 217      |\n",
      "|    time_elapsed    | 15727    |\n",
      "|    total_timesteps | 444416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=444500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 444500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022332553 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.153       |\n",
      "|    n_updates            | 2170        |\n",
      "|    policy_gradient_loss | -0.0457     |\n",
      "|    value_loss           | 0.556       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=445000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 445000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=445500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 445500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=446000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 446000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 85.5     |\n",
      "|    ep_rew_mean     | -0.036   |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 15787    |\n",
      "|    total_timesteps | 446464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=446500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 446500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022355959 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.11       |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0474     |\n",
      "|    value_loss           | 0.561       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=447000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 447000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=447500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 447500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 448000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=448500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 448500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.8     |\n",
      "|    ep_rew_mean     | 0.235    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 219      |\n",
      "|    time_elapsed    | 15848    |\n",
      "|    total_timesteps | 448512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=449000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 449000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023418237 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00479    |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.0351     |\n",
      "|    value_loss           | 0.635       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=449500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 449500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 85.5     |\n",
      "|    ep_rew_mean     | 0.009    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 15907    |\n",
      "|    total_timesteps | 450560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=451000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 451000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018646764 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.102       |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.036      |\n",
      "|    value_loss           | 0.955       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=451500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 451500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=452000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 452000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=452500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 452500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.6     |\n",
      "|    ep_rew_mean     | -1.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 221      |\n",
      "|    time_elapsed    | 15982    |\n",
      "|    total_timesteps | 452608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=453000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 453000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020704053 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00274     |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    value_loss           | 0.695       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=453500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 453500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=454000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 454000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=454500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 454500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.8     |\n",
      "|    ep_rew_mean     | -1.56    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 16041    |\n",
      "|    total_timesteps | 454656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=455000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 455000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020677265 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.196      |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    value_loss           | 0.632       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=455500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 455500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 456000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=456500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 456500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.8     |\n",
      "|    ep_rew_mean     | -1.47    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 223      |\n",
      "|    time_elapsed    | 16099    |\n",
      "|    total_timesteps | 456704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=457000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 457000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023339283 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0492      |\n",
      "|    n_updates            | 2230        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 0.854       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=457500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 457500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=458000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 458000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=458500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 458500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.5     |\n",
      "|    ep_rew_mean     | -1.63    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 16158    |\n",
      "|    total_timesteps | 458752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=459000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 459000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01882142 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.39      |\n",
      "|    explained_variance   | 0.802      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0819    |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0391    |\n",
      "|    value_loss           | 0.809      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=459500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 459500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.1     |\n",
      "|    ep_rew_mean     | -2.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 225      |\n",
      "|    time_elapsed    | 16217    |\n",
      "|    total_timesteps | 460800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=461000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 461000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027610952 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0513     |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | -0.0482     |\n",
      "|    value_loss           | 0.332       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=461500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 461500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=462000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 462000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=462500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 462500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.5     |\n",
      "|    ep_rew_mean     | -2.95    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 16277    |\n",
      "|    total_timesteps | 462848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 463000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026306957 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.179      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 0.229       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=463500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 463500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 464000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=464500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 464500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.3     |\n",
      "|    ep_rew_mean     | -3.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 227      |\n",
      "|    time_elapsed    | 16336    |\n",
      "|    total_timesteps | 464896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=465000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 465000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026443208 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.318      |\n",
      "|    n_updates            | 2270        |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=465500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 465500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=466000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 466000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=466500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 466500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.5     |\n",
      "|    ep_rew_mean     | -4.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 16395    |\n",
      "|    total_timesteps | 466944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=467000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 467000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024385484 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.255      |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0563     |\n",
      "|    value_loss           | 0.211       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=467500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 467500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 468000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=468500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 468500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.8     |\n",
      "|    ep_rew_mean     | -4.59    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 229      |\n",
      "|    time_elapsed    | 16454    |\n",
      "|    total_timesteps | 468992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=469000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 469000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02576613 |\n",
      "|    clip_fraction        | 0.265      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.43      |\n",
      "|    explained_variance   | 0.799      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0704    |\n",
      "|    n_updates            | 2290       |\n",
      "|    policy_gradient_loss | -0.0369    |\n",
      "|    value_loss           | 0.574      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=469500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 469500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=471000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 471000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.8     |\n",
      "|    ep_rew_mean     | -4.64    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 16515    |\n",
      "|    total_timesteps | 471040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=471500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 471500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023426715 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0716      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    value_loss           | 0.532       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=472000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 472000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=472500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 472500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=473000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 473000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.8     |\n",
      "|    ep_rew_mean     | -4.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 231      |\n",
      "|    time_elapsed    | 16590    |\n",
      "|    total_timesteps | 473088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=473500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 473500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022798285 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.105      |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    value_loss           | 0.51        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=474000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 474000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=474500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 474500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=475000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 475000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.6     |\n",
      "|    ep_rew_mean     | -4.09    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 16665    |\n",
      "|    total_timesteps | 475136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=475500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 475500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02630229 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | 0.674      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.124     |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0433    |\n",
      "|    value_loss           | 0.681      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=476000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 476000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=476500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 476500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=477000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 477000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.5     |\n",
      "|    ep_rew_mean     | -4.64    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 233      |\n",
      "|    time_elapsed    | 16739    |\n",
      "|    total_timesteps | 477184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=477500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 477500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022820728 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.262      |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    value_loss           | 0.671       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=478000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 478000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=478500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 478500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=479000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 479000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.9     |\n",
      "|    ep_rew_mean     | -4.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 16814    |\n",
      "|    total_timesteps | 479232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=479500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 479500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02839392 |\n",
      "|    clip_fraction        | 0.373      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.46      |\n",
      "|    explained_variance   | 0.72       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.333     |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0502    |\n",
      "|    value_loss           | 0.155      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 480000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 480500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=481000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 481000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.9     |\n",
      "|    ep_rew_mean     | -5.95    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 235      |\n",
      "|    time_elapsed    | 16889    |\n",
      "|    total_timesteps | 481280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=481500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 481500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027162451 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.29        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.297      |\n",
      "|    n_updates            | 2350        |\n",
      "|    policy_gradient_loss | -0.0549     |\n",
      "|    value_loss           | 0.0975      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=482000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 482000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=482500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 482500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 483000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.4     |\n",
      "|    ep_rew_mean     | -6.37    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 16964    |\n",
      "|    total_timesteps | 483328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 483500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024659034 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.326      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0604     |\n",
      "|    value_loss           | 0.0925      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=484000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 484000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=484500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 484500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=485000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 485000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99       |\n",
      "|    ep_rew_mean     | -7.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 237      |\n",
      "|    time_elapsed    | 17039    |\n",
      "|    total_timesteps | 485376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=485500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 485500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024380308 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.347      |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.057      |\n",
      "|    value_loss           | 0.0874      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=486000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 486000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=486500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 486500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=487000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 487000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.2     |\n",
      "|    ep_rew_mean     | -7.78    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 17114    |\n",
      "|    total_timesteps | 487424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=487500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 487500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024659723 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.226      |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 0.173       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=488000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 488000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=488500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 488500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=489000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 489000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99       |\n",
      "|    ep_rew_mean     | -7.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 239      |\n",
      "|    time_elapsed    | 17189    |\n",
      "|    total_timesteps | 489472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=489500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 489500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024572356 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.334      |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | -0.0477     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 490000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 490500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=491000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 491000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=491500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 491500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97       |\n",
      "|    ep_rew_mean     | -6.49    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 17250    |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 492000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022243274 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.263      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    value_loss           | 0.495       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=492500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 492500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=493000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 493000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=493500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 493500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.1     |\n",
      "|    ep_rew_mean     | -6.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 241      |\n",
      "|    time_elapsed    | 17325    |\n",
      "|    total_timesteps | 493568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=494000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 494000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02361425 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.46      |\n",
      "|    explained_variance   | 0.651      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.29      |\n",
      "|    n_updates            | 2410       |\n",
      "|    policy_gradient_loss | -0.0475    |\n",
      "|    value_loss           | 0.391      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=494500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 494500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=495000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 495000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=495500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 495500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.1     |\n",
      "|    ep_rew_mean     | -4.79    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 17384    |\n",
      "|    total_timesteps | 495616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 496000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016480042 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.644       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.139      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 1.09        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=496500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 496500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=497000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 497000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=497500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 497500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.9     |\n",
      "|    ep_rew_mean     | -4.55    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 243      |\n",
      "|    time_elapsed    | 17459    |\n",
      "|    total_timesteps | 497664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=498000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 498000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020323016 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.634       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.104      |\n",
      "|    n_updates            | 2430        |\n",
      "|    policy_gradient_loss | -0.036      |\n",
      "|    value_loss           | 0.658       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=498500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 498500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=499000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 499000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=499500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 499500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.7     |\n",
      "|    ep_rew_mean     | -4.83    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 17534    |\n",
      "|    total_timesteps | 499712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 500000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02341711 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.45      |\n",
      "|    explained_variance   | 0.692      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.353     |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0475    |\n",
      "|    value_loss           | 0.249      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=500500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=501000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 501000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=501500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 501500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.5     |\n",
      "|    ep_rew_mean     | -5       |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 245      |\n",
      "|    time_elapsed    | 17593    |\n",
      "|    total_timesteps | 501760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=502000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 502000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020170458 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.136      |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    value_loss           | 0.348       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=502500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 502500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=503000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 503000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=503500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 503500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.7     |\n",
      "|    ep_rew_mean     | -5.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 17668    |\n",
      "|    total_timesteps | 503808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019371033 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0433     |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    value_loss           | 0.507       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=504500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 504500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=505000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 505000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=505500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 505500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.8     |\n",
      "|    ep_rew_mean     | -6.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 247      |\n",
      "|    time_elapsed    | 17743    |\n",
      "|    total_timesteps | 505856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=506000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 506000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023176825 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.585       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.223      |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | -0.0412     |\n",
      "|    value_loss           | 0.416       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=506500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 506500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=507000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 507000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=507500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 507500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.2     |\n",
      "|    ep_rew_mean     | -6.55    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 17802    |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=508000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 508000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023589473 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | -0.0152     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.324      |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=508500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 508500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=509000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 509000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=509500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 509500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.2     |\n",
      "|    ep_rew_mean     | -6.69    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 249      |\n",
      "|    time_elapsed    | 17861    |\n",
      "|    total_timesteps | 509952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 510000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02398453 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.47      |\n",
      "|    explained_variance   | 0.436      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.337     |\n",
      "|    n_updates            | 2490       |\n",
      "|    policy_gradient_loss | -0.0568    |\n",
      "|    value_loss           | 0.219      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=510500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 510500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=511000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 511000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=511500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 511500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98       |\n",
      "|    ep_rew_mean     | -6.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 17942    |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 512500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022320418 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00127    |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0348     |\n",
      "|    value_loss           | 0.535       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=513000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 513000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=513500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 513500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=514000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 514000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.1     |\n",
      "|    ep_rew_mean     | -6.53    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 251      |\n",
      "|    time_elapsed    | 18001    |\n",
      "|    total_timesteps | 514048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=514500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 514500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023795156 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.596       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.299      |\n",
      "|    n_updates            | 2510        |\n",
      "|    policy_gradient_loss | -0.0555     |\n",
      "|    value_loss           | 0.201       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=515000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 515000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=515500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 515500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=516000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 516000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.4     |\n",
      "|    ep_rew_mean     | -5.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 18060    |\n",
      "|    total_timesteps | 516096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=516500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 516500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017816953 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.289      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0349     |\n",
      "|    value_loss           | 0.44        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=517000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 517000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=517500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 517500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=518000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 518000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.3     |\n",
      "|    ep_rew_mean     | -6       |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 253      |\n",
      "|    time_elapsed    | 18119    |\n",
      "|    total_timesteps | 518144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=518500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 518500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024132585 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.344      |\n",
      "|    n_updates            | 2530        |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=519000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 519000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=519500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 519500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 520000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.2     |\n",
      "|    ep_rew_mean     | -5.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 18178    |\n",
      "|    total_timesteps | 520192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 520500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014838188 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.156      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0336     |\n",
      "|    value_loss           | 0.819       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=521000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 521000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=521500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 521500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=522000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 522000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.1     |\n",
      "|    ep_rew_mean     | -4.93    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 255      |\n",
      "|    time_elapsed    | 18237    |\n",
      "|    total_timesteps | 522240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=522500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 522500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023154868 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.244      |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    value_loss           | 0.484       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=523000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 523000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=523500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 523500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 524000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92       |\n",
      "|    ep_rew_mean     | -3.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 18296    |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 524500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021909429 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.142      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0483     |\n",
      "|    value_loss           | 0.411       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=525000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 525000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=525500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 525500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=526000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 526000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.3     |\n",
      "|    ep_rew_mean     | -2.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 257      |\n",
      "|    time_elapsed    | 18355    |\n",
      "|    total_timesteps | 526336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=526500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 526500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022859853 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.157      |\n",
      "|    n_updates            | 2570        |\n",
      "|    policy_gradient_loss | -0.0475     |\n",
      "|    value_loss           | 0.36        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=527000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 527000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=527500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 527500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 528000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.2     |\n",
      "|    ep_rew_mean     | -2.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 18430    |\n",
      "|    total_timesteps | 528384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 528500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019099558 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0286      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0333     |\n",
      "|    value_loss           | 0.831       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=529000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 529000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=529500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 529500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 530000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.9     |\n",
      "|    ep_rew_mean     | -1.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 259      |\n",
      "|    time_elapsed    | 18489    |\n",
      "|    total_timesteps | 530432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 530500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020162372 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0172      |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | -0.0354     |\n",
      "|    value_loss           | 0.945       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=531000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 531000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=531500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 531500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=532000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 532000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.8     |\n",
      "|    ep_rew_mean     | -1.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 18548    |\n",
      "|    total_timesteps | 532480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=532500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 532500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022207957 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.209      |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.042      |\n",
      "|    value_loss           | 0.44        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=533000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 533000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=533500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 533500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=534000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 534000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=534500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 534500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.5     |\n",
      "|    ep_rew_mean     | -2.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 261      |\n",
      "|    time_elapsed    | 18609    |\n",
      "|    total_timesteps | 534528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=535000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 535000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023028277 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.28       |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | -0.051      |\n",
      "|    value_loss           | 0.495       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=535500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 535500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=536000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 536000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=536500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 536500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.1     |\n",
      "|    ep_rew_mean     | -2.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 18668    |\n",
      "|    total_timesteps | 536576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=537000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 537000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025837725 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.228      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 0.267       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=537500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 537500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=538000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 538000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=538500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 538500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.9     |\n",
      "|    ep_rew_mean     | -1.37    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 263      |\n",
      "|    time_elapsed    | 18727    |\n",
      "|    total_timesteps | 538624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=539000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 539000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020801235 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.17        |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    value_loss           | 0.784       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=539500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 539500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.2     |\n",
      "|    ep_rew_mean     | -1.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 18786    |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=541000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 541000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023727112 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.128      |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 0.36        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=541500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 541500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=542000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 542000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=542500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 542500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.3     |\n",
      "|    ep_rew_mean     | -0.024   |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 265      |\n",
      "|    time_elapsed    | 18845    |\n",
      "|    total_timesteps | 542720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=543000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 543000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024809547 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.223      |\n",
      "|    n_updates            | 2650        |\n",
      "|    policy_gradient_loss | -0.0555     |\n",
      "|    value_loss           | 0.273       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=543500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 543500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 544000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 544500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.4     |\n",
      "|    ep_rew_mean     | 2        |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 18904    |\n",
      "|    total_timesteps | 544768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=545000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 545000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030418664 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.154      |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 0.331       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=545500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 545500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=546000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 546000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=546500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 546500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.9     |\n",
      "|    ep_rew_mean     | 2.67     |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 267      |\n",
      "|    time_elapsed    | 18963    |\n",
      "|    total_timesteps | 546816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=547000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 547000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019737776 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 2670        |\n",
      "|    policy_gradient_loss | -0.0349     |\n",
      "|    value_loss           | 0.981       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=547500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 547500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=548000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 548000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=548500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 548500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.2     |\n",
      "|    ep_rew_mean     | 3.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 19038    |\n",
      "|    total_timesteps | 548864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=549000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 549000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018992484 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0446      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0342     |\n",
      "|    value_loss           | 0.948       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=549500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 549500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 75.3     |\n",
      "|    ep_rew_mean     | 2.38     |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 269      |\n",
      "|    time_elapsed    | 19097    |\n",
      "|    total_timesteps | 550912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=551000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 551000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021739386 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.297       |\n",
      "|    n_updates            | 2690        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 1.07        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=551500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 551500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=552000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 552000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=552500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 552500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 78.9     |\n",
      "|    ep_rew_mean     | 1.98     |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 19173    |\n",
      "|    total_timesteps | 552960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=553000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 553000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023556657 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.818       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.281       |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    value_loss           | 0.819       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=553500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 553500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=554000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 554000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=554500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 554500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=555000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 555000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84       |\n",
      "|    ep_rew_mean     | 0.502    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 271      |\n",
      "|    time_elapsed    | 19234    |\n",
      "|    total_timesteps | 555008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=555500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 555500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022721823 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0758     |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | -0.0394     |\n",
      "|    value_loss           | 0.311       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=556000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 556000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=556500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 556500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=557000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 557000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.3     |\n",
      "|    ep_rew_mean     | -1.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 19293    |\n",
      "|    total_timesteps | 557056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=557500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 557500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021272153 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0397     |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0386     |\n",
      "|    value_loss           | 0.541       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=558000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 558000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=558500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 558500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=559000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 559000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.7     |\n",
      "|    ep_rew_mean     | -2.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 273      |\n",
      "|    time_elapsed    | 19352    |\n",
      "|    total_timesteps | 559104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=559500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 559500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024164014 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.259      |\n",
      "|    n_updates            | 2730        |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 0.212       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 560000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 560500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=561000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 561000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.7     |\n",
      "|    ep_rew_mean     | -2.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 19411    |\n",
      "|    total_timesteps | 561152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=561500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 561500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02873277 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.29      |\n",
      "|    explained_variance   | 0.928      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.198     |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0567    |\n",
      "|    value_loss           | 0.36       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=562000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 562000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=562500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 562500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=563000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 563000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.9     |\n",
      "|    ep_rew_mean     | -1.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 275      |\n",
      "|    time_elapsed    | 19485    |\n",
      "|    total_timesteps | 563200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=563500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 563500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020723883 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.164      |\n",
      "|    n_updates            | 2750        |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    value_loss           | 0.396       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=564000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 564000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 564500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=565000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 565000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81.2     |\n",
      "|    ep_rew_mean     | 0.571    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 19544    |\n",
      "|    total_timesteps | 565248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=565500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 565500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022740752 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00238    |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    value_loss           | 0.759       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=566000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 566000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=566500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 566500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=567000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 567000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.2     |\n",
      "|    ep_rew_mean     | 2.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 277      |\n",
      "|    time_elapsed    | 19603    |\n",
      "|    total_timesteps | 567296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=567500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 33        |\n",
      "|    mean_reward          | 10.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 567500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0225055 |\n",
      "|    clip_fraction        | 0.26      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.17     |\n",
      "|    explained_variance   | 0.901     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.229     |\n",
      "|    n_updates            | 2770      |\n",
      "|    policy_gradient_loss | -0.0388   |\n",
      "|    value_loss           | 0.592     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=568000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 568000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=568500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 568500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=569000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 569000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.3     |\n",
      "|    ep_rew_mean     | 3.8      |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 19662    |\n",
      "|    total_timesteps | 569344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=569500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 569500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024498597 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0622     |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0489     |\n",
      "|    value_loss           | 0.363       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 570000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 570500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=571000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 571000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 71.5     |\n",
      "|    ep_rew_mean     | 4.12     |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 279      |\n",
      "|    time_elapsed    | 19721    |\n",
      "|    total_timesteps | 571392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=571500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 571500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024660978 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.183      |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 0.458       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=572000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 572000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=572500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 572500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=573000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 573000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73       |\n",
      "|    ep_rew_mean     | 3.35     |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 19780    |\n",
      "|    total_timesteps | 573440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=573500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 573500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019111158 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.168      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    value_loss           | 0.63        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=574000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 574000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=574500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 574500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=575000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 575000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.2     |\n",
      "|    ep_rew_mean     | 3.12     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 281      |\n",
      "|    time_elapsed    | 19839    |\n",
      "|    total_timesteps | 575488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=575500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 575500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018559447 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0403     |\n",
      "|    n_updates            | 2810        |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    value_loss           | 0.527       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 576000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=576500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 576500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=577000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 577000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=577500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 577500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 71.4     |\n",
      "|    ep_rew_mean     | 3.67     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 19900    |\n",
      "|    total_timesteps | 577536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=578000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 578000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025873518 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0568     |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 0.41        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=578500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 578500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=579000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 579000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=579500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 579500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 65.3     |\n",
      "|    ep_rew_mean     | 5.38     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 283      |\n",
      "|    time_elapsed    | 19959    |\n",
      "|    total_timesteps | 579584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 580000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02638005 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1         |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0817    |\n",
      "|    n_updates            | 2830       |\n",
      "|    policy_gradient_loss | -0.0537    |\n",
      "|    value_loss           | 0.345      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=580500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 580500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=581000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 581000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=581500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 581500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 63       |\n",
      "|    ep_rew_mean     | 5.73     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 20019    |\n",
      "|    total_timesteps | 581632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=582000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 582000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023120046 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.62        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.398       |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=582500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 582500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=583000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 583000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=583500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 583500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 65.9     |\n",
      "|    ep_rew_mean     | 4.84     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 285      |\n",
      "|    time_elapsed    | 20078    |\n",
      "|    total_timesteps | 583680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 584000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023484718 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.41        |\n",
      "|    n_updates            | 2850        |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 1.33        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=584500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 584500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=585000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 585000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=585500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 585500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 69.2     |\n",
      "|    ep_rew_mean     | 4.38     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 20137    |\n",
      "|    total_timesteps | 585728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=586000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 586000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028748522 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.193      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    value_loss           | 0.507       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=586500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 586500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=587000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 587000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=587500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 587500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 75.7     |\n",
      "|    ep_rew_mean     | 2.39     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 287      |\n",
      "|    time_elapsed    | 20195    |\n",
      "|    total_timesteps | 587776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=588000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 588000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024220008 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.133       |\n",
      "|    n_updates            | 2870        |\n",
      "|    policy_gradient_loss | -0.053      |\n",
      "|    value_loss           | 0.456       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=588500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 588500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=589000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 589000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=589500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 589500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 82       |\n",
      "|    ep_rew_mean     | 0.035    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 20255    |\n",
      "|    total_timesteps | 589824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 590000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024694044 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.349      |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0585     |\n",
      "|    value_loss           | 0.232       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=590500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 590500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=591000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 591000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=591500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 591500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 85.3     |\n",
      "|    ep_rew_mean     | -0.814   |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 289      |\n",
      "|    time_elapsed    | 20315    |\n",
      "|    total_timesteps | 591872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 592000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02226977 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.3       |\n",
      "|    explained_variance   | 0.93       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.126     |\n",
      "|    n_updates            | 2890       |\n",
      "|    policy_gradient_loss | -0.0554    |\n",
      "|    value_loss           | 0.259      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=592500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 592500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=593000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 593000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=593500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 593500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.3     |\n",
      "|    ep_rew_mean     | -1.25    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 20374    |\n",
      "|    total_timesteps | 593920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=594000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 594000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02035651 |\n",
      "|    clip_fraction        | 0.241      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.26      |\n",
      "|    explained_variance   | 0.893      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0158     |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.0434    |\n",
      "|    value_loss           | 0.673      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=594500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 594500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=595000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 595000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=595500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 595500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 82.8     |\n",
      "|    ep_rew_mean     | -0.201   |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 291      |\n",
      "|    time_elapsed    | 20433    |\n",
      "|    total_timesteps | 595968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=596000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 596000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027062345 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.182      |\n",
      "|    n_updates            | 2910        |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 0.342       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=596500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 596500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=597000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 597000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=597500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 597500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=598000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 598000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.9     |\n",
      "|    ep_rew_mean     | 2.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 20494    |\n",
      "|    total_timesteps | 598016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=598500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 598500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020324185 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.289       |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    value_loss           | 1.16        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=599000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 599000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=599500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 599500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 600000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.6     |\n",
      "|    ep_rew_mean     | 3.17     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 293      |\n",
      "|    time_elapsed    | 20554    |\n",
      "|    total_timesteps | 600064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 600500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019491125 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.747       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.862       |\n",
      "|    n_updates            | 2930        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    value_loss           | 1.54        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=601000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 601000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=601500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 601500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=602000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 602000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.8     |\n",
      "|    ep_rew_mean     | 3.08     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 20613    |\n",
      "|    total_timesteps | 602112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=602500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 602500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022652684 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0329      |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0361     |\n",
      "|    value_loss           | 0.915       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=603000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 603000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=603500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 603500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 604000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.3     |\n",
      "|    ep_rew_mean     | 2.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 295      |\n",
      "|    time_elapsed    | 20673    |\n",
      "|    total_timesteps | 604160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 604500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027431047 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.188      |\n",
      "|    n_updates            | 2950        |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 0.327       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=605000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 605000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=605500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 605500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=606000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 606000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.7     |\n",
      "|    ep_rew_mean     | 0.128    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 20732    |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=606500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 606500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022476904 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.164      |\n",
      "|    n_updates            | 2960        |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 0.278       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=607000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 607000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=607500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 607500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=608000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 608000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88       |\n",
      "|    ep_rew_mean     | -1.74    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 297      |\n",
      "|    time_elapsed    | 20792    |\n",
      "|    total_timesteps | 608256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=608500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 608500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025088102 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.289      |\n",
      "|    n_updates            | 2970        |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=609000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 609000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=609500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 609500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 610000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.8     |\n",
      "|    ep_rew_mean     | -1.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 20851    |\n",
      "|    total_timesteps | 610304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 610500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026296437 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.271      |\n",
      "|    n_updates            | 2980        |\n",
      "|    policy_gradient_loss | -0.065      |\n",
      "|    value_loss           | 0.209       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=611000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 611000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=611500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 611500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=612000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 612000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.3     |\n",
      "|    ep_rew_mean     | -1.95    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 299      |\n",
      "|    time_elapsed    | 20910    |\n",
      "|    total_timesteps | 612352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=612500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 612500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023965852 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.129      |\n",
      "|    n_updates            | 2990        |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 0.468       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=613000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 613000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=613500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 613500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=614000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 614000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81.1     |\n",
      "|    ep_rew_mean     | -0.526   |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 20970    |\n",
      "|    total_timesteps | 614400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=614500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 614500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026065167 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.094      |\n",
      "|    n_updates            | 3000        |\n",
      "|    policy_gradient_loss | -0.0394     |\n",
      "|    value_loss           | 0.638       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=615000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 615000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=615500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 615500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=616000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 616000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.5     |\n",
      "|    ep_rew_mean     | 1.67     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 301      |\n",
      "|    time_elapsed    | 21030    |\n",
      "|    total_timesteps | 616448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=616500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 616500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020230168 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0695     |\n",
      "|    n_updates            | 3010        |\n",
      "|    policy_gradient_loss | -0.0368     |\n",
      "|    value_loss           | 0.777       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=617000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 617000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=617500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 617500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=618000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 618000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 78.5     |\n",
      "|    ep_rew_mean     | 1.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 302      |\n",
      "|    time_elapsed    | 21090    |\n",
      "|    total_timesteps | 618496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=618500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 618500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027229037 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.136       |\n",
      "|    n_updates            | 3020        |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    value_loss           | 0.401       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=619000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 619000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=619500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 619500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 80.4     |\n",
      "|    ep_rew_mean     | 1.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 303      |\n",
      "|    time_elapsed    | 21151    |\n",
      "|    total_timesteps | 620544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=621000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 621000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022727324 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0352     |\n",
      "|    n_updates            | 3030        |\n",
      "|    policy_gradient_loss | -0.0496     |\n",
      "|    value_loss           | 0.37        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=621500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 621500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=622000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 622000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=622500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 622500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.3     |\n",
      "|    ep_rew_mean     | -0.298   |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 304      |\n",
      "|    time_elapsed    | 21210    |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=623000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 623000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022683164 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.255      |\n",
      "|    n_updates            | 3040        |\n",
      "|    policy_gradient_loss | -0.0478     |\n",
      "|    value_loss           | 0.42        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=623500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 623500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 624000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 624500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87       |\n",
      "|    ep_rew_mean     | -1.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 305      |\n",
      "|    time_elapsed    | 21269    |\n",
      "|    total_timesteps | 624640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=625000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 625000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025079146 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.263      |\n",
      "|    n_updates            | 3050        |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 0.189       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=625500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 625500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=626000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 626000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=626500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 626500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.2     |\n",
      "|    ep_rew_mean     | -1.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 306      |\n",
      "|    time_elapsed    | 21329    |\n",
      "|    total_timesteps | 626688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=627000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 627000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03591545 |\n",
      "|    clip_fraction        | 0.341      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.28      |\n",
      "|    explained_variance   | 0.949      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.218     |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | -0.0652    |\n",
      "|    value_loss           | 0.223      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=627500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 627500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=628000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 628000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=628500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 628500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.2     |\n",
      "|    ep_rew_mean     | -0.83    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 307      |\n",
      "|    time_elapsed    | 21405    |\n",
      "|    total_timesteps | 628736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=629000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 629000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021756385 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.3         |\n",
      "|    n_updates            | 3070        |\n",
      "|    policy_gradient_loss | -0.0372     |\n",
      "|    value_loss           | 0.782       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=629500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 629500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.4     |\n",
      "|    ep_rew_mean     | -0.842   |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 308      |\n",
      "|    time_elapsed    | 21465    |\n",
      "|    total_timesteps | 630784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=631000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 631000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027905278 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.241       |\n",
      "|    n_updates            | 3080        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    value_loss           | 1.25        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=631500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 631500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=632000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 632000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=632500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 632500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.4     |\n",
      "|    ep_rew_mean     | -1.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 309      |\n",
      "|    time_elapsed    | 21524    |\n",
      "|    total_timesteps | 632832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=633000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 633000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021879897 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.654       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.271      |\n",
      "|    n_updates            | 3090        |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    value_loss           | 0.747       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=633500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 633500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=634000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 634000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=634500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 634500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.5     |\n",
      "|    ep_rew_mean     | -2.45    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 310      |\n",
      "|    time_elapsed    | 21600    |\n",
      "|    total_timesteps | 634880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=635000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 635000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022994962 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0608     |\n",
      "|    n_updates            | 3100        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    value_loss           | 0.508       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=635500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 635500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=636000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 636000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=636500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 636500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.2     |\n",
      "|    ep_rew_mean     | -4.11    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 311      |\n",
      "|    time_elapsed    | 21660    |\n",
      "|    total_timesteps | 636928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=637000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 637000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026149694 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.323      |\n",
      "|    n_updates            | 3110        |\n",
      "|    policy_gradient_loss | -0.0592     |\n",
      "|    value_loss           | 0.0936      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=637500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 637500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=638000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 638000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=638500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 638500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.5     |\n",
      "|    ep_rew_mean     | -5.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 312      |\n",
      "|    time_elapsed    | 21735    |\n",
      "|    total_timesteps | 638976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=639000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 639000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021126468 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.719       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.273      |\n",
      "|    n_updates            | 3120        |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    value_loss           | 0.242       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=639500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 639500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=641000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 641000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.7     |\n",
      "|    ep_rew_mean     | -6.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 313      |\n",
      "|    time_elapsed    | 21817    |\n",
      "|    total_timesteps | 641024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=641500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 641500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02568064 |\n",
      "|    clip_fraction        | 0.355      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.5       |\n",
      "|    explained_variance   | 0.138      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.342     |\n",
      "|    n_updates            | 3130       |\n",
      "|    policy_gradient_loss | -0.0648    |\n",
      "|    value_loss           | 0.0989     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=642000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 642000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=642500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 642500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=643000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 643000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.7     |\n",
      "|    ep_rew_mean     | -7       |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 314      |\n",
      "|    time_elapsed    | 21877    |\n",
      "|    total_timesteps | 643072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=643500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 643500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022536917 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.311      |\n",
      "|    n_updates            | 3140        |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=644000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 644000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=644500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 644500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 645000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.8     |\n",
      "|    ep_rew_mean     | -7.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 315      |\n",
      "|    time_elapsed    | 21936    |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 645500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016579375 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.483       |\n",
      "|    n_updates            | 3150        |\n",
      "|    policy_gradient_loss | -0.0357     |\n",
      "|    value_loss           | 0.595       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=646000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 646000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=646500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 646500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=647000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 647000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.2     |\n",
      "|    ep_rew_mean     | -5.84    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 316      |\n",
      "|    time_elapsed    | 21996    |\n",
      "|    total_timesteps | 647168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=647500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 647500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025398804 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.203      |\n",
      "|    n_updates            | 3160        |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 0.364       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=648000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 648000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=648500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 648500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=649000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 649000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94       |\n",
      "|    ep_rew_mean     | -4.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 317      |\n",
      "|    time_elapsed    | 22071    |\n",
      "|    total_timesteps | 649216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=649500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 649500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017247016 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.257      |\n",
      "|    n_updates            | 3170        |\n",
      "|    policy_gradient_loss | -0.0361     |\n",
      "|    value_loss           | 0.475       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 650000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 650500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=651000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 651000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.2     |\n",
      "|    ep_rew_mean     | -3.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 318      |\n",
      "|    time_elapsed    | 22147    |\n",
      "|    total_timesteps | 651264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=651500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 651500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019150015 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.597       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0531     |\n",
      "|    n_updates            | 3180        |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    value_loss           | 0.898       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=652000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 652000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=652500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 652500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=653000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 653000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.7     |\n",
      "|    ep_rew_mean     | -3.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 319      |\n",
      "|    time_elapsed    | 22223    |\n",
      "|    total_timesteps | 653312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=653500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 653500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01915924 |\n",
      "|    clip_fraction        | 0.213      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | 0.515      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.115      |\n",
      "|    n_updates            | 3190       |\n",
      "|    policy_gradient_loss | -0.0352    |\n",
      "|    value_loss           | 1.15       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=654000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 654000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=654500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 654500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=655000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 655000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.7     |\n",
      "|    ep_rew_mean     | -3.92    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 320      |\n",
      "|    time_elapsed    | 22283    |\n",
      "|    total_timesteps | 655360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=655500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 655500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023070127 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.568       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 3200        |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    value_loss           | 0.673       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=656000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 656000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=656500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 656500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=657000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 657000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.1     |\n",
      "|    ep_rew_mean     | -4.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 321      |\n",
      "|    time_elapsed    | 22358    |\n",
      "|    total_timesteps | 657408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=657500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 657500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021927023 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.647       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0604     |\n",
      "|    n_updates            | 3210        |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    value_loss           | 0.619       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=658000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 658000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=658500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 658500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=659000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 659000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.1     |\n",
      "|    ep_rew_mean     | -4.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 322      |\n",
      "|    time_elapsed    | 22434    |\n",
      "|    total_timesteps | 659456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=659500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 659500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025204765 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.12       |\n",
      "|    n_updates            | 3220        |\n",
      "|    policy_gradient_loss | -0.0476     |\n",
      "|    value_loss           | 0.272       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 660000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 660500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=661000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 661000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=661500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 661500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.4     |\n",
      "|    ep_rew_mean     | -4.79    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 323      |\n",
      "|    time_elapsed    | 22515    |\n",
      "|    total_timesteps | 661504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=662000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 662000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018535145 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.104      |\n",
      "|    n_updates            | 3230        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    value_loss           | 0.74        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=662500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 662500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=663000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 663000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=663500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 663500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.8     |\n",
      "|    ep_rew_mean     | -5.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 324      |\n",
      "|    time_elapsed    | 22575    |\n",
      "|    total_timesteps | 663552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=664000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 664000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019551491 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.238      |\n",
      "|    n_updates            | 3240        |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    value_loss           | 0.511       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=664500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 664500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 665000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 665500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.6     |\n",
      "|    ep_rew_mean     | -4.85    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 325      |\n",
      "|    time_elapsed    | 22635    |\n",
      "|    total_timesteps | 665600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=666000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 666000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02548945 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.827      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00681   |\n",
      "|    n_updates            | 3250       |\n",
      "|    policy_gradient_loss | -0.0493    |\n",
      "|    value_loss           | 0.514      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=666500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 666500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=667000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 667000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=667500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 667500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.1     |\n",
      "|    ep_rew_mean     | -4.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 326      |\n",
      "|    time_elapsed    | 22694    |\n",
      "|    total_timesteps | 667648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=668000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 668000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022891564 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.222      |\n",
      "|    n_updates            | 3260        |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    value_loss           | 0.638       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=668500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 668500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=669000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 669000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=669500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 669500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.1     |\n",
      "|    ep_rew_mean     | -3.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 327      |\n",
      "|    time_elapsed    | 22754    |\n",
      "|    total_timesteps | 669696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 670000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026388858 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.189      |\n",
      "|    n_updates            | 3270        |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 0.461       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=670500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 670500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=671000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 671000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=671500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 671500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.4     |\n",
      "|    ep_rew_mean     | -2.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 328      |\n",
      "|    time_elapsed    | 22814    |\n",
      "|    total_timesteps | 671744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 672000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028102957 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.282      |\n",
      "|    n_updates            | 3280        |\n",
      "|    policy_gradient_loss | -0.0653     |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=672500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 672500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=673000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 673000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=673500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 673500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 85.3     |\n",
      "|    ep_rew_mean     | -0.707   |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 329      |\n",
      "|    time_elapsed    | 22889    |\n",
      "|    total_timesteps | 673792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=674000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 674000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021588823 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0446     |\n",
      "|    n_updates            | 3290        |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=674500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 674500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=675000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 675000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=675500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 675500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 79.4     |\n",
      "|    ep_rew_mean     | 1.28     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 330      |\n",
      "|    time_elapsed    | 22949    |\n",
      "|    total_timesteps | 675840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=676000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 676000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030535884 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0595      |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    value_loss           | 0.698       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=676500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 676500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=677000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 677000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=677500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 677500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.2     |\n",
      "|    ep_rew_mean     | 2.09     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 331      |\n",
      "|    time_elapsed    | 23008    |\n",
      "|    total_timesteps | 677888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=678000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 678000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022245765 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.117       |\n",
      "|    n_updates            | 3310        |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    value_loss           | 0.731       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=678500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 678500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=679000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 679000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=679500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 679500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.3     |\n",
      "|    ep_rew_mean     | 3.08     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 332      |\n",
      "|    time_elapsed    | 23068    |\n",
      "|    total_timesteps | 679936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 680000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030193185 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.124      |\n",
      "|    n_updates            | 3320        |\n",
      "|    policy_gradient_loss | -0.0529     |\n",
      "|    value_loss           | 0.403       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=680500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 680500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=681000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 681000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=681500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 681500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.1     |\n",
      "|    ep_rew_mean     | 2.65     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 333      |\n",
      "|    time_elapsed    | 23144    |\n",
      "|    total_timesteps | 681984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=682000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 682000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02440887 |\n",
      "|    clip_fraction        | 0.299      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.3       |\n",
      "|    explained_variance   | 0.913      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.233     |\n",
      "|    n_updates            | 3330       |\n",
      "|    policy_gradient_loss | -0.0409    |\n",
      "|    value_loss           | 0.503      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=682500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 682500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=683000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 683000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=683500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 683500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=684000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 684000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81.3     |\n",
      "|    ep_rew_mean     | 0.723    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 334      |\n",
      "|    time_elapsed    | 23206    |\n",
      "|    total_timesteps | 684032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=684500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 684500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025656968 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.299      |\n",
      "|    n_updates            | 3340        |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 0.261       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=685000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 685000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=685500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 685500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=686000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 686000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.9     |\n",
      "|    ep_rew_mean     | -0.314   |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 335      |\n",
      "|    time_elapsed    | 23265    |\n",
      "|    total_timesteps | 686080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=686500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 686500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02638045 |\n",
      "|    clip_fraction        | 0.334      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.36      |\n",
      "|    explained_variance   | 0.943      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.249     |\n",
      "|    n_updates            | 3350       |\n",
      "|    policy_gradient_loss | -0.0563    |\n",
      "|    value_loss           | 0.26       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=687000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 687000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=687500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 687500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=688000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 688000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.7     |\n",
      "|    ep_rew_mean     | -2.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 336      |\n",
      "|    time_elapsed    | 23325    |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=688500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 688500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023571853 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.451       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.341      |\n",
      "|    n_updates            | 3360        |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 0.206       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=689000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 689000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=689500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 689500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 690000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.4     |\n",
      "|    ep_rew_mean     | -3.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 337      |\n",
      "|    time_elapsed    | 23385    |\n",
      "|    total_timesteps | 690176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 690500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02467003 |\n",
      "|    clip_fraction        | 0.268      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.36      |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.175     |\n",
      "|    n_updates            | 3370       |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    value_loss           | 0.263      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=691000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 691000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=691500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 691500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=692000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 692000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.5     |\n",
      "|    ep_rew_mean     | -2.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 338      |\n",
      "|    time_elapsed    | 23444    |\n",
      "|    total_timesteps | 692224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=692500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 692500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020128462 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.105      |\n",
      "|    n_updates            | 3380        |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 0.379       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=693000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 693000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=693500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 693500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=694000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 694000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.3     |\n",
      "|    ep_rew_mean     | -1.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 339      |\n",
      "|    time_elapsed    | 23504    |\n",
      "|    total_timesteps | 694272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=694500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 694500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017476823 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0151      |\n",
      "|    n_updates            | 3390        |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    value_loss           | 0.428       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=695000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 695000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=695500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 695500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=696000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 696000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.3     |\n",
      "|    ep_rew_mean     | 1.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 340      |\n",
      "|    time_elapsed    | 23564    |\n",
      "|    total_timesteps | 696320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=696500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 696500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021624826 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0188     |\n",
      "|    n_updates            | 3400        |\n",
      "|    policy_gradient_loss | -0.041      |\n",
      "|    value_loss           | 0.608       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=697000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 697000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=697500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 697500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=698000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 698000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.7     |\n",
      "|    ep_rew_mean     | 2.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 341      |\n",
      "|    time_elapsed    | 23623    |\n",
      "|    total_timesteps | 698368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=698500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 698500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019637119 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.152      |\n",
      "|    n_updates            | 3410        |\n",
      "|    policy_gradient_loss | -0.0358     |\n",
      "|    value_loss           | 0.703       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=699000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 699000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=699500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 699500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 70.1     |\n",
      "|    ep_rew_mean     | 3.78     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 342      |\n",
      "|    time_elapsed    | 23683    |\n",
      "|    total_timesteps | 700416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 700500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020833248 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.218       |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | -0.036      |\n",
      "|    value_loss           | 0.668       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=701000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 701000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=701500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 701500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=702000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 702000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 69.9     |\n",
      "|    ep_rew_mean     | 4.1      |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 343      |\n",
      "|    time_elapsed    | 23743    |\n",
      "|    total_timesteps | 702464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=702500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 702500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021347463 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.611       |\n",
      "|    n_updates            | 3430        |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    value_loss           | 0.729       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=703000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 703000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=703500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 703500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 704000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=704500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 704500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 71.3     |\n",
      "|    ep_rew_mean     | 3.92     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 344      |\n",
      "|    time_elapsed    | 23805    |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=705000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 705000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020518739 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.124      |\n",
      "|    n_updates            | 3440        |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    value_loss           | 0.473       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=705500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 705500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=706000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 706000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=706500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 706500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73       |\n",
      "|    ep_rew_mean     | 3.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 345      |\n",
      "|    time_elapsed    | 23865    |\n",
      "|    total_timesteps | 706560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=707000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 707000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023926131 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.248      |\n",
      "|    n_updates            | 3450        |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    value_loss           | 0.321       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=707500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 707500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=708000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 708000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=708500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 708500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.8     |\n",
      "|    ep_rew_mean     | 3.33     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 346      |\n",
      "|    time_elapsed    | 23924    |\n",
      "|    total_timesteps | 708608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=709000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 709000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021677189 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.173      |\n",
      "|    n_updates            | 3460        |\n",
      "|    policy_gradient_loss | -0.0508     |\n",
      "|    value_loss           | 0.445       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=709500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 709500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 71.8     |\n",
      "|    ep_rew_mean     | 3.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 347      |\n",
      "|    time_elapsed    | 23984    |\n",
      "|    total_timesteps | 710656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=711000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 711000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026016746 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.223      |\n",
      "|    n_updates            | 3470        |\n",
      "|    policy_gradient_loss | -0.0617     |\n",
      "|    value_loss           | 0.23        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=711500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 711500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=712000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 712000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=712500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 712500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 70.8     |\n",
      "|    ep_rew_mean     | 3.31     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 348      |\n",
      "|    time_elapsed    | 24043    |\n",
      "|    total_timesteps | 712704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=713000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 713000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026833974 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.157      |\n",
      "|    n_updates            | 3480        |\n",
      "|    policy_gradient_loss | -0.0495     |\n",
      "|    value_loss           | 0.598       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=713500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 713500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=714000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 714000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=714500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 714500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 65.4     |\n",
      "|    ep_rew_mean     | 4.74     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 349      |\n",
      "|    time_elapsed    | 24103    |\n",
      "|    total_timesteps | 714752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=715000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 715000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027964054 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.257       |\n",
      "|    n_updates            | 3490        |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    value_loss           | 0.622       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=715500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 715500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=716000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 716000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=716500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 716500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 65.6     |\n",
      "|    ep_rew_mean     | 4.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 350      |\n",
      "|    time_elapsed    | 24163    |\n",
      "|    total_timesteps | 716800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=717000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 717000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027463712 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0258      |\n",
      "|    n_updates            | 3500        |\n",
      "|    policy_gradient_loss | -0.0331     |\n",
      "|    value_loss           | 0.9         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=717500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 717500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=718000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 718000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=718500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 718500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 67.5     |\n",
      "|    ep_rew_mean     | 4.29     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 351      |\n",
      "|    time_elapsed    | 24223    |\n",
      "|    total_timesteps | 718848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=719000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 719000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029233726 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.163      |\n",
      "|    n_updates            | 3510        |\n",
      "|    policy_gradient_loss | -0.0412     |\n",
      "|    value_loss           | 0.358       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=719500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 719500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.8     |\n",
      "|    ep_rew_mean     | 2.49     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 352      |\n",
      "|    time_elapsed    | 24283    |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=721000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 721000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024334721 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.273      |\n",
      "|    n_updates            | 3520        |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    value_loss           | 0.401       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=721500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 721500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=722000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 722000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=722500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 722500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81.6     |\n",
      "|    ep_rew_mean     | 0.347    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 353      |\n",
      "|    time_elapsed    | 24360    |\n",
      "|    total_timesteps | 722944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=723000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 723000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023094203 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.158      |\n",
      "|    n_updates            | 3530        |\n",
      "|    policy_gradient_loss | -0.0402     |\n",
      "|    value_loss           | 0.605       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=723500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 723500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=724000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 724000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=724500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 724500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 85.3     |\n",
      "|    ep_rew_mean     | -0.515   |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 354      |\n",
      "|    time_elapsed    | 24420    |\n",
      "|    total_timesteps | 724992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 725000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020826396 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.142      |\n",
      "|    n_updates            | 3540        |\n",
      "|    policy_gradient_loss | -0.033      |\n",
      "|    value_loss           | 0.698       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=725500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 725500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=726000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 726000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=726500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 726500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=727000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 727000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 85.2     |\n",
      "|    ep_rew_mean     | -0.235   |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 355      |\n",
      "|    time_elapsed    | 24482    |\n",
      "|    total_timesteps | 727040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=727500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 33        |\n",
      "|    mean_reward          | 10.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 727500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0219123 |\n",
      "|    clip_fraction        | 0.263     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.28     |\n",
      "|    explained_variance   | 0.939     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.189     |\n",
      "|    n_updates            | 3550      |\n",
      "|    policy_gradient_loss | -0.0447   |\n",
      "|    value_loss           | 0.439     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=728000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 728000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=728500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 728500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=729000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 729000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 85.9     |\n",
      "|    ep_rew_mean     | -0.361   |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 356      |\n",
      "|    time_elapsed    | 24542    |\n",
      "|    total_timesteps | 729088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=729500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 729500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021726713 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.11        |\n",
      "|    n_updates            | 3560        |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    value_loss           | 0.661       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 730000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=730500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 730500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=731000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 731000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.5     |\n",
      "|    ep_rew_mean     | 0.235    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 357      |\n",
      "|    time_elapsed    | 24601    |\n",
      "|    total_timesteps | 731136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=731500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 731500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021060083 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0836     |\n",
      "|    n_updates            | 3570        |\n",
      "|    policy_gradient_loss | -0.042      |\n",
      "|    value_loss           | 0.596       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=732000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 732000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=732500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 732500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=733000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 733000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.3     |\n",
      "|    ep_rew_mean     | -0.605   |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 358      |\n",
      "|    time_elapsed    | 24677    |\n",
      "|    total_timesteps | 733184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=733500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 733500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027425887 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.213      |\n",
      "|    n_updates            | 3580        |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    value_loss           | 0.662       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=734000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 734000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=734500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 734500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=735000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 735000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.6     |\n",
      "|    ep_rew_mean     | -1.18    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 359      |\n",
      "|    time_elapsed    | 24753    |\n",
      "|    total_timesteps | 735232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=735500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 735500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021681584 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0177      |\n",
      "|    n_updates            | 3590        |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    value_loss           | 0.724       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 736000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=736500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 736500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=737000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 737000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.1     |\n",
      "|    ep_rew_mean     | -1.84    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 360      |\n",
      "|    time_elapsed    | 24813    |\n",
      "|    total_timesteps | 737280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=737500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 737500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021905689 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.217      |\n",
      "|    n_updates            | 3600        |\n",
      "|    policy_gradient_loss | -0.051      |\n",
      "|    value_loss           | 0.344       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=738000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 738000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=738500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 738500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=739000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 739000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.9     |\n",
      "|    ep_rew_mean     | -2.84    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 361      |\n",
      "|    time_elapsed    | 24872    |\n",
      "|    total_timesteps | 739328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=739500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 739500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023148378 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.213      |\n",
      "|    n_updates            | 3610        |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 0.296       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=741000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 741000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.1     |\n",
      "|    ep_rew_mean     | -2.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 362      |\n",
      "|    time_elapsed    | 24932    |\n",
      "|    total_timesteps | 741376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=741500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 741500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021001045 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0733     |\n",
      "|    n_updates            | 3620        |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    value_loss           | 0.626       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=742000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 742000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=742500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 742500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=743000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 743000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.8     |\n",
      "|    ep_rew_mean     | -1.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 363      |\n",
      "|    time_elapsed    | 24992    |\n",
      "|    total_timesteps | 743424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=743500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 743500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01894599 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.3       |\n",
      "|    explained_variance   | 0.829      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0617    |\n",
      "|    n_updates            | 3630       |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    value_loss           | 0.942      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=744000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 744000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=744500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 744500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 745000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.5     |\n",
      "|    ep_rew_mean     | 0.289    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 364      |\n",
      "|    time_elapsed    | 25052    |\n",
      "|    total_timesteps | 745472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 745500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019843008 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.14       |\n",
      "|    n_updates            | 3640        |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    value_loss           | 0.906       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=746000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 746000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=746500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 746500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=747000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 747000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=747500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 747500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 80       |\n",
      "|    ep_rew_mean     | 1.92     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 365      |\n",
      "|    time_elapsed    | 25113    |\n",
      "|    total_timesteps | 747520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=748000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 748000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025093382 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.145      |\n",
      "|    n_updates            | 3650        |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 0.358       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=748500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 748500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=749000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 749000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=749500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 749500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.7     |\n",
      "|    ep_rew_mean     | 2.27     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 366      |\n",
      "|    time_elapsed    | 25173    |\n",
      "|    total_timesteps | 749568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 750000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02361587 |\n",
      "|    clip_fraction        | 0.262      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.19      |\n",
      "|    explained_variance   | 0.922      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.137     |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.048     |\n",
      "|    value_loss           | 0.561      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=750500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 750500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=751000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 751000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=751500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 751500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.5     |\n",
      "|    ep_rew_mean     | 2.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 367      |\n",
      "|    time_elapsed    | 25232    |\n",
      "|    total_timesteps | 751616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 752000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024666041 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.222      |\n",
      "|    n_updates            | 3670        |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 0.33        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=752500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 752500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=753000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 753000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=753500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 753500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 71.5     |\n",
      "|    ep_rew_mean     | 3.13     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 368      |\n",
      "|    time_elapsed    | 25291    |\n",
      "|    total_timesteps | 753664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=754000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 754000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017842198 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.137      |\n",
      "|    n_updates            | 3680        |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    value_loss           | 0.525       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=754500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 754500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=755000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 755000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=755500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 755500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 68       |\n",
      "|    ep_rew_mean     | 3.9      |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 369      |\n",
      "|    time_elapsed    | 25351    |\n",
      "|    total_timesteps | 755712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=756000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 756000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020995036 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.116      |\n",
      "|    n_updates            | 3690        |\n",
      "|    policy_gradient_loss | -0.0476     |\n",
      "|    value_loss           | 0.359       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=756500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 756500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=757000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 757000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=757500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 757500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 69       |\n",
      "|    ep_rew_mean     | 3.87     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 370      |\n",
      "|    time_elapsed    | 25411    |\n",
      "|    total_timesteps | 757760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=758000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 758000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025567763 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.023      |\n",
      "|    n_updates            | 3700        |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    value_loss           | 0.576       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=758500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 758500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=759000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 759000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=759500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 759500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 70.8     |\n",
      "|    ep_rew_mean     | 3.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 371      |\n",
      "|    time_elapsed    | 25471    |\n",
      "|    total_timesteps | 759808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 760000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023628214 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0198      |\n",
      "|    n_updates            | 3710        |\n",
      "|    policy_gradient_loss | -0.0381     |\n",
      "|    value_loss           | 0.565       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=760500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 760500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=761000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 761000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=761500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 761500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.1     |\n",
      "|    ep_rew_mean     | 2.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 372      |\n",
      "|    time_elapsed    | 25531    |\n",
      "|    total_timesteps | 761856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=762000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 762000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021571828 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.238      |\n",
      "|    n_updates            | 3720        |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 0.316       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=762500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 762500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=763000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 763000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=763500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 763500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.7     |\n",
      "|    ep_rew_mean     | 1.93     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 373      |\n",
      "|    time_elapsed    | 25591    |\n",
      "|    total_timesteps | 763904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=764000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 764000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02314844 |\n",
      "|    clip_fraction        | 0.277      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.24      |\n",
      "|    explained_variance   | 0.926      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.156     |\n",
      "|    n_updates            | 3730       |\n",
      "|    policy_gradient_loss | -0.0459    |\n",
      "|    value_loss           | 0.515      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=764500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 764500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=765000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 765000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=765500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 765500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.7     |\n",
      "|    ep_rew_mean     | 1.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 374      |\n",
      "|    time_elapsed    | 25651    |\n",
      "|    total_timesteps | 765952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=766000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 766000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023324057 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0881     |\n",
      "|    n_updates            | 3740        |\n",
      "|    policy_gradient_loss | -0.0594     |\n",
      "|    value_loss           | 0.325       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=766500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 766500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=767000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 767000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=767500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 767500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 768000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74       |\n",
      "|    ep_rew_mean     | 2.13     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 375      |\n",
      "|    time_elapsed    | 25713    |\n",
      "|    total_timesteps | 768000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=768500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 768500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027477166 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.196      |\n",
      "|    n_updates            | 3750        |\n",
      "|    policy_gradient_loss | -0.0643     |\n",
      "|    value_loss           | 0.299       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=769000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 769000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=769500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 769500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 770000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 71.8     |\n",
      "|    ep_rew_mean     | 2.52     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 376      |\n",
      "|    time_elapsed    | 25773    |\n",
      "|    total_timesteps | 770048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 770500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020001737 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0328     |\n",
      "|    n_updates            | 3760        |\n",
      "|    policy_gradient_loss | -0.0448     |\n",
      "|    value_loss           | 0.492       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=771000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 771000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=771500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 771500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=772000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 772000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 68.4     |\n",
      "|    ep_rew_mean     | 3.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 377      |\n",
      "|    time_elapsed    | 25832    |\n",
      "|    total_timesteps | 772096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=772500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 772500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022084232 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.687       |\n",
      "|    n_updates            | 3770        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    value_loss           | 1.3         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=773000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 773000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=773500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 773500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=774000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 774000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 71.2     |\n",
      "|    ep_rew_mean     | 3.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 378      |\n",
      "|    time_elapsed    | 25892    |\n",
      "|    total_timesteps | 774144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=774500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 774500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021957973 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.758       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.729       |\n",
      "|    n_updates            | 3780        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 1.48        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=775000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 775000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=775500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 775500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=776000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 776000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74       |\n",
      "|    ep_rew_mean     | 2.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 379      |\n",
      "|    time_elapsed    | 25951    |\n",
      "|    total_timesteps | 776192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=776500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 776500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024903363 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0507      |\n",
      "|    n_updates            | 3790        |\n",
      "|    policy_gradient_loss | -0.0347     |\n",
      "|    value_loss           | 0.7         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=777000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 777000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=777500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 777500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=778000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 778000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 79.4     |\n",
      "|    ep_rew_mean     | 1.69     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 380      |\n",
      "|    time_elapsed    | 26011    |\n",
      "|    total_timesteps | 778240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=778500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 778500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025065755 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 3800        |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    value_loss           | 0.347       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=779000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 779000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=779500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 779500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 780000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.9     |\n",
      "|    ep_rew_mean     | -0.002   |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 381      |\n",
      "|    time_elapsed    | 26087    |\n",
      "|    total_timesteps | 780288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 780500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026566945 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0923     |\n",
      "|    n_updates            | 3810        |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 0.268       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=781000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 781000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=781500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 781500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=782000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 782000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.5     |\n",
      "|    ep_rew_mean     | -0.593   |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 382      |\n",
      "|    time_elapsed    | 26146    |\n",
      "|    total_timesteps | 782336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=782500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 33        |\n",
      "|    mean_reward          | 10.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 782500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0186741 |\n",
      "|    clip_fraction        | 0.241     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.38     |\n",
      "|    explained_variance   | 0.765     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0719    |\n",
      "|    n_updates            | 3820      |\n",
      "|    policy_gradient_loss | -0.0413   |\n",
      "|    value_loss           | 0.861     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=783000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 783000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=783500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 783500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 784000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.5     |\n",
      "|    ep_rew_mean     | -1.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 383      |\n",
      "|    time_elapsed    | 26206    |\n",
      "|    total_timesteps | 784384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=784500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 784500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022684943 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.184      |\n",
      "|    n_updates            | 3830        |\n",
      "|    policy_gradient_loss | -0.0547     |\n",
      "|    value_loss           | 0.4         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=785000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 785000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=785500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 785500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 786000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 82.9     |\n",
      "|    ep_rew_mean     | 0.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 384      |\n",
      "|    time_elapsed    | 26265    |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 786500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02492961 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.2       |\n",
      "|    explained_variance   | 0.943      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.203     |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.054     |\n",
      "|    value_loss           | 0.395      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=787000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 787000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=787500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 787500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=788000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 788000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 78.8     |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 385      |\n",
      "|    time_elapsed    | 26325    |\n",
      "|    total_timesteps | 788480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=788500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 788500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020800665 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.294       |\n",
      "|    n_updates            | 3850        |\n",
      "|    policy_gradient_loss | -0.0468     |\n",
      "|    value_loss           | 0.57        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=789000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 789000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=789500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 789500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.8     |\n",
      "|    ep_rew_mean     | 2.23     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 386      |\n",
      "|    time_elapsed    | 26387    |\n",
      "|    total_timesteps | 790528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=791000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 791000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019183133 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0711     |\n",
      "|    n_updates            | 3860        |\n",
      "|    policy_gradient_loss | -0.0413     |\n",
      "|    value_loss           | 0.664       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=791500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 791500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=792000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 792000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=792500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 792500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 70.4     |\n",
      "|    ep_rew_mean     | 3.99     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 387      |\n",
      "|    time_elapsed    | 26447    |\n",
      "|    total_timesteps | 792576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=793000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 793000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021811314 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00317    |\n",
      "|    n_updates            | 3870        |\n",
      "|    policy_gradient_loss | -0.0412     |\n",
      "|    value_loss           | 0.646       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=793500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 793500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=794000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 794000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=794500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 794500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 69.2     |\n",
      "|    ep_rew_mean     | 4.29     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 388      |\n",
      "|    time_elapsed    | 26506    |\n",
      "|    total_timesteps | 794624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=795000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 795000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026357295 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.158      |\n",
      "|    n_updates            | 3880        |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 0.241       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=795500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 795500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=796000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 796000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=796500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 796500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 68.7     |\n",
      "|    ep_rew_mean     | 4.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 389      |\n",
      "|    time_elapsed    | 26566    |\n",
      "|    total_timesteps | 796672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=797000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 797000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023063786 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0839     |\n",
      "|    n_updates            | 3890        |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    value_loss           | 0.563       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=797500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 797500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=798000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 798000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=798500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 798500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 69.3     |\n",
      "|    ep_rew_mean     | 4.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 390      |\n",
      "|    time_elapsed    | 26625    |\n",
      "|    total_timesteps | 798720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=799000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 799000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019433957 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0263      |\n",
      "|    n_updates            | 3900        |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    value_loss           | 1.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=799500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 799500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 71       |\n",
      "|    ep_rew_mean     | 4.09     |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 391      |\n",
      "|    time_elapsed    | 26684    |\n",
      "|    total_timesteps | 800768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=801000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 801000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024796959 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.161       |\n",
      "|    n_updates            | 3910        |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    value_loss           | 0.893       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=801500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 801500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=802000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 802000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=802500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 802500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.3     |\n",
      "|    ep_rew_mean     | 3.29     |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 392      |\n",
      "|    time_elapsed    | 26744    |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=803000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 803000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024349585 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.154       |\n",
      "|    n_updates            | 3920        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    value_loss           | 1.1         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=803500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 803500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=804000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 804000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=804500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 804500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 78       |\n",
      "|    ep_rew_mean     | 2.08     |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 393      |\n",
      "|    time_elapsed    | 26820    |\n",
      "|    total_timesteps | 804864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=805000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 805000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02374111 |\n",
      "|    clip_fraction        | 0.237      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.36      |\n",
      "|    explained_variance   | 0.758      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.166      |\n",
      "|    n_updates            | 3930       |\n",
      "|    policy_gradient_loss | -0.0283    |\n",
      "|    value_loss           | 1.04       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=805500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 805500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 806000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 806500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.3     |\n",
      "|    ep_rew_mean     | 0.417    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 394      |\n",
      "|    time_elapsed    | 26880    |\n",
      "|    total_timesteps | 806912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=807000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 807000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024347872 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0739     |\n",
      "|    n_updates            | 3940        |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    value_loss           | 0.64        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=807500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 807500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=808000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 808000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=808500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 808500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88       |\n",
      "|    ep_rew_mean     | -0.83    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 395      |\n",
      "|    time_elapsed    | 26939    |\n",
      "|    total_timesteps | 808960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=809000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 809000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024385547 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.228      |\n",
      "|    n_updates            | 3950        |\n",
      "|    policy_gradient_loss | -0.0481     |\n",
      "|    value_loss           | 0.351       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=809500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 809500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=811000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 811000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.8     |\n",
      "|    ep_rew_mean     | -1.97    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 396      |\n",
      "|    time_elapsed    | 27002    |\n",
      "|    total_timesteps | 811008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=811500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 811500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02587185 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.121     |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.0536    |\n",
      "|    value_loss           | 0.365      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=812000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 812000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=812500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 812500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=813000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 813000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.2     |\n",
      "|    ep_rew_mean     | -1.97    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 397      |\n",
      "|    time_elapsed    | 27063    |\n",
      "|    total_timesteps | 813056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=813500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 813500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019749898 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.115       |\n",
      "|    n_updates            | 3970        |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 0.738       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=814000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 814000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=814500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 814500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=815000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 815000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89       |\n",
      "|    ep_rew_mean     | -2.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 398      |\n",
      "|    time_elapsed    | 27123    |\n",
      "|    total_timesteps | 815104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=815500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 815500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018620223 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00614    |\n",
      "|    n_updates            | 3980        |\n",
      "|    policy_gradient_loss | -0.0357     |\n",
      "|    value_loss           | 0.831       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=816000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 816000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=816500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 816500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=817000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 817000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.6     |\n",
      "|    ep_rew_mean     | -0.671   |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 399      |\n",
      "|    time_elapsed    | 27183    |\n",
      "|    total_timesteps | 817152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=817500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 817500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025331972 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0823      |\n",
      "|    n_updates            | 3990        |\n",
      "|    policy_gradient_loss | -0.0457     |\n",
      "|    value_loss           | 0.767       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=818000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 818000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=818500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 818500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=819000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 819000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.5     |\n",
      "|    ep_rew_mean     | -0.762   |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 400      |\n",
      "|    time_elapsed    | 27243    |\n",
      "|    total_timesteps | 819200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=819500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 819500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023470644 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.501       |\n",
      "|    n_updates            | 4000        |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 0.724       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=820500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=821000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 821000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86       |\n",
      "|    ep_rew_mean     | -1.17    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 401      |\n",
      "|    time_elapsed    | 27303    |\n",
      "|    total_timesteps | 821248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=821500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 821500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02797847 |\n",
      "|    clip_fraction        | 0.327      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.4       |\n",
      "|    explained_variance   | 0.858      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.193     |\n",
      "|    n_updates            | 4010       |\n",
      "|    policy_gradient_loss | -0.0461    |\n",
      "|    value_loss           | 0.356      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=822000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 822000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=822500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 822500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=823000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 823000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89       |\n",
      "|    ep_rew_mean     | -2.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 402      |\n",
      "|    time_elapsed    | 27363    |\n",
      "|    total_timesteps | 823296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=823500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 823500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022560246 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.222      |\n",
      "|    n_updates            | 4020        |\n",
      "|    policy_gradient_loss | -0.0555     |\n",
      "|    value_loss           | 0.221       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=824000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 824000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=824500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 824500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=825000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 825000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.5     |\n",
      "|    ep_rew_mean     | -3.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 403      |\n",
      "|    time_elapsed    | 27439    |\n",
      "|    total_timesteps | 825344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=825500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 825500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01946998 |\n",
      "|    clip_fraction        | 0.256      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.38      |\n",
      "|    explained_variance   | 0.882      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.148     |\n",
      "|    n_updates            | 4030       |\n",
      "|    policy_gradient_loss | -0.0427    |\n",
      "|    value_loss           | 0.43       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=826000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 826000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 826500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=827000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 827000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.2     |\n",
      "|    ep_rew_mean     | -3.58    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 404      |\n",
      "|    time_elapsed    | 27500    |\n",
      "|    total_timesteps | 827392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=827500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 827500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021295037 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0368     |\n",
      "|    n_updates            | 4040        |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 0.51        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=828000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 828000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=828500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 828500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=829000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 829000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.1     |\n",
      "|    ep_rew_mean     | -3.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 405      |\n",
      "|    time_elapsed    | 27560    |\n",
      "|    total_timesteps | 829440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=829500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 829500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021981359 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.73        |\n",
      "|    n_updates            | 4050        |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    value_loss           | 0.72        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 830000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 830500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=831000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 831000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.5     |\n",
      "|    ep_rew_mean     | -2.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 406      |\n",
      "|    time_elapsed    | 27620    |\n",
      "|    total_timesteps | 831488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=831500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 831500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021912811 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.069      |\n",
      "|    n_updates            | 4060        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    value_loss           | 0.882       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=832000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 832000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=832500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 832500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=833000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 833000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=833500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 833500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.9     |\n",
      "|    ep_rew_mean     | -3.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 407      |\n",
      "|    time_elapsed    | 27683    |\n",
      "|    total_timesteps | 833536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=834000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 834000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023575887 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.556       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.161       |\n",
      "|    n_updates            | 4070        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    value_loss           | 0.738       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=834500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 834500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=835000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 835000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=835500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 835500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.3     |\n",
      "|    ep_rew_mean     | -4.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 408      |\n",
      "|    time_elapsed    | 27759    |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=836000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 836000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02758541 |\n",
      "|    clip_fraction        | 0.331      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.46      |\n",
      "|    explained_variance   | -0.386     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.308     |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.0315    |\n",
      "|    value_loss           | 0.12       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=836500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 836500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=837000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 837000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=837500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 837500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.2     |\n",
      "|    ep_rew_mean     | -5.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 409      |\n",
      "|    time_elapsed    | 27836    |\n",
      "|    total_timesteps | 837632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=838000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 838000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031117298 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.383       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.321      |\n",
      "|    n_updates            | 4090        |\n",
      "|    policy_gradient_loss | -0.0489     |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=838500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 838500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=839000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 839000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=839500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 839500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.2     |\n",
      "|    ep_rew_mean     | -6.79    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 410      |\n",
      "|    time_elapsed    | 27911    |\n",
      "|    total_timesteps | 839680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 840000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027314568 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.332      |\n",
      "|    n_updates            | 4100        |\n",
      "|    policy_gradient_loss | -0.0648     |\n",
      "|    value_loss           | 0.0876      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=840500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 840500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=841000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 841000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=841500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 841500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.3     |\n",
      "|    ep_rew_mean     | -7.19    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 411      |\n",
      "|    time_elapsed    | 27986    |\n",
      "|    total_timesteps | 841728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=842000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 842000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023698669 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.304      |\n",
      "|    n_updates            | 4110        |\n",
      "|    policy_gradient_loss | -0.0568     |\n",
      "|    value_loss           | 0.177       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=842500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 842500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=843000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 843000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=843500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 843500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.5     |\n",
      "|    ep_rew_mean     | -7.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 412      |\n",
      "|    time_elapsed    | 28062    |\n",
      "|    total_timesteps | 843776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=844000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 844000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021479847 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.241      |\n",
      "|    n_updates            | 4120        |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    value_loss           | 0.396       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=844500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 844500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=845000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 845000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=845500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 845500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.6     |\n",
      "|    ep_rew_mean     | -6.87    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 413      |\n",
      "|    time_elapsed    | 28138    |\n",
      "|    total_timesteps | 845824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=846000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 846000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024160324 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0628     |\n",
      "|    n_updates            | 4130        |\n",
      "|    policy_gradient_loss | -0.0477     |\n",
      "|    value_loss           | 0.399       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=846500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 846500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=847000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 847000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=847500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 847500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98       |\n",
      "|    ep_rew_mean     | -6.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 414      |\n",
      "|    time_elapsed    | 28213    |\n",
      "|    total_timesteps | 847872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=848000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 848000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020207755 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.653       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.14        |\n",
      "|    n_updates            | 4140        |\n",
      "|    policy_gradient_loss | -0.0393     |\n",
      "|    value_loss           | 0.601       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=848500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 848500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=849000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 849000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=849500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 849500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.5     |\n",
      "|    ep_rew_mean     | -5.22    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 415      |\n",
      "|    time_elapsed    | 28272    |\n",
      "|    total_timesteps | 849920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 850000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018296808 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.252      |\n",
      "|    n_updates            | 4150        |\n",
      "|    policy_gradient_loss | -0.0395     |\n",
      "|    value_loss           | 0.52        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=850500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 850500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=851000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 851000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=851500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 851500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.9     |\n",
      "|    ep_rew_mean     | -4.86    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 416      |\n",
      "|    time_elapsed    | 28335    |\n",
      "|    total_timesteps | 851968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=852000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 852000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021669785 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.234      |\n",
      "|    n_updates            | 4160        |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    value_loss           | 0.313       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=852500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 852500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=853000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 853000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=853500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 853500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=854000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 854000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.5     |\n",
      "|    ep_rew_mean     | -3.92    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 417      |\n",
      "|    time_elapsed    | 28398    |\n",
      "|    total_timesteps | 854016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=854500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 854500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021733351 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.265      |\n",
      "|    n_updates            | 4170        |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    value_loss           | 0.479       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=855000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 855000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=855500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 855500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=856000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 856000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.9     |\n",
      "|    ep_rew_mean     | -2.23    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 418      |\n",
      "|    time_elapsed    | 28458    |\n",
      "|    total_timesteps | 856064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=856500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 856500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025394987 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.192      |\n",
      "|    n_updates            | 4180        |\n",
      "|    policy_gradient_loss | -0.0585     |\n",
      "|    value_loss           | 0.395       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=857000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 857000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=857500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 857500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=858000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 858000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87       |\n",
      "|    ep_rew_mean     | -0.88    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 419      |\n",
      "|    time_elapsed    | 28519    |\n",
      "|    total_timesteps | 858112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=858500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 858500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019762468 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.146      |\n",
      "|    n_updates            | 4190        |\n",
      "|    policy_gradient_loss | -0.0474     |\n",
      "|    value_loss           | 0.544       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=859000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 859000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=859500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 859500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 860000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 80       |\n",
      "|    ep_rew_mean     | 1.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 420      |\n",
      "|    time_elapsed    | 28578    |\n",
      "|    total_timesteps | 860160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 860500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020876745 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0287     |\n",
      "|    n_updates            | 4200        |\n",
      "|    policy_gradient_loss | -0.0395     |\n",
      "|    value_loss           | 0.638       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=861000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 861000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=861500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 861500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=862000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 862000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81       |\n",
      "|    ep_rew_mean     | 1.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 421      |\n",
      "|    time_elapsed    | 28637    |\n",
      "|    total_timesteps | 862208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=862500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 862500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026044361 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0285     |\n",
      "|    n_updates            | 4210        |\n",
      "|    policy_gradient_loss | -0.0351     |\n",
      "|    value_loss           | 0.657       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=863000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 863000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=863500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 863500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=864000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 864000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 82       |\n",
      "|    ep_rew_mean     | 1.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 422      |\n",
      "|    time_elapsed    | 28697    |\n",
      "|    total_timesteps | 864256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=864500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 864500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025374051 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0732     |\n",
      "|    n_updates            | 4220        |\n",
      "|    policy_gradient_loss | -0.0347     |\n",
      "|    value_loss           | 0.563       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=865000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 865000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=865500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 865500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 866000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 82.5     |\n",
      "|    ep_rew_mean     | 0.895    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 423      |\n",
      "|    time_elapsed    | 28757    |\n",
      "|    total_timesteps | 866304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 866500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02438879 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.35      |\n",
      "|    explained_variance   | 0.894      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0713    |\n",
      "|    n_updates            | 4230       |\n",
      "|    policy_gradient_loss | -0.0394    |\n",
      "|    value_loss           | 0.567      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=867000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 867000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=867500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 867500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=868000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 868000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.4     |\n",
      "|    ep_rew_mean     | -1.23    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 424      |\n",
      "|    time_elapsed    | 28816    |\n",
      "|    total_timesteps | 868352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=868500, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -9          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 868500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028518822 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.332      |\n",
      "|    n_updates            | 4240        |\n",
      "|    policy_gradient_loss | -0.0616     |\n",
      "|    value_loss           | 0.231       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=869000, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -9       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 869000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=869500, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -9       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 869500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -9       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 870000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.2     |\n",
      "|    ep_rew_mean     | -2.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 425      |\n",
      "|    time_elapsed    | 28891    |\n",
      "|    total_timesteps | 870400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 870500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02765836 |\n",
      "|    clip_fraction        | 0.359      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.46      |\n",
      "|    explained_variance   | 0.844      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.23      |\n",
      "|    n_updates            | 4250       |\n",
      "|    policy_gradient_loss | -0.0603    |\n",
      "|    value_loss           | 0.263      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=871000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 871000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=871500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 871500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=872000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 872000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91       |\n",
      "|    ep_rew_mean     | -3.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 426      |\n",
      "|    time_elapsed    | 28968    |\n",
      "|    total_timesteps | 872448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=872500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 872500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023446884 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.282      |\n",
      "|    n_updates            | 4260        |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    value_loss           | 0.429       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=873000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 873000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=873500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 873500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=874000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 874000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.4     |\n",
      "|    ep_rew_mean     | -3.91    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 427      |\n",
      "|    time_elapsed    | 29028    |\n",
      "|    total_timesteps | 874496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=874500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 874500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023452677 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.258      |\n",
      "|    n_updates            | 4270        |\n",
      "|    policy_gradient_loss | -0.0492     |\n",
      "|    value_loss           | 0.366       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=875000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 875000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=875500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 875500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=876000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 876000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=876500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 876500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.3     |\n",
      "|    ep_rew_mean     | -3.21    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 428      |\n",
      "|    time_elapsed    | 29089    |\n",
      "|    total_timesteps | 876544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=877000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 877000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022009632 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.291       |\n",
      "|    n_updates            | 4280        |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    value_loss           | 0.677       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=877500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 877500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=878000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 878000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=878500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 878500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.6     |\n",
      "|    ep_rew_mean     | -1.51    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 429      |\n",
      "|    time_elapsed    | 29146    |\n",
      "|    total_timesteps | 878592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=879000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 879000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018550994 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.346       |\n",
      "|    n_updates            | 4290        |\n",
      "|    policy_gradient_loss | -0.0351     |\n",
      "|    value_loss           | 1.12        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=879500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 879500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 85.7     |\n",
      "|    ep_rew_mean     | -0.213   |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 430      |\n",
      "|    time_elapsed    | 29204    |\n",
      "|    total_timesteps | 880640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=881000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 881000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018426146 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.286       |\n",
      "|    n_updates            | 4300        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    value_loss           | 1.18        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=881500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 881500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=882000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 882000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=882500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 882500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.7     |\n",
      "|    ep_rew_mean     | 0.553    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 431      |\n",
      "|    time_elapsed    | 29263    |\n",
      "|    total_timesteps | 882688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=883000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 883000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023467451 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0161      |\n",
      "|    n_updates            | 4310        |\n",
      "|    policy_gradient_loss | -0.0359     |\n",
      "|    value_loss           | 0.813       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=883500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 883500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=884000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 884000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=884500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 884500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.5     |\n",
      "|    ep_rew_mean     | -0.344   |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 432      |\n",
      "|    time_elapsed    | 29322    |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=885000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 885000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020838188 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.186      |\n",
      "|    n_updates            | 4320        |\n",
      "|    policy_gradient_loss | -0.0383     |\n",
      "|    value_loss           | 0.554       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=885500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 885500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=886000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 886000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=886500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 886500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.7     |\n",
      "|    ep_rew_mean     | -1.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 433      |\n",
      "|    time_elapsed    | 29382    |\n",
      "|    total_timesteps | 886784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=887000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 887000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02700304 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.47      |\n",
      "|    explained_variance   | 0.85       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.309     |\n",
      "|    n_updates            | 4330       |\n",
      "|    policy_gradient_loss | -0.0578    |\n",
      "|    value_loss           | 0.139      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=887500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 887500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=888000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 888000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=888500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 888500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.7     |\n",
      "|    ep_rew_mean     | -2.81    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 434      |\n",
      "|    time_elapsed    | 29441    |\n",
      "|    total_timesteps | 888832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=889000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 889000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024025664 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.275      |\n",
      "|    n_updates            | 4340        |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 0.237       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=889500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 889500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=890500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.3     |\n",
      "|    ep_rew_mean     | -3.55    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 435      |\n",
      "|    time_elapsed    | 29500    |\n",
      "|    total_timesteps | 890880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=891000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 891000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021809978 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.1         |\n",
      "|    n_updates            | 4350        |\n",
      "|    policy_gradient_loss | -0.0384     |\n",
      "|    value_loss           | 0.73        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=891500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 891500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=892000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 892000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=892500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 892500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.7     |\n",
      "|    ep_rew_mean     | -4.59    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 436      |\n",
      "|    time_elapsed    | 29559    |\n",
      "|    total_timesteps | 892928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=893000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 33        |\n",
      "|    mean_reward          | 10.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 893000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0157646 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.47     |\n",
      "|    explained_variance   | 0.495     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.28      |\n",
      "|    n_updates            | 4360      |\n",
      "|    policy_gradient_loss | -0.0311   |\n",
      "|    value_loss           | 0.842     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=893500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 893500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=894000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 894000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=894500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 894500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.5     |\n",
      "|    ep_rew_mean     | -5.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 437      |\n",
      "|    time_elapsed    | 29618    |\n",
      "|    total_timesteps | 894976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=895000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 895000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024307046 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0392     |\n",
      "|    n_updates            | 4370        |\n",
      "|    policy_gradient_loss | -0.0483     |\n",
      "|    value_loss           | 0.594       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=895500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 895500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=896000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 896000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=896500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 896500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=897000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 897000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.4     |\n",
      "|    ep_rew_mean     | -5.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 438      |\n",
      "|    time_elapsed    | 29679    |\n",
      "|    total_timesteps | 897024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=897500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 897500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028375551 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.138      |\n",
      "|    n_updates            | 4380        |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 0.28        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=898000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 898000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=898500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 898500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=899000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 899000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.4     |\n",
      "|    ep_rew_mean     | -5.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 439      |\n",
      "|    time_elapsed    | 29755    |\n",
      "|    total_timesteps | 899072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=899500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 899500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025003852 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.232      |\n",
      "|    n_updates            | 4390        |\n",
      "|    policy_gradient_loss | -0.0517     |\n",
      "|    value_loss           | 0.235       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=901000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 901000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.6     |\n",
      "|    ep_rew_mean     | -7       |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 440      |\n",
      "|    time_elapsed    | 29830    |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=901500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 901500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027540583 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.263       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.304      |\n",
      "|    n_updates            | 4400        |\n",
      "|    policy_gradient_loss | -0.0628     |\n",
      "|    value_loss           | 0.0976      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=902000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 902000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=902500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 902500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=903000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 903000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.3     |\n",
      "|    ep_rew_mean     | -6.68    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 441      |\n",
      "|    time_elapsed    | 29904    |\n",
      "|    total_timesteps | 903168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=903500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 903500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023020994 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0646      |\n",
      "|    n_updates            | 4410        |\n",
      "|    policy_gradient_loss | -0.0358     |\n",
      "|    value_loss           | 0.409       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=904000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 904000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=904500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 904500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=905000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 905000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.1     |\n",
      "|    ep_rew_mean     | -6.37    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 442      |\n",
      "|    time_elapsed    | 29963    |\n",
      "|    total_timesteps | 905216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=905500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 905500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023258556 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.248      |\n",
      "|    n_updates            | 4420        |\n",
      "|    policy_gradient_loss | -0.0511     |\n",
      "|    value_loss           | 0.353       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=906000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 906000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=906500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 906500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 907000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.5     |\n",
      "|    ep_rew_mean     | -6.12    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 443      |\n",
      "|    time_elapsed    | 30022    |\n",
      "|    total_timesteps | 907264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 907500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022972919 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.192      |\n",
      "|    n_updates            | 4430        |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 0.397       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=908000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 908000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=908500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 908500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=909000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 909000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.8     |\n",
      "|    ep_rew_mean     | -4.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 444      |\n",
      "|    time_elapsed    | 30081    |\n",
      "|    total_timesteps | 909312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=909500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 909500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025899047 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.133      |\n",
      "|    n_updates            | 4440        |\n",
      "|    policy_gradient_loss | -0.0494     |\n",
      "|    value_loss           | 0.359       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 910000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 910500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=911000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 911000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.7     |\n",
      "|    ep_rew_mean     | -4.49    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 445      |\n",
      "|    time_elapsed    | 30141    |\n",
      "|    total_timesteps | 911360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=911500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 911500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02221957 |\n",
      "|    clip_fraction        | 0.265      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.44      |\n",
      "|    explained_variance   | 0.813      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.145     |\n",
      "|    n_updates            | 4450       |\n",
      "|    policy_gradient_loss | -0.0473    |\n",
      "|    value_loss           | 0.398      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=912000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 912000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=912500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 912500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=913000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 913000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.6     |\n",
      "|    ep_rew_mean     | -3.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 446      |\n",
      "|    time_elapsed    | 30200    |\n",
      "|    total_timesteps | 913408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=913500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 913500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019131288 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.038       |\n",
      "|    n_updates            | 4460        |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    value_loss           | 0.994       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=914000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 914000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=914500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 914500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=915000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 915000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.6     |\n",
      "|    ep_rew_mean     | -1.45    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 447      |\n",
      "|    time_elapsed    | 30259    |\n",
      "|    total_timesteps | 915456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=915500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 915500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019664168 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.173       |\n",
      "|    n_updates            | 4470        |\n",
      "|    policy_gradient_loss | -0.0349     |\n",
      "|    value_loss           | 1.06        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=916000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 916000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=916500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 916500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=917000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 917000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=917500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 917500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.4     |\n",
      "|    ep_rew_mean     | -0.454   |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 448      |\n",
      "|    time_elapsed    | 30321    |\n",
      "|    total_timesteps | 917504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=918000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 918000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024615638 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.234      |\n",
      "|    n_updates            | 4480        |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    value_loss           | 0.468       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=918500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 918500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=919000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 919000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=919500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 919500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.3     |\n",
      "|    ep_rew_mean     | 0.334    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 449      |\n",
      "|    time_elapsed    | 30380    |\n",
      "|    total_timesteps | 919552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 920000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025863718 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0946     |\n",
      "|    n_updates            | 4490        |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 0.408       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=920500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 920500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=921000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 921000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=921500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 921500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.5     |\n",
      "|    ep_rew_mean     | 0.329    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 450      |\n",
      "|    time_elapsed    | 30439    |\n",
      "|    total_timesteps | 921600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=922000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 922000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026941754 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.21       |\n",
      "|    n_updates            | 4500        |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 0.291       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=922500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 922500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=923000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 923000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=923500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 923500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 85.2     |\n",
      "|    ep_rew_mean     | -0.555   |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 451      |\n",
      "|    time_elapsed    | 30499    |\n",
      "|    total_timesteps | 923648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=924000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 924000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023218159 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.748       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.138       |\n",
      "|    n_updates            | 4510        |\n",
      "|    policy_gradient_loss | -0.037      |\n",
      "|    value_loss           | 0.943       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=924500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 924500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=925000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 925000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=925500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 925500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.9     |\n",
      "|    ep_rew_mean     | -1.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 452      |\n",
      "|    time_elapsed    | 30558    |\n",
      "|    total_timesteps | 925696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=926000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 926000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02156159 |\n",
      "|    clip_fraction        | 0.247      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.726      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0593    |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.0364    |\n",
      "|    value_loss           | 0.566      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=926500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 926500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=927000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 927000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=927500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 927500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.3     |\n",
      "|    ep_rew_mean     | -2.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 453      |\n",
      "|    time_elapsed    | 30617    |\n",
      "|    total_timesteps | 927744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=928000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 928000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021413827 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.101      |\n",
      "|    n_updates            | 4530        |\n",
      "|    policy_gradient_loss | -0.0386     |\n",
      "|    value_loss           | 0.561       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=928500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 928500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=929000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 929000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=929500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 929500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.1     |\n",
      "|    ep_rew_mean     | -2.97    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 454      |\n",
      "|    time_elapsed    | 30676    |\n",
      "|    total_timesteps | 929792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 930000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022357095 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.748       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.181      |\n",
      "|    n_updates            | 4540        |\n",
      "|    policy_gradient_loss | -0.0374     |\n",
      "|    value_loss           | 0.672       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=930500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 930500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=931000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 931000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=931500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 931500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.2     |\n",
      "|    ep_rew_mean     | -3.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 455      |\n",
      "|    time_elapsed    | 30751    |\n",
      "|    total_timesteps | 931840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=932000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 932000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02252749 |\n",
      "|    clip_fraction        | 0.235      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.39      |\n",
      "|    explained_variance   | 0.747      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.16      |\n",
      "|    n_updates            | 4550       |\n",
      "|    policy_gradient_loss | -0.0308    |\n",
      "|    value_loss           | 1.02       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=932500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 932500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=933000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 933000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=933500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 933500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92       |\n",
      "|    ep_rew_mean     | -2.99    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 456      |\n",
      "|    time_elapsed    | 30830    |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=934000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 934000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02799185 |\n",
      "|    clip_fraction        | 0.369      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.44      |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.23      |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.0539    |\n",
      "|    value_loss           | 0.216      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=934500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 934500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=935000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 935000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=935500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 935500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93       |\n",
      "|    ep_rew_mean     | -3.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 457      |\n",
      "|    time_elapsed    | 30890    |\n",
      "|    total_timesteps | 935936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=936000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 936000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027953975 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.298      |\n",
      "|    n_updates            | 4570        |\n",
      "|    policy_gradient_loss | -0.0613     |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=936500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 936500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=937000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 937000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=937500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 937500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.8     |\n",
      "|    ep_rew_mean     | -4.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 458      |\n",
      "|    time_elapsed    | 30966    |\n",
      "|    total_timesteps | 937984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=938000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 938000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027169125 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.334      |\n",
      "|    n_updates            | 4580        |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=938500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 938500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=939000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 939000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=939500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 939500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 940000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95       |\n",
      "|    ep_rew_mean     | -4.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 459      |\n",
      "|    time_elapsed    | 31049    |\n",
      "|    total_timesteps | 940032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 940500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02471523 |\n",
      "|    clip_fraction        | 0.277      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | 0.76       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.113     |\n",
      "|    n_updates            | 4590       |\n",
      "|    policy_gradient_loss | -0.0453    |\n",
      "|    value_loss           | 0.585      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=941000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 941000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=941500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 941500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=942000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 942000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.3     |\n",
      "|    ep_rew_mean     | -5.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 460      |\n",
      "|    time_elapsed    | 31109    |\n",
      "|    total_timesteps | 942080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=942500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 942500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020277683 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.228      |\n",
      "|    n_updates            | 4600        |\n",
      "|    policy_gradient_loss | -0.0336     |\n",
      "|    value_loss           | 0.509       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=943000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 943000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=943500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 943500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=944000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 944000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.2     |\n",
      "|    ep_rew_mean     | -3.56    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 461      |\n",
      "|    time_elapsed    | 31169    |\n",
      "|    total_timesteps | 944128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=944500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 944500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02029354 |\n",
      "|    clip_fraction        | 0.228      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.33      |\n",
      "|    explained_variance   | 0.902      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0268    |\n",
      "|    n_updates            | 4610       |\n",
      "|    policy_gradient_loss | -0.0483    |\n",
      "|    value_loss           | 0.479      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=945000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 945000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=945500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 945500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=946000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 946000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.7     |\n",
      "|    ep_rew_mean     | -2.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 462      |\n",
      "|    time_elapsed    | 31229    |\n",
      "|    total_timesteps | 946176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=946500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 946500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02294446 |\n",
      "|    clip_fraction        | 0.273      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.35      |\n",
      "|    explained_variance   | 0.885      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.113     |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.049     |\n",
      "|    value_loss           | 0.463      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=947000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 947000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 947500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=948000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 948000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.7     |\n",
      "|    ep_rew_mean     | -2.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 463      |\n",
      "|    time_elapsed    | 31288    |\n",
      "|    total_timesteps | 948224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=948500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 948500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020865595 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0556     |\n",
      "|    n_updates            | 4630        |\n",
      "|    policy_gradient_loss | -0.0486     |\n",
      "|    value_loss           | 0.368       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=949000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 949000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=949500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 949500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 950000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.1     |\n",
      "|    ep_rew_mean     | -1.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 464      |\n",
      "|    time_elapsed    | 31349    |\n",
      "|    total_timesteps | 950272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 950500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024449345 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.199      |\n",
      "|    n_updates            | 4640        |\n",
      "|    policy_gradient_loss | -0.0551     |\n",
      "|    value_loss           | 0.229       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=951000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 951000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=951500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 951500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=952000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 952000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89       |\n",
      "|    ep_rew_mean     | -1.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 465      |\n",
      "|    time_elapsed    | 31408    |\n",
      "|    total_timesteps | 952320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=952500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 952500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026263827 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.674       |\n",
      "|    n_updates            | 4650        |\n",
      "|    policy_gradient_loss | -0.0491     |\n",
      "|    value_loss           | 0.534       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=953000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 953000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=953500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 953500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=954000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 954000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.1     |\n",
      "|    ep_rew_mean     | -1.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 466      |\n",
      "|    time_elapsed    | 31469    |\n",
      "|    total_timesteps | 954368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=954500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 954500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020009067 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.768       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.145      |\n",
      "|    n_updates            | 4660        |\n",
      "|    policy_gradient_loss | -0.0328     |\n",
      "|    value_loss           | 1           |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=955000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 955000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=955500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 955500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=956000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 956000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.1     |\n",
      "|    ep_rew_mean     | -1.23    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 467      |\n",
      "|    time_elapsed    | 31529    |\n",
      "|    total_timesteps | 956416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=956500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 956500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025234338 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.505       |\n",
      "|    n_updates            | 4670        |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    value_loss           | 0.894       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=957000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 957000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=957500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 957500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=958000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 958000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.5     |\n",
      "|    ep_rew_mean     | -1.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 468      |\n",
      "|    time_elapsed    | 31589    |\n",
      "|    total_timesteps | 958464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=958500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 33         |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 958500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01935239 |\n",
      "|    clip_fraction        | 0.237      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | 0.806      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0414     |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.0354    |\n",
      "|    value_loss           | 0.737      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=959000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 959000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=959500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 959500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89       |\n",
      "|    ep_rew_mean     | -1.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 469      |\n",
      "|    time_elapsed    | 31651    |\n",
      "|    total_timesteps | 960512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=961000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 961000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020365704 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.16       |\n",
      "|    n_updates            | 4690        |\n",
      "|    policy_gradient_loss | -0.0355     |\n",
      "|    value_loss           | 0.918       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=961500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 961500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=962000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 962000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=962500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 962500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.6     |\n",
      "|    ep_rew_mean     | -2.23    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 470      |\n",
      "|    time_elapsed    | 31727    |\n",
      "|    total_timesteps | 962560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=963000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 963000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023225524 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.056       |\n",
      "|    n_updates            | 4700        |\n",
      "|    policy_gradient_loss | -0.0355     |\n",
      "|    value_loss           | 0.725       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=963500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 963500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=964000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 964000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=964500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 964500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.3     |\n",
      "|    ep_rew_mean     | -3.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 471      |\n",
      "|    time_elapsed    | 31787    |\n",
      "|    total_timesteps | 964608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=965000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 965000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025979016 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.335      |\n",
      "|    n_updates            | 4710        |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=965500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 965500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=966000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 966000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=966500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 966500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.4     |\n",
      "|    ep_rew_mean     | -4.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 472      |\n",
      "|    time_elapsed    | 31863    |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=967000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33          |\n",
      "|    mean_reward          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 967000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028093094 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.316      |\n",
      "|    n_updates            | 4720        |\n",
      "|    policy_gradient_loss | -0.0566     |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=967500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 967500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=968000, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 968000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=968500, episode_reward=10.70 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 10.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 968500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.9     |\n",
      "|    ep_rew_mean     | -5.61    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 473      |\n",
      "|    time_elapsed    | 31923    |\n",
      "|    total_timesteps | 968704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=969000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 969000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021673813 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.34        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.286      |\n",
      "|    n_updates            | 4730        |\n",
      "|    policy_gradient_loss | -0.0466     |\n",
      "|    value_loss           | 0.25        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=969500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 969500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.6     |\n",
      "|    ep_rew_mean     | -6.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 474      |\n",
      "|    time_elapsed    | 32000    |\n",
      "|    total_timesteps | 970752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=971000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 971000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02477919 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.49      |\n",
      "|    explained_variance   | 0.243      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.307     |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.0473    |\n",
      "|    value_loss           | 0.239      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=971500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 971500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=972000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 972000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=972500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 972500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.4     |\n",
      "|    ep_rew_mean     | -6.97    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 475      |\n",
      "|    time_elapsed    | 32076    |\n",
      "|    total_timesteps | 972800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=973000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 973000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021618137 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.468       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.279      |\n",
      "|    n_updates            | 4750        |\n",
      "|    policy_gradient_loss | -0.0408     |\n",
      "|    value_loss           | 0.468       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=973500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 973500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=974000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 974000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=974500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 974500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.7     |\n",
      "|    ep_rew_mean     | -7.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 476      |\n",
      "|    time_elapsed    | 32152    |\n",
      "|    total_timesteps | 974848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=975000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 975000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029488482 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.589       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.332      |\n",
      "|    n_updates            | 4760        |\n",
      "|    policy_gradient_loss | -0.0605     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=975500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 975500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=976000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 976000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=976500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 976500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.8     |\n",
      "|    ep_rew_mean     | -7.17    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 477      |\n",
      "|    time_elapsed    | 32228    |\n",
      "|    total_timesteps | 976896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=977000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 977000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020959137 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.601       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.269      |\n",
      "|    n_updates            | 4770        |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=977500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 977500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=978000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 978000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=978500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 978500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.4     |\n",
      "|    ep_rew_mean     | -6.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 478      |\n",
      "|    time_elapsed    | 32305    |\n",
      "|    total_timesteps | 978944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=979000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 979000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02084621 |\n",
      "|    clip_fraction        | 0.273      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.787      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.3       |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.0463    |\n",
      "|    value_loss           | 0.206      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=979500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 979500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.6     |\n",
      "|    ep_rew_mean     | -6.51    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 479      |\n",
      "|    time_elapsed    | 32382    |\n",
      "|    total_timesteps | 980992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=981000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 981000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020075973 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.259      |\n",
      "|    n_updates            | 4790        |\n",
      "|    policy_gradient_loss | -0.0395     |\n",
      "|    value_loss           | 0.342       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=981500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 981500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=982000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 982000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=982500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 982500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=983000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 983000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.7     |\n",
      "|    ep_rew_mean     | -6.57    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 480      |\n",
      "|    time_elapsed    | 32464    |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=983500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 983500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022078378 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.485       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.26       |\n",
      "|    n_updates            | 4800        |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=984000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 984000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=984500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 984500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=985000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 985000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.1     |\n",
      "|    ep_rew_mean     | -6.81    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 481      |\n",
      "|    time_elapsed    | 32541    |\n",
      "|    total_timesteps | 985088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=985500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 985500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025663536 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.347      |\n",
      "|    n_updates            | 4810        |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 0.102       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=986000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 986000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=986500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 986500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 987000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.8     |\n",
      "|    ep_rew_mean     | -6.63    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 482      |\n",
      "|    time_elapsed    | 32617    |\n",
      "|    total_timesteps | 987136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 987500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02095106 |\n",
      "|    clip_fraction        | 0.241      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.43      |\n",
      "|    explained_variance   | 0.742      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.226     |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | -0.0468    |\n",
      "|    value_loss           | 0.202      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=988000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 988000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=988500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 988500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=989000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 989000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.7     |\n",
      "|    ep_rew_mean     | -6.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 483      |\n",
      "|    time_elapsed    | 32693    |\n",
      "|    total_timesteps | 989184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=989500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 989500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025025668 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.666       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.106      |\n",
      "|    n_updates            | 4830        |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    value_loss           | 0.343       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 990000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 990500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=991000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 991000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.6     |\n",
      "|    ep_rew_mean     | -6.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 484      |\n",
      "|    time_elapsed    | 32769    |\n",
      "|    total_timesteps | 991232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=991500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 991500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021830272 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.638       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0467     |\n",
      "|    n_updates            | 4840        |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    value_loss           | 0.536       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=992000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 992000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=992500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 992500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=993000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 993000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.3     |\n",
      "|    ep_rew_mean     | -6.68    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 485      |\n",
      "|    time_elapsed    | 32846    |\n",
      "|    total_timesteps | 993280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=993500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -7         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 993500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02179978 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.46      |\n",
      "|    explained_variance   | 0.691      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.313     |\n",
      "|    n_updates            | 4850       |\n",
      "|    policy_gradient_loss | -0.0515    |\n",
      "|    value_loss           | 0.139      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=994000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 994000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=994500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 994500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=995000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 995000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.1     |\n",
      "|    ep_rew_mean     | -6.23    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 486      |\n",
      "|    time_elapsed    | 32922    |\n",
      "|    total_timesteps | 995328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=995500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 995500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023982638 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.234      |\n",
      "|    n_updates            | 4860        |\n",
      "|    policy_gradient_loss | -0.0478     |\n",
      "|    value_loss           | 0.294       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=996000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 996000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=996500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 996500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=997000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 997000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.2     |\n",
      "|    ep_rew_mean     | -5.51    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 487      |\n",
      "|    time_elapsed    | 32999    |\n",
      "|    total_timesteps | 997376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=997500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 997500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022403926 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.255      |\n",
      "|    n_updates            | 4870        |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 0.24        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=998000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 998000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=998500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 998500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=999000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 999000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.3     |\n",
      "|    ep_rew_mean     | -4.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 488      |\n",
      "|    time_elapsed    | 33076    |\n",
      "|    total_timesteps | 999424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=999500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 999500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013576885 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.155       |\n",
      "|    n_updates            | 4880        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 1.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000500, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000500  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1001000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1001000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93       |\n",
      "|    ep_rew_mean     | -4.22    |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 489      |\n",
      "|    time_elapsed    | 33153    |\n",
      "|    total_timesteps | 1001472  |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = SokobanEnvFixated()\n",
    "test_env = SokobanEnvFixated()\n",
    "\n",
    "# Define the model\n",
    "model = PPO(CnnPolicy, \n",
    "            env, \n",
    "            tensorboard_log=\"./tensorboard_logs/\",\n",
    "            gamma = 0.95, # prioritize future actions\n",
    "            ent_coef = 0.2, # prevent premature convergence\n",
    "            seed = 24,\n",
    "            verbose=1)\n",
    "\n",
    "eval_callback = EvalCallback(test_env, \n",
    "                             best_model_save_path='./models/',\n",
    "                             eval_freq=500,\n",
    "                             deterministic=True,\n",
    "                             render=False)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=1000000,\n",
    "            callback=eval_callback,\n",
    "            tb_log_name=\"log_sokoban_ppo_fixated_env\")\n",
    "env.close()\n",
    "\n",
    "# Run TensorBoard\n",
    "# In the terminal, run: tensorboard --logdir=./03_resource/03_DRL/tensorboard_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:                                  \t Total: 33\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwsAAAMLCAYAAAABpgu6AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAewgAAHsIBbtB1PgAAGqJJREFUeJzt3TFuG0uex/HmwpkyJQLmBpNNNJHFK1BQzlByLGZ7hwUVWwqZC9QVJEdzgbnBA5Q4e3EvHOxgLf/0WGpXs9jU55OJQ3b958EW+EW5ULO+7/sOAADglf96/QIAAMAPYgEAAIjEAgAAEIkFAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQiQUAACASCwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACIxAIAABB96ip7nM+60SzXv762WY233qGtP5U5W68/lTlbrz+VOVuvP5U5W68/lTlbrz+VOVuvP5U5W68/lTlbrz+VOSuuv3jqu1rsLAAAAJFYAAAAIrEAAABEYgEAAIjEAgAAEIkFAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQiQUAACASCwAAQCQWAACAaNb3fd9V9Hh/W/NxAADAOyyubrpa7CwAAACRWAAAACKxAAAARGIBAACIxAIAABCJBQAAIBILAABAJBYAAIBILAAAAHu6wXk+G/bB5frnnzerrjkzlTFTGTOVMVMZM5UxUxkzlTFTGTM1n2nxVO/rvZ0FAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQiQUAACASCwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACIxAIAABCJBQAAIJr1fd93FT3e39Z8HAAA8A6Lq5uuFjsLAABAJBYAAIBILAAAAJFYAAAAIrEAAABEYgEAAIjEAgAAEIkFAABgT5eyzWfdaJbrX1/brMZb79DWn8qcrdefypyt15/KnK3Xn8qcrdefypyt15/KnK3Xn8qcrdefypyt15/KnMt66y+e6n29t7MAAABEYgEAAIjEAgAAEIkFAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQiQUAACASCwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACIxAIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAIrEAAABEYgEAAIjEAgAAEIkFAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQiQUAACCa9X3fdxU93t/WfBwAAPAOi6ubrhY7CwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACIxAIAABCJBQAAIBILAADAnm5wns+GfXC5/vnnzaprzkxlzFTGTGXMVMZMZcxUxkxlzFTGTM1nWjzV+3pvZwEAAIjEAgAAEIkFAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQiQUAACASCwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACIZn3f911Fj/e3NR8HAAC8w+LqpqvFzgIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAIrEAAABEYgEAANjTpWzzWTea5frX1zar8dY7tPWnMmfr9acyZ+v1pzJn6/WnMmfr9acyZ+v1pzJn6/WnMmfr9acyZ+v1pzLnst76i6d6X+/tLAAAAJFYAAAAIrEAAABEYgEAAIjEAgAAEIkFAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQiQUAACASCwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACIxAIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAIrEAAABEYgEAAIjEAgAAEIkFAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAERiAQAAiGZ93/ddRY/3tzUfBwAAvMPi6qarxc4CAAAQiQUAACASCwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACIxAIAALCnG5zns2EfXK5//nmz6pozUxkzlTFTGTOVMVMZM5UxUxkzlTFT85kWT/W+3ttZAAAAIrEAAABEYgEAAIjEAgAAEIkFAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQiQUAACASCwAAQCQWAACASCwAAACRWAAAAKJZ3/d9V9Hj/W3NxwEAAO+wuLrparGzAAAARGIBAACIxAIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAA9nQp23zWjWa5/vW1zWq89Q5t/anM2Xr9qczZev2pzNl6/anM2Xr9qczZev2pzNl6/anM2Xr9qczZev2pzLmst/7iqd7XezsLAABAJBYAAIBILAAAAJFYAAAAIrEAAABEYgEAAIjEAgAAEIkFAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQiQUAACASCwAAQCQWAACA6FN+GZiSi+cRH/68GvHhE1h/KnNWXH97Xu1RR83fuzL+PMG02VkAAAAisQAAAERiAQAAiMQCAAAQiQUAACASCwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACIxAIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAolnf931X0eP9bc3HAQUurletR+CIbO/WrUeYBH/vyvjzBPu3uLqp9iw7CwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACIxAIAABCJBQAAIBILAABA9KmrbTPwRsvlus5zajJTGTNNdyZI/B6nJn+exmWmMh9tpis3OAMAACMTCwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACIxAIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAIrEAAABEYgEAAIjEAgAAEIkFAAAgmvV933cVPd7f1nwcUODietV6BI7I9m7deoRJ8PeujD9PsH+Lq5tqz7KzAAAARGIBAACIxAIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAok9dbZsRL6lZrve73qGtP5U5W68/lTlbrw9v8Xucmvx5aq/1nK3Xn8qcy4rru5QNAAAYm1gAAAAisQAAAERiAQAAiMQCAAAQiQUAACASCwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACIxAIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAolnf931X0eN81o1muf71tc1qvPVGXP/i+ffHARjD99YDTMRp6wHgA9qej/jwI/qeuXiq9/XezgIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAIrEAAABEYgEAAIg+5ZcB4G0vJfcEXXbTsPn1pbOvLQYBODx2FgAAgEgsAAAAkVgAAAAisQAAAEQOOAPwfunw8sPug8Pdsmvr9Yw/nDSYA2Ai7CwAAACRWAAAACKxAAAARLO+7/uuosf725qPO1oX1yU3GgHs3/eC97x8KzwP8Nqfez7H8DDs/MXZ590fOx00EPA7tnfr1iNMwuLqptqz7CwAAACRWAAAACKxAAAARGIBAACIXMoGQB2XAy9Aeyi48G3Ew8wAvM3OAgAAEIkFAAAgEgsAAEAkFgAAgD3d4DyfDfvg8tWNfJsDuOF4xJkunqs9CuAwbnAusSk89FxyMHnEA81ucIbDtD0f+MEP9j1z8VTv672dBQAAIBILAABAJBYAAIDIpWwA7M9y9zmGs6/hPa/+ae8PL18Kng3Ab7GzAAAARGIBAACIxAIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAIrEAAABEbnAGoK2Tn398WYX3XIbXHnb8/NbnAChmZwEAAIjEAgAAEIkFAAAgmvV933cVPd7f1nzc0bq4Tv8oF6C97wXvefk28OHpXMHQcwYlzxr47LPPu99zOmB54Pds79atR5iExdVNtWfZWQAAACKxAAAARGIBAACIxAIAABC5lA2AOjZ/fdnam4ZenHa5Y/00g4vbAN7FzgIAABCJBQAAIBILAADAni5lm8+60SzDRRybPV5uVnH9i+ffHweg2aVsXwredHIA5wMGnqM4K7j3yaVssH/b8xEffkTfMxdP9b7e21kAAAAisQAAAERiAQAAiMQCAAAQuZQNgPcrOSh8CJedLQdc3AbAf9hZAAAAIrEAAABEYgEAAIjEAgAAEDngDMC7ldxw3JW8B4CDZmcBAACIxAIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAIrEAAABELmUDPozvrQeYiNPWAwBwMOwsAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQiQUAACASCwAAQCQWAACAyKVsAP/Py6rgTZfdNGx+fensa4tBAJgqOwsAAEAkFgAAgEgsAAAAkVgAAAAiB5wBdh1efth9cLhbdm29nvGHkwZzAHBU7CwAAACRWAAAACKxAAAARLO+7/uuosf725qPO1oX1yU3PwE1fS94z8u3wvMAr/2553MMD8POX5x93v2x00EDAYxve7duPcIkLK5uqj3LzgIAABCJBQAAIBILAABAJBYAAIDIpWwA7z0oXHoB2kPBhW8jHmYGgN9lZwEAAIjEAgAAEIkFAAAgEgsAAMCebnCez4Z9cPnqRr7NAdxwPOJMF8/VHgWMfYNziU3hoeeSg8kjHmh2gzMwZdvzgR/8YN8zF0/1vt7bWQAAACKxAAAARGIBAACIXMoGUMNy9zmGs6/hPa/+yeoPL18Kng0Ae2BnAQAAiMQCAAAQiQUAACASCwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACI3OAMMJaTn398WYX3XIbXHnb8/NbnAKAyOwsAAEAkFgAAgEgsAAAA0azv+76r6PH+tubjjtbFdfrHy8CYvhe85+XbwIencwVDzxmUPGvgs88+737P6YDlAfZhe7duPcIkLK5uqj3LzgIAABCJBQAAIBILAABAJBYAAIDIpWwAu2z++rK1Nw29OO1yx/ppBhe3ATACOwsAAEAkFgAAgEgsAAAAe7qUbT7rRrMMF3Fs9ni5WcX1L55/fxxghEvZvhS86eQAzgcMPEdxVnCfkUvZgEO1PR/x4Uf0PXPxVO/rvZ0FAAAgEgsAAEAkFgAAgEgsAAAAkUvZAN57UPgQLjtbDri4DQDeyc4CAAAQiQUAACASCwAAQCQWAACAyAFngHfecNyVvAcAjoCdBQAAIBILAABAJBYAAIBILAAAAJFYAAAAIrEAAABEYgEAAIjEAgAAELmUbWL+/q+//fTzv//5R3foMx7qnHw8T+cjPnwZbmrbrEZccMT1n397GviP760HmIjT1gPAG+wsAAAAkVgAAAAisQAAAERiAQAAiBxwnrhDOEycZgAAYPrsLAAAAJFYAAAAIrEAAABEzixMzOvzCOm8QOszBC5gA+A9XkruD7zspmHz60tnX1sMAnXYWQAAACKxAAAARGIBAACIxAIAABA54AwAtJUOLz/sPjjcLbu2Xs/4w0mDOWBEdhYAAIBILAAAAJFYAAAAolnf931X0eP9bc3HHa2L65IbaLpBF66VXIpWenFbrWe5qI1DsL1btx7hqH8/QfK94D0v3wrPA7z2557PMTwMO39x9nn3x04HDfTx+D1eZnF109ViZwEAAIjEAgAAEIkFAAAgEgsAAEDkUrYDVnoIear/Xxx6BqD4oHDpBWgPBRe+jXiYGY6NnQUAACASCwAAQCQWAACASCwAAAB7usF5Phv2weWrG/k2B3CD6IgzXTwf94HmIRx4Zmzb84Ef9PsJ9n+Dc4lN4aHnkoPJIx5odoNzPX6Pl1k81ft6b2cBAACIxAIAABCJBQAAIHIpWyMf/XxC4uI2AN5lufscw9nX8J5X/1T8h5cvBc+GD8jOAgAAEIkFAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQucEZADgeJz//+LIK77kMrz3s+Pmtz8GRs7MAAABEYgEAAIjEAgAAEM36vu+7ih7vb2s+7mj99z/+p/UIk/Dvf/7RegSOyPZu3XqESbi4Tv/IG4b5XvCel28DH57OFQw9Z1DyrIHPPvu8+z2nA5b/iPweL7O4uulqsbMAAABEYgEAAIjEAgAAEIkFAAAgcikbAHB4Nn992dqbhl6cdrlj/TSDi9v4AOwsAAAAkVgAAAAisQAAAOzpUrb5rBvNMlzEsdnj5UEV17943v2ev//rb90xc+Ea+7Y9H/HhH+z3E1S9lO1LwZtODuB8wMBzFGcF94i5lK2M3+NlFk/1vt7bWQAAACKxAAAARGIBAACIxAIAABC5lG3iB4DTIeh9Hxx+PYODywC8S8lB4UO47Gw54OI2mDg7CwAAQCQWAACASCwAAACRWAAAACIHnAGApkpuOO5K3gNUZ2cBAACIxAIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAIrEAAABELmVr5HutB/3zj1pPOsgZTkd78nHZno/48GW4CWmzGnHBA1v/A/Ln6QC0nrPi+k/diI7ov9O2G9ER/Xdi/+wsAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQiQUAACASCwAAQCQWAACAyKVsB+yl5L6Sy24aNr++dPa1xSAAAJSyswAAAERiAQAAiMQCAAAQiQUAACBywPmQpcPLD7sPDnfLrq3XM/5w0mAOAAB+i50FAAAgEgsAAEAkFgAAgGjW933fVfR4f1vzcUdrfr37xrWXb4XnAV77c8/nGB6Gnb84+7z7Y6eDBvp4tnfr1iMAAAdicXVT7Vl2FgAAgEgsAAAAkVgAAAAisQAAAEQuZZuay4EXoD0UXPg24mFmAACmx84CAAAQiQUAACASCwAAQCQWAACAPd3gPJ8N++Dy1Q20m903HI9uxJnmz92wG5xLbAoPPZccTB7xQLMbnOvZng/84Af7ezeYmcqYqYyZypipjJnKfLCZFk/1vt7bWQAAACKxAAAARGIBAACIXMp2jJa7zzGcfQ3vefVP5354+VLwbAAAjpKdBQAAIBILAABAJBYAAIBILAAAAJFYAAAAIrEAAABEYgEAAIjEAgAAEIkFAAAgcoPzR3Hy848vq/Cey/Daw46f3/ocAACTZ2cBAACIxAIAABCJBQAAIJr1fd93FT3e39Z83NGaX6dDAz97+Tbw4elcwdBzBiXPGvjss8+733M6YPmPaHu3bj0CAHAgFlc31Z5lZwEAAIjEAgAAEIkFAAAgEgsAAEDkUrap2fz1ZWtvGnpx2uWO9dMMLm4DADgKdhYAAIBILAAAAJFYAAAA9nQp23zWjWYZLp7a7L7c7BDXnz/vfs/Ll4IHnRzA+YCB5yjOCu4Rcylbme35iA8/or93Rz1n6/WnMmfr9acyZ+v1pzJn6/WnMmfr9acy57Le+ounel/v7SwAAACRWAAAACKxAAAARGIBAACIXMp2yEoOCh/CZWfLARe3AQBw8OwsAAAAkVgAAAAisQAAAERiAQAAiBxwPmAlNxx3Je8BAIAB7CwAAACRWAAAACKxAAAARGIBAACIxAIAABCJBQAAIBILAABAJBYAAIDIpWyNnLYeAAAAdrCzAAAARGIBAACIxAIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAIrEAAABEYgEAAIjEAgAAEIkFAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAESzvu/7rqLH+9uajztaF9er1iNwRLZ369YjAAAHYnF1U+1ZdhYAAIBILAAAAJFYAAAAIrEAAABEYgEAAIjEAgAAEIkFAAAgEgsAAEAkFgAAgD3d4DyfDfvg8tUNtJsDuOF4xJkunqs9Crrt+cAPfrC/d4OZqYyZypipjJnKmKnMB5tp8VTv672dBQAAIBILAABAJBYAAIBILAAAAJFYAAAAIrEAAABEYgEAAIjEAgAAEIkFAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQiQUAACCa9X3fdxU93t/WfNzRurhetR6BI7K9W7ceAQA4EIurm2rPsrMAAABEYgEAAIjEAgAAEIkFAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAD2dCnbfNaNZhkuntrs8XKziutfPP/+OPB/tucjPvyI/t4d9Zyt15/KnK3Xn8qcrdefypyt15/KnK3Xn8qcy3rrL57qfb23swAAAERiAQAAiMQCAAAQiQUAACASCwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACIxAIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAIrEAAABEYgEAAIg+5ZcZ2/Z8xIcv17++tlmNuOCBrT+VOVuvDwCwg50FAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQiQUAACASCwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACIxAIAABCJBQAAIJr1fd93FT3e39Z8HAAA8A6Lq5uuFjsLAABAJBYAAIBILAAAAJFYAAAAIrEAAABEYgEAAIjEAgAAEIkFAAAgEgsAAMCebnCez4Z9cLn++efNqmvOTGXMVMZMZcxUxkxlzFTGTGXMVMZMzWdaPNX7em9nAQAAiMQCAAAQiQUAACASCwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACIxAIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAIrEAAABEYgEAAIhmfd/3XUWP97c1HwcAALzD4uqmq8XOAgAAEIkFAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAERiAQAA2NOlbPNZN5rl+tfXNqvx1ju09acyZ+v1pzJn6/WnMmfr9acyZ+v1pzJn6/WnMmfr9acyZ+v1pzJn6/WnMuey3vqLp3pf7+0sAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQiQUAACASCwAAQCQWAACASCwAAACRWAAAACKxAAAARGIBAACIxAIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAolnf933+nwAAgI/MzgIAABCJBQAAIBILAABAJBYAAIBILAAAAJFYAAAAIrEAAABEYgEAAIjEAgAAEIkFAAAgEgsAAEAkFgAAgEgsAAAAkVgAAAAisQAAAERiAQAAiMQCAAAQiQUAACASCwAAQJf8LzxF32FMDdAvAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1280x960 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"./models/model_sokoban_ppo_fixated_env_2\")\n",
    "\n",
    "# Create test enviroment\n",
    "test_env = SokobanEnvFixated()\n",
    "\n",
    "# Run the model in the test environment\n",
    "obs = test_env.reset()\n",
    "done = False\n",
    "actions = []\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic = True)\n",
    "    actions.append(int(action))  # Get action from model\n",
    "    obs, reward, done, info = test_env.step(int(action))  # Apply action\n",
    "\n",
    "# Print actions\n",
    "actionDict = {0:\"\", 1:\"\", 2:\"\", 3:\"\", 4:\"\"}\n",
    "print(\"Actions:\", \" \".join(actionDict.get(action) for action in actions), \"\\t Total:\", len(actions))\n",
    "\n",
    "render_state(test_env)\n",
    "test_env.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
