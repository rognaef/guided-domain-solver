{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym.spaces.discrete import Discrete\n",
    "from gym_sokoban.envs import SokobanEnv\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import CnnPolicy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SokobanEnvFixated(SokobanEnv): \n",
    "    def __init__(self):\n",
    "        SokobanEnv.__init__(self,\n",
    "                            dim_room=(10, 10), \n",
    "                            max_steps=100, \n",
    "                            num_boxes=4, \n",
    "                            num_gen_steps=None, \n",
    "                            reset=False)\n",
    "        self.action_space = Discrete(5) # limit to push actions\n",
    "\n",
    "    def reset(self, second_player=False, render_mode='rgb_array'):\n",
    "\n",
    "        self.room_fixed = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "                                    [0, 1, 1, 1, 1, 2, 1, 1, 1, 0],\n",
    "                                    [0, 1, 1, 1, 2, 1, 1, 1, 1, 0],\n",
    "                                    [0, 1, 1, 1, 0, 1, 1, 2, 1, 0],\n",
    "                                    [0, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "        self.room_state = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "                                    [0, 1, 1, 4, 5, 2, 1, 1, 1, 0],\n",
    "                                    [0, 1, 1, 1, 2, 4, 1, 4, 1, 0],\n",
    "                                    [0, 1, 1, 1, 0, 1, 1, 2, 1, 0],\n",
    "                                    [0, 2, 4, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "        self.box_mapping = {(6, 7): (5, 7), (5, 4): (5, 5), (4, 5): (4, 3), (7, 1): (7, 2)}\n",
    "\n",
    "        self.player_position = np.argwhere(self.room_state == 5)[0]\n",
    "        self.num_env_steps = 0\n",
    "        self.reward_last = 0\n",
    "        self.boxes_on_target = 0\n",
    "\n",
    "        starting_observation = self.render(render_mode)\n",
    "        return starting_observation # Close environment after testing\n",
    "\n",
    "def render_state(env, mode=\"rgb_array\"):\n",
    "    \"\"\"Renders the Sokoban environment as image and displays it.\"\"\"\n",
    "    image = env.render(mode)\n",
    "\n",
    "    plt.figure(dpi=200) \n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roger\\Documents\\Projects\\vm-code-generation-with-knowledge-graph\\venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./tensorboard_logs/log_sokoban_ppo_fixated_env_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roger\\Documents\\Projects\\vm-code-generation-with-knowledge-graph\\venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Roger\\Documents\\Projects\\vm-code-generation-with-knowledge-graph\\venv\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x00000203C7643ED0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x00000203C6B87A50>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n",
      "c:\\Users\\Roger\\Documents\\Projects\\vm-code-generation-with-knowledge-graph\\venv\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -9.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -10        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01097081 |\n",
      "|    clip_fraction        | 0.0985     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.6       |\n",
      "|    explained_variance   | -0.0753    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0351    |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    value_loss           | 0.47       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -9.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 32       |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 127      |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -9          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022563798 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.249       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0362     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 0.0577      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -9       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -9       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -9       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -9.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 206      |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -9          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027243555 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | -0.0565     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0693     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 0.0677      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -9       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -9       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -9       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -9.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 283      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -9          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027855659 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0705     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0497     |\n",
      "|    value_loss           | 0.0887      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -9       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -9       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -9       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -9.41    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 360      |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039446756 |\n",
      "|    clip_fraction        | 0.436       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.158       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0913     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0583     |\n",
      "|    value_loss           | 0.0757      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -9.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 437      |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027002238 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.279       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00216    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0351     |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.88    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 514      |\n",
      "|    total_timesteps | 14336    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038186766 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0526     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.61    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 591      |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029893765 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.34        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0201      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    value_loss           | 0.101       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 668      |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024177652 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0455      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 745      |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025756788 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0243      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.18    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 827      |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 23000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02847346 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.2       |\n",
      "|    explained_variance   | 0.432      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0287    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0223    |\n",
      "|    value_loss           | 0.113      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.11    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 904      |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 25000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01868027 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.12      |\n",
      "|    explained_variance   | 0.476      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0368     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.00857   |\n",
      "|    value_loss           | 0.11       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 981      |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010597869 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.441       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0193      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00623    |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 1058     |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025115468 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00378    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 1134     |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 31000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018103195 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00964    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 1211     |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013910127 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0169      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 1288     |\n",
      "|    total_timesteps | 34816    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034385055 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.309       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 1364     |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025965862 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.283       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0211      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.17    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 1441     |\n",
      "|    total_timesteps | 38912    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 39000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018257178 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.893      |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 1518     |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 41000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011217339 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.779      |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00487    |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 1600     |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 43500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066240197 |\n",
      "|    clip_fraction        | 0.0917       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.642       |\n",
      "|    explained_variance   | 0.434        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0857       |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00468     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 1677     |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003662452 |\n",
      "|    clip_fraction        | 0.054       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.674      |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0541      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00177    |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 1754     |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 47500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011007186 |\n",
      "|    clip_fraction        | 0.0946      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.726      |\n",
      "|    explained_variance   | 0.475       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0786      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00567    |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 1831     |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 49500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00651054 |\n",
      "|    clip_fraction        | 0.0726     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.699     |\n",
      "|    explained_variance   | 0.44       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0426     |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0046    |\n",
      "|    value_loss           | 0.127      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 1907     |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 51500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00706847 |\n",
      "|    clip_fraction        | 0.0662     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.673     |\n",
      "|    explained_variance   | 0.423      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.052      |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.00183   |\n",
      "|    value_loss           | 0.132      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 1984     |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 53500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031115115 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.87       |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0325      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 2061     |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010117885 |\n",
      "|    clip_fraction        | 0.0521      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.684      |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0764      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00404    |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 2137     |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 57500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005491023 |\n",
      "|    clip_fraction        | 0.0406      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.173       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00212    |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 2214     |\n",
      "|    total_timesteps | 59392    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 59500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007642997 |\n",
      "|    clip_fraction        | 0.0775      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.027       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00169    |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 2290     |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 61500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003913982 |\n",
      "|    clip_fraction        | 0.046       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.429      |\n",
      "|    explained_variance   | 0.456       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0435      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0014     |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 2367     |\n",
      "|    total_timesteps | 63488    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 63500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022485568 |\n",
      "|    clip_fraction        | 0.031        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.442       |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0573       |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 2449     |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 66000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037005632 |\n",
      "|    clip_fraction        | 0.0383       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.49        |\n",
      "|    explained_variance   | 0.435        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0817       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | 8.85e-05     |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 2526     |\n",
      "|    total_timesteps | 67584    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 68000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006703802 |\n",
      "|    clip_fraction        | 0.0677      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.538      |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0643      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00611    |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 2603     |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011405567 |\n",
      "|    clip_fraction        | 0.0968      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.481      |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0574      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00646    |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 2680     |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016252574 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.109       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00984    |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 2757     |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 74000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02783681 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.449     |\n",
      "|    explained_variance   | 0.428      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00596    |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    value_loss           | 0.123      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 2834     |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 76000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004878525 |\n",
      "|    clip_fraction        | 0.0543      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.416      |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0221      |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 2911     |\n",
      "|    total_timesteps | 77824    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 78000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033377968 |\n",
      "|    clip_fraction        | 0.0251       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.374       |\n",
      "|    explained_variance   | 0.441        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0946       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | 0.000629     |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 2987     |\n",
      "|    total_timesteps | 79872    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036408128 |\n",
      "|    clip_fraction        | 0.04         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.272       |\n",
      "|    explained_variance   | 0.442        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0609       |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00211     |\n",
      "|    value_loss           | 0.129        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 3064     |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 82000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030925486 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.362       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0375     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=82500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 82500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 3141     |\n",
      "|    total_timesteps | 83968    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 84000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012666434 |\n",
      "|    clip_fraction        | 0.0538      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.271      |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0554      |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00495    |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=84500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 3223     |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 86500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02114596 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.365     |\n",
      "|    explained_variance   | 0.352      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00721   |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    value_loss           | 0.122      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 87000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 87500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 3300     |\n",
      "|    total_timesteps | 88064    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 88500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018367583 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00618     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 89000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 89500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 3377     |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 90500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030904943 |\n",
      "|    clip_fraction        | 0.0292       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.206       |\n",
      "|    explained_variance   | 0.451        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0912       |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 91000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 91500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 3453     |\n",
      "|    total_timesteps | 92160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 92500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014711847 |\n",
      "|    clip_fraction        | 0.0179       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.158       |\n",
      "|    explained_variance   | 0.444        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0562       |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.000339    |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 93000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 93500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 3529     |\n",
      "|    total_timesteps | 94208    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94500, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -10         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 94500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013208706 |\n",
      "|    clip_fraction        | 0.066       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.203      |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0482      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00218    |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 95000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95500, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 95500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.23    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 3606     |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 96500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04481013 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.981     |\n",
      "|    explained_variance   | 0.00443    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00988   |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.0367    |\n",
      "|    value_loss           | 0.0877     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 97000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 97500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 3683     |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 98500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026151586 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.0966      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0746     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0365     |\n",
      "|    value_loss           | 0.0997      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 99000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=99500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 99500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.68    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 3760     |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025402047 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.846      |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.051       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0341     |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 101000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=101500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 101500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.82    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 3837     |\n",
      "|    total_timesteps | 102400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 102500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027888559 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.683      |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0367      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 103000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=103500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 103500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.83    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 3913     |\n",
      "|    total_timesteps | 104448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 104500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01540871 |\n",
      "|    clip_fraction        | 0.0952     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.224     |\n",
      "|    explained_variance   | 0.377      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0289     |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    value_loss           | 0.116      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 105000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 105500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.63    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 3990     |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 106500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010330096 |\n",
      "|    clip_fraction        | 0.0573      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.166      |\n",
      "|    explained_variance   | 0.451       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0352      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.00713    |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 107000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 107500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.37    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 4073     |\n",
      "|    total_timesteps | 108544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 109000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007949585 |\n",
      "|    clip_fraction        | 0.00693     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.109      |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0757      |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.000262   |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=109500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 109500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.17    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 4149     |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 111000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011812935 |\n",
      "|    clip_fraction        | 0.0287       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.353       |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0812       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | 0.0007       |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=111500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 111500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 4226     |\n",
      "|    total_timesteps | 112640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 113000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027414863 |\n",
      "|    clip_fraction        | 0.024        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.386       |\n",
      "|    explained_variance   | 0.462        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0954       |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | 0.000431     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=113500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 113500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 4303     |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 115000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004172079 |\n",
      "|    clip_fraction        | 0.0565      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.483      |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.00496    |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=115500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 115500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 4380     |\n",
      "|    total_timesteps | 116736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 117000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025679946 |\n",
      "|    clip_fraction        | 0.0317       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.475       |\n",
      "|    explained_variance   | 0.473        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0576       |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.000479    |\n",
      "|    value_loss           | 0.117        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=117500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 117500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=118500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 4459     |\n",
      "|    total_timesteps | 118784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 119000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013449653 |\n",
      "|    clip_fraction        | 0.0191       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.477       |\n",
      "|    explained_variance   | 0.454        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0534       |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.000602    |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=119500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 119500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 4537     |\n",
      "|    total_timesteps | 120832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 121000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017961769 |\n",
      "|    clip_fraction        | 0.0161       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.466       |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.104        |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | 9.87e-05     |\n",
      "|    value_loss           | 0.129        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=121500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 121500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 4616     |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 123000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029703411 |\n",
      "|    clip_fraction        | 0.0287       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.428       |\n",
      "|    explained_variance   | 0.449        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0525       |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=123500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 123500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 4694     |\n",
      "|    total_timesteps | 124928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 125000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035693597 |\n",
      "|    clip_fraction        | 0.0258       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.408       |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0532       |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.000699    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=125500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 125500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=126500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 4773     |\n",
      "|    total_timesteps | 126976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 127000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009969934 |\n",
      "|    clip_fraction        | 0.094       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0501      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.00769    |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=127500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 127500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 129000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 63       |\n",
      "|    time_elapsed    | 4858     |\n",
      "|    total_timesteps | 129024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 129500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088385325 |\n",
      "|    clip_fraction        | 0.113        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.594       |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0809       |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00745     |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 131000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 4936     |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=131500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 131500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016597008 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.657      |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0168      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 132000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 132500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 133000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 5015     |\n",
      "|    total_timesteps | 133120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=133500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 133500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015218284 |\n",
      "|    clip_fraction        | 0.0998      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.465      |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0663      |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00856    |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 134000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=134500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 134500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 135000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.09    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 5094     |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=135500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 135500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013695015 |\n",
      "|    clip_fraction        | 0.0753      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.372      |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0886      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00824    |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 136000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 136500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 137000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 5172     |\n",
      "|    total_timesteps | 137216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=137500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 137500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104705095 |\n",
      "|    clip_fraction        | 0.0763       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.42        |\n",
      "|    explained_variance   | 0.423        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0182       |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.00588     |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 138000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=138500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 138500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 139000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 5250     |\n",
      "|    total_timesteps | 139264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=139500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 139500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00130222 |\n",
      "|    clip_fraction        | 0.0229     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.385     |\n",
      "|    explained_variance   | 0.44       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0732     |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.00163   |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 141000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 5328     |\n",
      "|    total_timesteps | 141312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 141500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004942449 |\n",
      "|    clip_fraction        | 0.0552      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.389      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0403      |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.00362    |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 142000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=142500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 142500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 143000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 5406     |\n",
      "|    total_timesteps | 143360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=143500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 143500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002562075 |\n",
      "|    clip_fraction        | 0.0336      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.442      |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0505      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.000712   |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 144000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 144500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 145000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 5484     |\n",
      "|    total_timesteps | 145408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=145500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 145500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025412366 |\n",
      "|    clip_fraction        | 0.0395       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.397       |\n",
      "|    explained_variance   | 0.46         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0794       |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 146000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=146500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 146500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 147000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 5562     |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=147500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 147500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002954822 |\n",
      "|    clip_fraction        | 0.0159      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.354      |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0768      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | 0.000189    |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 148000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=148500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 148500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 149000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=149500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 149500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 73       |\n",
      "|    time_elapsed    | 5645     |\n",
      "|    total_timesteps | 149504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 150000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019231694 |\n",
      "|    clip_fraction        | 0.0226       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.33        |\n",
      "|    explained_variance   | 0.451        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0592       |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.000801    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=150500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 151000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=151500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 151500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 5724     |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 152000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001076951 |\n",
      "|    clip_fraction        | 0.0117      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0702      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | 6.29e-05    |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=152500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 152500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 153000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=153500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 153500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 75       |\n",
      "|    time_elapsed    | 5802     |\n",
      "|    total_timesteps | 153600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 154000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016127148 |\n",
      "|    clip_fraction        | 0.0163       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.24        |\n",
      "|    explained_variance   | 0.431        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0398       |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.000243    |\n",
      "|    value_loss           | 0.133        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=154500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 154500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=155500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 5881     |\n",
      "|    total_timesteps | 155648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 156000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00087184657 |\n",
      "|    clip_fraction        | 0.0153        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.204        |\n",
      "|    explained_variance   | 0.454         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0748        |\n",
      "|    n_updates            | 760           |\n",
      "|    policy_gradient_loss | -0.000339     |\n",
      "|    value_loss           | 0.127         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=156500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 156500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 157000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=157500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 157500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 77       |\n",
      "|    time_elapsed    | 5962     |\n",
      "|    total_timesteps | 157696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 158000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004455785 |\n",
      "|    clip_fraction        | 0.0292      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.214      |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0718      |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00108    |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=158500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 158500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 159000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=159500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 159500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 6042     |\n",
      "|    total_timesteps | 159744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 160000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01044246 |\n",
      "|    clip_fraction        | 0.0914     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.241     |\n",
      "|    explained_variance   | 0.488      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0388     |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0074    |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=160500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 160500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 161000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 161500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.09    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 6122     |\n",
      "|    total_timesteps | 161792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 162000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03412088 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.533     |\n",
      "|    explained_variance   | 0.274      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00894    |\n",
      "|    n_updates            | 790        |\n",
      "|    policy_gradient_loss | -0.00784   |\n",
      "|    value_loss           | 0.113      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=162500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 162500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 163000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=163500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 163500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.13    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 6201     |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 164000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01013866 |\n",
      "|    clip_fraction        | 0.0971     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.358     |\n",
      "|    explained_variance   | 0.37       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0379     |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.00967   |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=164500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 164500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 165000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=165500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 165500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 6278     |\n",
      "|    total_timesteps | 165888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 166000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00782056 |\n",
      "|    clip_fraction        | 0.0747     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.233     |\n",
      "|    explained_variance   | 0.452      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0632     |\n",
      "|    n_updates            | 810        |\n",
      "|    policy_gradient_loss | -0.00656   |\n",
      "|    value_loss           | 0.12       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=166500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 166500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 167000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=167500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 167500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 6354     |\n",
      "|    total_timesteps | 167936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 168000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008090431 |\n",
      "|    clip_fraction        | 0.0602      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.246      |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0759      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.00368    |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=168500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 168500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 169000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=169500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 169500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 83       |\n",
      "|    time_elapsed    | 6431     |\n",
      "|    total_timesteps | 169984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 170000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038238708 |\n",
      "|    clip_fraction        | 0.0392       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.176       |\n",
      "|    explained_variance   | 0.49         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0515       |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=170500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 171000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=171500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 171500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 172000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 6513     |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=172500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 172500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003605815 |\n",
      "|    clip_fraction        | 0.0287      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.129      |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0499      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0021     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 173000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=173500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 173500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 174000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 85       |\n",
      "|    time_elapsed    | 6589     |\n",
      "|    total_timesteps | 174080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=174500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 174500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011213073 |\n",
      "|    clip_fraction        | 0.00894      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.13        |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0522       |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.000278    |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 175000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=175500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 175500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 176000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 6665     |\n",
      "|    total_timesteps | 176128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 176500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012254273 |\n",
      "|    clip_fraction        | 0.0115       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.14        |\n",
      "|    explained_variance   | 0.453        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0729       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.000451    |\n",
      "|    value_loss           | 0.125        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 177000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=177500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 177500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 178000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 87       |\n",
      "|    time_elapsed    | 6742     |\n",
      "|    total_timesteps | 178176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=178500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 178500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027037794 |\n",
      "|    clip_fraction        | 0.0231       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.146       |\n",
      "|    explained_variance   | 0.458        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0814       |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    value_loss           | 0.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 179000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=179500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 179500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 6818     |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008725664 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.194      |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0435      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.00355    |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 181000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 181500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 182000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 89       |\n",
      "|    time_elapsed    | 6895     |\n",
      "|    total_timesteps | 182272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=182500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 182500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013234278 |\n",
      "|    clip_fraction        | 0.0445      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.143      |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0603      |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00205    |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 183000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=183500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 183500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 184000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 6971     |\n",
      "|    total_timesteps | 184320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=184500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 184500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008373137 |\n",
      "|    clip_fraction        | 0.0571      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.191      |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0186      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00649    |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 185000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=185500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 185500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 186000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 7048     |\n",
      "|    total_timesteps | 186368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=186500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 186500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016086178 |\n",
      "|    clip_fraction        | 0.0849      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.237      |\n",
      "|    explained_variance   | 0.401       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0957      |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 187000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=187500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 187500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 188000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 7124     |\n",
      "|    total_timesteps | 188416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=188500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 188500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022855748 |\n",
      "|    clip_fraction        | 0.0248       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.109       |\n",
      "|    explained_variance   | 0.439        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0706       |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    value_loss           | 0.132        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 189000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=189500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 189500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 7201     |\n",
      "|    total_timesteps | 190464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 190500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004025298 |\n",
      "|    clip_fraction        | 0.0161      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0728     |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0346      |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00168    |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 191000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=191500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 191500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 192500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 7283     |\n",
      "|    total_timesteps | 192512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 193000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029794166 |\n",
      "|    clip_fraction        | 0.0148       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0639      |\n",
      "|    explained_variance   | 0.444        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0818       |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    value_loss           | 0.13         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=193500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 193500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 194000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=194500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 194500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 95       |\n",
      "|    time_elapsed    | 7359     |\n",
      "|    total_timesteps | 194560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 195000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031670935 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.11        |\n",
      "|    explained_variance   | 0.422        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0593       |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -0.0022      |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=195500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 195500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 196000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=196500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 196500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 7436     |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 197000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112904925 |\n",
      "|    clip_fraction        | 0.111        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.252       |\n",
      "|    explained_variance   | 0.376        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0832       |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0129      |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=197500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 197500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 198000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=198500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 198500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 97       |\n",
      "|    time_elapsed    | 7512     |\n",
      "|    total_timesteps | 198656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 199000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014688497 |\n",
      "|    clip_fraction        | 0.0957      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.442       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0642      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0089     |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=199500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 199500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 7589     |\n",
      "|    total_timesteps | 200704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 201000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034126215 |\n",
      "|    clip_fraction        | 0.0249      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.168      |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0329      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.00244    |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=201500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 201500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=202000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 202000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=202500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 202500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 99       |\n",
      "|    time_elapsed    | 7665     |\n",
      "|    total_timesteps | 202752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=203000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 203000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013437116 |\n",
      "|    clip_fraction        | 0.0959      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.56       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0188      |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.00547    |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=203500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 203500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 204000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=204500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 204500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 7742     |\n",
      "|    total_timesteps | 204800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 205000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00206779 |\n",
      "|    clip_fraction        | 0.0378     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.576     |\n",
      "|    explained_variance   | 0.439      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0983     |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.000267  |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=205500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 205500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=206000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 206000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=206500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 206500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 101      |\n",
      "|    time_elapsed    | 7818     |\n",
      "|    total_timesteps | 206848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=207000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 207000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039709336 |\n",
      "|    clip_fraction        | 0.0324       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.586       |\n",
      "|    explained_variance   | 0.472        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0466       |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | 5.27e-05     |\n",
      "|    value_loss           | 0.116        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=207500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 207500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 208000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=208500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 208500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 7894     |\n",
      "|    total_timesteps | 208896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=209000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 209000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051219645 |\n",
      "|    clip_fraction        | 0.0622       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.575       |\n",
      "|    explained_variance   | 0.456        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0778       |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=209500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 209500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 7971     |\n",
      "|    total_timesteps | 210944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=211000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 211000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032518995 |\n",
      "|    clip_fraction        | 0.0458       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.656       |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0656       |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    value_loss           | 0.129        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=211500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 211500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 212000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=212500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 212500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 8047     |\n",
      "|    total_timesteps | 212992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=213000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 213000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011078117 |\n",
      "|    clip_fraction        | 0.0505      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.654      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0328      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.00191    |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=213500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 213500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=214000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 214000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=214500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 214500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 215000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 105      |\n",
      "|    time_elapsed    | 8129     |\n",
      "|    total_timesteps | 215040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=215500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 215500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008304821 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.702      |\n",
      "|    explained_variance   | 0.413       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0143      |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.00569    |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 216000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=216500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 216500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=217000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 217000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 8206     |\n",
      "|    total_timesteps | 217088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=217500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 217500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012177302 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.783      |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0105      |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.00996    |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=218000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 218000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=218500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 218500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=219000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 219000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 107      |\n",
      "|    time_elapsed    | 8282     |\n",
      "|    total_timesteps | 219136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=219500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 219500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007833593 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.783      |\n",
      "|    explained_variance   | 0.454       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0809      |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | -0.00669    |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 221000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 8359     |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 221500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014987676 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.834      |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0527      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=222000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 222000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=222500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 222500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=223000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 223000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 109      |\n",
      "|    time_elapsed    | 8435     |\n",
      "|    total_timesteps | 223232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=223500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 223500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022010744 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.924      |\n",
      "|    explained_variance   | 0.322       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00523    |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 224000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 224500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 225000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 8512     |\n",
      "|    total_timesteps | 225280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=225500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 225500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023154777 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.85       |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0179      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=226000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 226000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=226500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 226500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=227000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 227000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 111      |\n",
      "|    time_elapsed    | 8588     |\n",
      "|    total_timesteps | 227328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=227500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 227500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009098166 |\n",
      "|    clip_fraction        | 0.0954      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.837      |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0885      |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.00885    |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 228000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=228500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 228500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=229000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 229000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 8664     |\n",
      "|    total_timesteps | 229376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=229500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 229500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043014875 |\n",
      "|    clip_fraction        | 0.0466       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.816       |\n",
      "|    explained_variance   | 0.445        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0731       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 230000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=230500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 230500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=231000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 231000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 8741     |\n",
      "|    total_timesteps | 231424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=231500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 231500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018047495 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.825      |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0523      |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | -0.00943    |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 232000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=232500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 232500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=233000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 233000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 8817     |\n",
      "|    total_timesteps | 233472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=233500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 233500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0114077255 |\n",
      "|    clip_fraction        | 0.0889       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.799       |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0627       |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.00517     |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=234000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 234000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=234500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 234500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 235000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=235500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 235500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 115      |\n",
      "|    time_elapsed    | 8900     |\n",
      "|    total_timesteps | 235520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 236000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009727241 |\n",
      "|    clip_fraction        | 0.0877      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.697      |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0755      |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.00537    |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=236500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 236500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=237000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 237000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=237500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 237500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 8976     |\n",
      "|    total_timesteps | 237568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=238000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 238000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030126576 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.685       |\n",
      "|    explained_variance   | 0.458        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0608       |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -7.09e-05    |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=238500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 238500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=239000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 239000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=239500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 239500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 117      |\n",
      "|    time_elapsed    | 9053     |\n",
      "|    total_timesteps | 239616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008848752 |\n",
      "|    clip_fraction        | 0.0988      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.759      |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0937      |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=240500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 240500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 241000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 241500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 9129     |\n",
      "|    total_timesteps | 241664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=242000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 242000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007281439 |\n",
      "|    clip_fraction        | 0.0452      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | 0.44        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0407      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.00278    |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=242500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 242500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=243000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 243000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=243500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 243500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 119      |\n",
      "|    time_elapsed    | 9206     |\n",
      "|    total_timesteps | 243712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 244000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008799201 |\n",
      "|    clip_fraction        | 0.0813      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0561      |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | -0.00547    |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=244500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 244500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 245000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=245500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 245500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 9282     |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=246000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 246000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01042652 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.709     |\n",
      "|    explained_variance   | 0.435      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0236     |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    value_loss           | 0.118      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=246500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 246500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=247000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 247000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=247500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 247500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 121      |\n",
      "|    time_elapsed    | 9358     |\n",
      "|    total_timesteps | 247808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 248000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010410206 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.654      |\n",
      "|    explained_variance   | 0.451       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0223      |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=248500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 248500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=249000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 249000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=249500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 249500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 9434     |\n",
      "|    total_timesteps | 249856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 250000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005402383 |\n",
      "|    clip_fraction        | 0.0477      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.605      |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00241    |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.00337    |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=250500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 250500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=251000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 251000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=251500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 251500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 9511     |\n",
      "|    total_timesteps | 251904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 252000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042774505 |\n",
      "|    clip_fraction        | 0.0551       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.597       |\n",
      "|    explained_variance   | 0.458        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.067        |\n",
      "|    n_updates            | 1230         |\n",
      "|    policy_gradient_loss | -0.00314     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=252500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 252500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=253000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 253000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=253500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 253500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 9587     |\n",
      "|    total_timesteps | 253952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=254000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 254000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004655186 |\n",
      "|    clip_fraction        | 0.0418      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.592      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0338      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.00243    |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=254500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 254500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 255000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=255500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 255500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 125      |\n",
      "|    time_elapsed    | 9669     |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 256500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006705081 |\n",
      "|    clip_fraction        | 0.0556      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.666      |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00681     |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.00631    |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=257000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 257000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=257500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 257500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=258000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 258000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 9745     |\n",
      "|    total_timesteps | 258048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=258500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 258500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005801871 |\n",
      "|    clip_fraction        | 0.0667      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.679      |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0483      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.00812    |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=259000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 259000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=259500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 259500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 260000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 9822     |\n",
      "|    total_timesteps | 260096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003925211 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.046       |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | -4.64e-05   |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=261000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 261000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=261500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 261500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 262000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 9898     |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 262500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014788922 |\n",
      "|    clip_fraction        | 0.0342       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.483       |\n",
      "|    explained_variance   | 0.445        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0368       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.000834    |\n",
      "|    value_loss           | 0.13         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=263000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 263000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=263500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 263500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 264000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 129      |\n",
      "|    time_elapsed    | 9975     |\n",
      "|    total_timesteps | 264192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=264500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 264500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005603025 |\n",
      "|    clip_fraction        | 0.0242      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.476      |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0466      |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 265000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=265500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 265500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=266000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 266000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 10051    |\n",
      "|    total_timesteps | 266240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=266500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 266500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007410611 |\n",
      "|    clip_fraction        | 0.056       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0435      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.00608    |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=267000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 267000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=267500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 267500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 268000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 131      |\n",
      "|    time_elapsed    | 10128    |\n",
      "|    total_timesteps | 268288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=268500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 268500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070380783 |\n",
      "|    clip_fraction        | 0.0333       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.471       |\n",
      "|    explained_variance   | 0.439        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0566       |\n",
      "|    n_updates            | 1310         |\n",
      "|    policy_gradient_loss | -0.00198     |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=269000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 269000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=269500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 269500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 10204    |\n",
      "|    total_timesteps | 270336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 270500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015168746 |\n",
      "|    clip_fraction        | 0.0371       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.541       |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0343       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.00376     |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=271000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 271000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=271500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 271500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 272000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 133      |\n",
      "|    time_elapsed    | 10281    |\n",
      "|    total_timesteps | 272384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 272500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003072457 |\n",
      "|    clip_fraction        | 0.0285      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.567      |\n",
      "|    explained_variance   | 0.448       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0635      |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=273000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 273000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=273500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 273500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=274000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 274000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 10357    |\n",
      "|    total_timesteps | 274432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=274500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 274500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00546385 |\n",
      "|    clip_fraction        | 0.0315     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.569     |\n",
      "|    explained_variance   | 0.453      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0468     |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.00122   |\n",
      "|    value_loss           | 0.125      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 275000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=275500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 275500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 276000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 135      |\n",
      "|    time_elapsed    | 10433    |\n",
      "|    total_timesteps | 276480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=276500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 276500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034099952 |\n",
      "|    clip_fraction        | 0.0358       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.59        |\n",
      "|    explained_variance   | 0.432        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0244       |\n",
      "|    n_updates            | 1350         |\n",
      "|    policy_gradient_loss | -0.0043      |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=277000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 277000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=277500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 277500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=278000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 278000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=278500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 278500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 10516    |\n",
      "|    total_timesteps | 278528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=279000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 279000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066925823 |\n",
      "|    clip_fraction        | 0.0546       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.535       |\n",
      "|    explained_variance   | 0.453        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0903       |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.00289     |\n",
      "|    value_loss           | 0.125        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=279500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 279500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 10592    |\n",
      "|    total_timesteps | 280576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=281000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 281000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005135219 |\n",
      "|    clip_fraction        | 0.0409      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.476      |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0485      |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | -0.00276    |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=281500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 281500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 282000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 282500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 10669    |\n",
      "|    total_timesteps | 282624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=283000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 283000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018119628 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.312       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0105      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=283500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 283500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 284000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=284500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 284500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 139      |\n",
      "|    time_elapsed    | 10745    |\n",
      "|    total_timesteps | 284672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 100       |\n",
      "|    mean_reward          | -8        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 285000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0164431 |\n",
      "|    clip_fraction        | 0.121     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.525    |\n",
      "|    explained_variance   | 0.423     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0447    |\n",
      "|    n_updates            | 1390      |\n",
      "|    policy_gradient_loss | -0.0146   |\n",
      "|    value_loss           | 0.12      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=285500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 285500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=286000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 286000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=286500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 286500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 10821    |\n",
      "|    total_timesteps | 286720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=287000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 287000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004963242 |\n",
      "|    clip_fraction        | 0.0227      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.379      |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.00112    |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=287500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 287500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 288000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 288500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 141      |\n",
      "|    time_elapsed    | 10898    |\n",
      "|    total_timesteps | 288768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=289000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 289000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031180314 |\n",
      "|    clip_fraction        | 0.0397       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.3         |\n",
      "|    explained_variance   | 0.452        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.035        |\n",
      "|    n_updates            | 1410         |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=289500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 289500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 10974    |\n",
      "|    total_timesteps | 290816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=291000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 291000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013558365 |\n",
      "|    clip_fraction        | 0.0218       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.271       |\n",
      "|    explained_variance   | 0.451        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0378       |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.000604    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=291500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 291500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 292000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=292500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 292500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 143      |\n",
      "|    time_elapsed    | 11051    |\n",
      "|    total_timesteps | 292864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=293000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 293000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002221435 |\n",
      "|    clip_fraction        | 0.0388      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.209      |\n",
      "|    explained_variance   | 0.451       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0571      |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | -0.00267    |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=293500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 293500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=294000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 294000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=294500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 294500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 11127    |\n",
      "|    total_timesteps | 294912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 295000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015492227 |\n",
      "|    clip_fraction        | 0.0193       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.249       |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0299       |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.000484    |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=295500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 295500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 296000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=296500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 296500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 145      |\n",
      "|    time_elapsed    | 11204    |\n",
      "|    total_timesteps | 296960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=297000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 297000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015101521 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.204       |\n",
      "|    explained_variance   | 0.437        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0425       |\n",
      "|    n_updates            | 1450         |\n",
      "|    policy_gradient_loss | -5.64e-05    |\n",
      "|    value_loss           | 0.129        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=297500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 297500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=298000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 298000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=298500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 298500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=299000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 299000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 11286    |\n",
      "|    total_timesteps | 299008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=299500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 299500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001238659 |\n",
      "|    clip_fraction        | 0.00942     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.2        |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0462      |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.000144   |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=301000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 301000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 11362    |\n",
      "|    total_timesteps | 301056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=301500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 301500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037077377 |\n",
      "|    clip_fraction        | 0.0211       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0767       |\n",
      "|    n_updates            | 1470         |\n",
      "|    policy_gradient_loss | -0.000709    |\n",
      "|    value_loss           | 0.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=302000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 302000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 302500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=303000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 303000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 11438    |\n",
      "|    total_timesteps | 303104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=303500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 303500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015495215 |\n",
      "|    clip_fraction        | 0.0118       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.197       |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0523       |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -7.26e-05    |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 304000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=304500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 304500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 305000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 149      |\n",
      "|    time_elapsed    | 11515    |\n",
      "|    total_timesteps | 305152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=305500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 305500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022937963 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.198       |\n",
      "|    explained_variance   | 0.439        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0389       |\n",
      "|    n_updates            | 1490         |\n",
      "|    policy_gradient_loss | -0.00022     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=306000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 306000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=306500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 306500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=307000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 307000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 11591    |\n",
      "|    total_timesteps | 307200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=307500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 307500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036357935 |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.222       |\n",
      "|    explained_variance   | 0.415        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0711       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.00238     |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 308000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=308500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 308500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=309000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 309000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 151      |\n",
      "|    time_elapsed    | 11667    |\n",
      "|    total_timesteps | 309248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=309500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 309500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028881645 |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.199       |\n",
      "|    explained_variance   | 0.47         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0666       |\n",
      "|    n_updates            | 1510         |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=311000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 311000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 11744    |\n",
      "|    total_timesteps | 311296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=311500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 311500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003507779 |\n",
      "|    clip_fraction        | 0.0168      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.191      |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0429      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.000462   |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 312000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=312500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 312500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=313000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 313000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 153      |\n",
      "|    time_elapsed    | 11820    |\n",
      "|    total_timesteps | 313344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=313500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 313500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00052306557 |\n",
      "|    clip_fraction        | 0.00542       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.176        |\n",
      "|    explained_variance   | 0.453         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0569        |\n",
      "|    n_updates            | 1530          |\n",
      "|    policy_gradient_loss | 1.32e-06      |\n",
      "|    value_loss           | 0.126         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=314000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 314000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=314500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 314500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 315000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 11897    |\n",
      "|    total_timesteps | 315392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=315500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 315500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010230088 |\n",
      "|    clip_fraction        | 0.0139       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.192       |\n",
      "|    explained_variance   | 0.451        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0941       |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.000362    |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 316000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=316500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 316500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=317000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 317000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 155      |\n",
      "|    time_elapsed    | 11973    |\n",
      "|    total_timesteps | 317440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=317500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 317500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003256745 |\n",
      "|    clip_fraction        | 0.0429      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.225      |\n",
      "|    explained_variance   | 0.451       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0475      |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | -0.00217    |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=318000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 318000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=318500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 318500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=319000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 319000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 12050    |\n",
      "|    total_timesteps | 319488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=319500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 319500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001905228 |\n",
      "|    clip_fraction        | 0.0111      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.167      |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0591      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.000253   |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 320000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 320500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=321000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 321000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=321500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 321500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 12132    |\n",
      "|    total_timesteps | 321536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 322000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006881199 |\n",
      "|    clip_fraction        | 0.00972      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.134       |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0731       |\n",
      "|    n_updates            | 1570         |\n",
      "|    policy_gradient_loss | -0.000188    |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=322500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 322500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=323000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 323000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=323500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 323500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 12209    |\n",
      "|    total_timesteps | 323584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 324000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001571729 |\n",
      "|    clip_fraction        | 0.0189      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.173      |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.084       |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.000216   |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=324500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 324500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=325000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 325000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=325500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 325500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 159      |\n",
      "|    time_elapsed    | 12285    |\n",
      "|    total_timesteps | 325632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=326000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 326000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003992777 |\n",
      "|    clip_fraction        | 0.0482      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.207      |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0545      |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.00319    |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=326500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 326500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=327000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 327000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=327500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 327500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 12362    |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 328000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009612304 |\n",
      "|    clip_fraction        | 0.0571      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.217      |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0995      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.00413    |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=328500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 328500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=329000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 329000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=329500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 329500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 161      |\n",
      "|    time_elapsed    | 12438    |\n",
      "|    total_timesteps | 329728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 330000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064118737 |\n",
      "|    clip_fraction        | 0.0236       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.185       |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0398       |\n",
      "|    n_updates            | 1610         |\n",
      "|    policy_gradient_loss | -0.00301     |\n",
      "|    value_loss           | 0.125        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=330500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 330500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=331000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 331000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=331500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 331500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 12515    |\n",
      "|    total_timesteps | 331776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 332000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019454536 |\n",
      "|    clip_fraction        | 0.0182       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.17        |\n",
      "|    explained_variance   | 0.452        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0478       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.000194    |\n",
      "|    value_loss           | 0.118        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=332500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 332500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=333000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 333000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=333500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 333500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 163      |\n",
      "|    time_elapsed    | 12591    |\n",
      "|    total_timesteps | 333824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=334000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 334000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017103439 |\n",
      "|    clip_fraction        | 0.0125       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.167       |\n",
      "|    explained_variance   | 0.452        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0539       |\n",
      "|    n_updates            | 1630         |\n",
      "|    policy_gradient_loss | -2.64e-05    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=334500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 334500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=335000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 335000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=335500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 335500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 12667    |\n",
      "|    total_timesteps | 335872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 336000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015594463 |\n",
      "|    clip_fraction        | 0.0173       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.141       |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0428       |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.000558    |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=336500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 336500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=337000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 337000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=337500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 337500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 165      |\n",
      "|    time_elapsed    | 12744    |\n",
      "|    total_timesteps | 337920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=338000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 338000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015416399 |\n",
      "|    clip_fraction        | 0.0151       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.159       |\n",
      "|    explained_variance   | 0.445        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.107        |\n",
      "|    n_updates            | 1650         |\n",
      "|    policy_gradient_loss | -0.000509    |\n",
      "|    value_loss           | 0.131        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=338500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 338500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=339000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 339000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=339500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 339500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 12820    |\n",
      "|    total_timesteps | 339968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 340000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065458296 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.238       |\n",
      "|    explained_variance   | 0.453        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0241       |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.00392     |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=340500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 340500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=341000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 341000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=341500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 341500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 342000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 12902    |\n",
      "|    total_timesteps | 342016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 342500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028156112 |\n",
      "|    clip_fraction        | 0.0171       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.133       |\n",
      "|    explained_variance   | 0.444        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.06         |\n",
      "|    n_updates            | 1670         |\n",
      "|    policy_gradient_loss | -0.000257    |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=343000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 343000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=343500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 343500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 344000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 12978    |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=344500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 344500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009958773 |\n",
      "|    clip_fraction        | 0.00684      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.136       |\n",
      "|    explained_variance   | 0.436        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0672       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | 2.94e-05     |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 345000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=345500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 345500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=346000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 346000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 169      |\n",
      "|    time_elapsed    | 13055    |\n",
      "|    total_timesteps | 346112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=346500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 346500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020248946 |\n",
      "|    clip_fraction        | 0.0799      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.266      |\n",
      "|    explained_variance   | 0.388       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0427      |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=347000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 347000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=347500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 347500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 348000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.11    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 13131    |\n",
      "|    total_timesteps | 348160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=348500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 348500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026930073 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.393      |\n",
      "|    explained_variance   | 0.289       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00985    |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=349000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 349000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=349500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 349500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.13    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 171      |\n",
      "|    time_elapsed    | 13208    |\n",
      "|    total_timesteps | 350208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 350500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010613108 |\n",
      "|    clip_fraction        | 0.0614      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.222      |\n",
      "|    explained_variance   | 0.423       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0742      |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | -0.00911    |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=351000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 351000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=351500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 351500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 352000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.14    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 13284    |\n",
      "|    total_timesteps | 352256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 352500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050440906 |\n",
      "|    clip_fraction        | 0.0224       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.13        |\n",
      "|    explained_variance   | 0.437        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0715       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=353000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 353000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=353500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 353500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=354000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 354000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.14    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 173      |\n",
      "|    time_elapsed    | 13361    |\n",
      "|    total_timesteps | 354304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=354500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 354500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00088702247 |\n",
      "|    clip_fraction        | 0.0084        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.128        |\n",
      "|    explained_variance   | 0.449         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0719        |\n",
      "|    n_updates            | 1730          |\n",
      "|    policy_gradient_loss | 7.32e-05      |\n",
      "|    value_loss           | 0.127         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=355000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 355000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=355500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 355500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 356000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.14    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 13437    |\n",
      "|    total_timesteps | 356352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=356500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 356500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007335245 |\n",
      "|    clip_fraction        | 0.00781      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.134       |\n",
      "|    explained_variance   | 0.456        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0286       |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | 0.000121     |\n",
      "|    value_loss           | 0.119        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=357000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 357000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=357500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 357500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=358000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 358000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 175      |\n",
      "|    time_elapsed    | 13514    |\n",
      "|    total_timesteps | 358400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=358500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 358500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008110567 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.123       |\n",
      "|    explained_variance   | 0.445        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0504       |\n",
      "|    n_updates            | 1750         |\n",
      "|    policy_gradient_loss | -0.000831    |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=359000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 359000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=359500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 359500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 13590    |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 360500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003339506 |\n",
      "|    clip_fraction        | 0.00381      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.111       |\n",
      "|    explained_variance   | 0.46         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0522       |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | 0.000405     |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=361000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 361000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=361500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 361500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 362000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 177      |\n",
      "|    time_elapsed    | 13666    |\n",
      "|    total_timesteps | 362496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 362500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033353115 |\n",
      "|    clip_fraction        | 0.00767       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.132        |\n",
      "|    explained_variance   | 0.445         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.11          |\n",
      "|    n_updates            | 1770          |\n",
      "|    policy_gradient_loss | -0.00066      |\n",
      "|    value_loss           | 0.125         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=363000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 363000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=363500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 363500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 364000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=364500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 364500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 13749    |\n",
      "|    total_timesteps | 364544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=365000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 365000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026632417 |\n",
      "|    clip_fraction        | 0.0187       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0505       |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.000575    |\n",
      "|    value_loss           | 0.132        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=365500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 365500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=366000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 366000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=366500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 366500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 179      |\n",
      "|    time_elapsed    | 13825    |\n",
      "|    total_timesteps | 366592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=367000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 367000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00438523 |\n",
      "|    clip_fraction        | 0.0221     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.141     |\n",
      "|    explained_variance   | 0.433      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0721     |\n",
      "|    n_updates            | 1790       |\n",
      "|    policy_gradient_loss | -0.00063   |\n",
      "|    value_loss           | 0.131      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=367500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 367500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 368000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 368500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 13902    |\n",
      "|    total_timesteps | 368640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=369000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 369000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00417553 |\n",
      "|    clip_fraction        | 0.0483     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.189     |\n",
      "|    explained_variance   | 0.419      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0905     |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.00258   |\n",
      "|    value_loss           | 0.132      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=369500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 369500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 181      |\n",
      "|    time_elapsed    | 13978    |\n",
      "|    total_timesteps | 370688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=371000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 371000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01014133 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.342     |\n",
      "|    explained_variance   | 0.364      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0257     |\n",
      "|    n_updates            | 1810       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=371500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 371500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 372000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=372500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 372500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 14055    |\n",
      "|    total_timesteps | 372736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=373000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 373000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012970123 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.392      |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000688    |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=373500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 373500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=374000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 374000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=374500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 374500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 183      |\n",
      "|    time_elapsed    | 14131    |\n",
      "|    total_timesteps | 374784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 375000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01208444 |\n",
      "|    clip_fraction        | 0.0875     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.238     |\n",
      "|    explained_variance   | 0.409      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.011      |\n",
      "|    n_updates            | 1830       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    value_loss           | 0.125      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=375500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 375500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 376000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=376500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 376500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 14208    |\n",
      "|    total_timesteps | 376832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=377000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 377000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008176561 |\n",
      "|    clip_fraction        | 0.036       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.119      |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0406      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.00338    |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=377500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 377500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=378000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 378000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=378500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 378500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 185      |\n",
      "|    time_elapsed    | 14285    |\n",
      "|    total_timesteps | 378880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=379000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 379000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037640042 |\n",
      "|    clip_fraction        | 0.0111       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0753      |\n",
      "|    explained_variance   | 0.452        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0503       |\n",
      "|    n_updates            | 1850         |\n",
      "|    policy_gradient_loss | -0.000718    |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=379500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 379500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=380500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 14361    |\n",
      "|    total_timesteps | 380928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=381000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 381000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012649999 |\n",
      "|    clip_fraction        | 0.00894      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0732      |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0581       |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.000187    |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=381500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 381500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=382000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 382000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=382500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 382500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 187      |\n",
      "|    time_elapsed    | 14438    |\n",
      "|    total_timesteps | 382976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 383000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011250504 |\n",
      "|    clip_fraction        | 0.0154       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0746      |\n",
      "|    explained_variance   | 0.456        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.109        |\n",
      "|    n_updates            | 1870         |\n",
      "|    policy_gradient_loss | -0.000892    |\n",
      "|    value_loss           | 0.117        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=383500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 383500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 384000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 384500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=385000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 385000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 14521    |\n",
      "|    total_timesteps | 385024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=385500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 385500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014090315 |\n",
      "|    clip_fraction        | 0.0185       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0586      |\n",
      "|    explained_variance   | 0.461        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.057        |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=386000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 386000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=386500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 386500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=387000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 387000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 189      |\n",
      "|    time_elapsed    | 14598    |\n",
      "|    total_timesteps | 387072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=387500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 387500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016562772 |\n",
      "|    clip_fraction        | 0.00332       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0524       |\n",
      "|    explained_variance   | 0.453         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0238        |\n",
      "|    n_updates            | 1890          |\n",
      "|    policy_gradient_loss | 3.43e-05      |\n",
      "|    value_loss           | 0.121         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 388000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=388500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 388500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=389000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 389000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 14675    |\n",
      "|    total_timesteps | 389120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=389500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 389500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004268769 |\n",
      "|    clip_fraction        | 0.0172      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.065      |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0592      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.000958   |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=390500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=391000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 391000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 191      |\n",
      "|    time_elapsed    | 14752    |\n",
      "|    total_timesteps | 391168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=391500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 391500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00074733194 |\n",
      "|    clip_fraction        | 0.00601       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0378       |\n",
      "|    explained_variance   | 0.445         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0585        |\n",
      "|    n_updates            | 1910          |\n",
      "|    policy_gradient_loss | -0.00026      |\n",
      "|    value_loss           | 0.122         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 392000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=392500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 392500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=393000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 393000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 14829    |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=393500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 393500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009629822 |\n",
      "|    clip_fraction        | 0.0116       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0435      |\n",
      "|    explained_variance   | 0.449        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0736       |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -0.00032     |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=394000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 394000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=394500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 394500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=395000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 395000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 193      |\n",
      "|    time_elapsed    | 14906    |\n",
      "|    total_timesteps | 395264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=395500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 395500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040063358 |\n",
      "|    clip_fraction        | 0.00342       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0338       |\n",
      "|    explained_variance   | 0.446         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.131         |\n",
      "|    n_updates            | 1930          |\n",
      "|    policy_gradient_loss | -0.00011      |\n",
      "|    value_loss           | 0.123         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 396000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=396500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 396500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=397000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 397000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 14983    |\n",
      "|    total_timesteps | 397312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=397500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 397500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028070973 |\n",
      "|    clip_fraction        | 0.00361       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0418       |\n",
      "|    explained_variance   | 0.445         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.053         |\n",
      "|    n_updates            | 1940          |\n",
      "|    policy_gradient_loss | -6.43e-05     |\n",
      "|    value_loss           | 0.128         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=398000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 398000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=398500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 398500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=399000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 399000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 195      |\n",
      "|    time_elapsed    | 15060    |\n",
      "|    total_timesteps | 399360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=399500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 399500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011438441 |\n",
      "|    clip_fraction        | 0.00576      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0424      |\n",
      "|    explained_variance   | 0.442        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0632       |\n",
      "|    n_updates            | 1950         |\n",
      "|    policy_gradient_loss | 0.000111     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=401000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 401000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 15137    |\n",
      "|    total_timesteps | 401408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=401500, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -10         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 401500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008398028 |\n",
      "|    clip_fraction        | 0.0333      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.073      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0521      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.00158    |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=402000, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 402000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=402500, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 402500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403000, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 403000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 197      |\n",
      "|    time_elapsed    | 15215    |\n",
      "|    total_timesteps | 403456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 403500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010818417 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.244      |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0269      |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 404000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=404500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 404500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 405000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=405500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 405500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 15298    |\n",
      "|    total_timesteps | 405504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=406000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 406000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010880234 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.332      |\n",
      "|    explained_variance   | 0.303       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.034       |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=406500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 406500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=407000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 407000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=407500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 407500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.11    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 199      |\n",
      "|    time_elapsed    | 15375    |\n",
      "|    total_timesteps | 407552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 408000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012333492 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.348      |\n",
      "|    explained_variance   | 0.4         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0466      |\n",
      "|    n_updates            | 1990        |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=408500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 408500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=409000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 409000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=409500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 409500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.11    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 15452    |\n",
      "|    total_timesteps | 409600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 410000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005482426 |\n",
      "|    clip_fraction        | 0.0309      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.116      |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0579      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0036     |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=410500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 410500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=411000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 411000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=411500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 411500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.11    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 201      |\n",
      "|    time_elapsed    | 15529    |\n",
      "|    total_timesteps | 411648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 412000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008308172 |\n",
      "|    clip_fraction        | 0.0287      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0874     |\n",
      "|    explained_variance   | 0.485       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0878      |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | -0.00228    |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=412500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 412500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=413000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 413000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=413500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 413500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 15607    |\n",
      "|    total_timesteps | 413696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=414000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 414000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030104406 |\n",
      "|    clip_fraction        | 0.0127       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0864      |\n",
      "|    explained_variance   | 0.449        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0657       |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | -0.000733    |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=414500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 414500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=415000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 415000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=415500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 415500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 203      |\n",
      "|    time_elapsed    | 15684    |\n",
      "|    total_timesteps | 415744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 416000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023207364 |\n",
      "|    clip_fraction        | 0.0136       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0925      |\n",
      "|    explained_variance   | 0.432        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0737       |\n",
      "|    n_updates            | 2030         |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=416500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 416500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=417000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 417000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=417500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 417500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 15762    |\n",
      "|    total_timesteps | 417792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=418000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 418000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020544502 |\n",
      "|    clip_fraction        | 0.0041        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0617       |\n",
      "|    explained_variance   | 0.442         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0437        |\n",
      "|    n_updates            | 2040          |\n",
      "|    policy_gradient_loss | -0.000231     |\n",
      "|    value_loss           | 0.127         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=418500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 418500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=419000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 419000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=419500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 419500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 205      |\n",
      "|    time_elapsed    | 15840    |\n",
      "|    total_timesteps | 419840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 420000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006924315 |\n",
      "|    clip_fraction        | 0.00635      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0651      |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0764       |\n",
      "|    n_updates            | 2050         |\n",
      "|    policy_gradient_loss | -0.000317    |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=420500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 420500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=421000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 421000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=421500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 421500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 15917    |\n",
      "|    total_timesteps | 421888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=422000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 422000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00089957524 |\n",
      "|    clip_fraction        | 0.00557       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0607       |\n",
      "|    explained_variance   | 0.449         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0411        |\n",
      "|    n_updates            | 2060          |\n",
      "|    policy_gradient_loss | 0.000184      |\n",
      "|    value_loss           | 0.122         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=422500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 422500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 423000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 423500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 207      |\n",
      "|    time_elapsed    | 15995    |\n",
      "|    total_timesteps | 423936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 424000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012557341 |\n",
      "|    clip_fraction        | 0.01         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0625      |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0573       |\n",
      "|    n_updates            | 2070         |\n",
      "|    policy_gradient_loss | -0.000679    |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=424500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 424500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=425000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 425000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=425500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 425500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 16073    |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=426000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 426000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036647858 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0799      |\n",
      "|    explained_variance   | 0.442        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0611       |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.00218     |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=426500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 426500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=427000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 427000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=427500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 427500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=428000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 428000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 209      |\n",
      "|    time_elapsed    | 16156    |\n",
      "|    total_timesteps | 428032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=428500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 428500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015164181 |\n",
      "|    clip_fraction        | 0.0152       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0639      |\n",
      "|    explained_variance   | 0.442        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0295       |\n",
      "|    n_updates            | 2090         |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    value_loss           | 0.13         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=429000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 429000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=429500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 429500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 430000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 16234    |\n",
      "|    total_timesteps | 430080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=430500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 430500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00486355 |\n",
      "|    clip_fraction        | 0.0367     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.115     |\n",
      "|    explained_variance   | 0.431      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0667     |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.00318   |\n",
      "|    value_loss           | 0.125      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=431000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 431000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=431500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 431500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 432000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 211      |\n",
      "|    time_elapsed    | 16312    |\n",
      "|    total_timesteps | 432128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 432500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00944907 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.35      |\n",
      "|    explained_variance   | 0.416      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0288     |\n",
      "|    n_updates            | 2110       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    value_loss           | 0.121      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=433000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 433000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=433500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 433500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=434000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 434000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 16390    |\n",
      "|    total_timesteps | 434176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=434500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 434500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015641093 |\n",
      "|    clip_fraction        | 0.0891      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 435000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=435500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 435500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 436000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 213      |\n",
      "|    time_elapsed    | 16467    |\n",
      "|    total_timesteps | 436224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=436500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 436500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061740726 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0945      |\n",
      "|    explained_variance   | 0.438        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.114        |\n",
      "|    n_updates            | 2130         |\n",
      "|    policy_gradient_loss | -0.00231     |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=437000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 437000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=437500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 437500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=438000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 438000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 16545    |\n",
      "|    total_timesteps | 438272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=438500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 438500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019653668 |\n",
      "|    clip_fraction        | 0.0129       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.077       |\n",
      "|    explained_variance   | 0.453        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0998       |\n",
      "|    n_updates            | 2140         |\n",
      "|    policy_gradient_loss | -0.000899    |\n",
      "|    value_loss           | 0.119        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=439000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 439000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=439500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 439500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 440000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 215      |\n",
      "|    time_elapsed    | 16623    |\n",
      "|    total_timesteps | 440320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 440500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002486282 |\n",
      "|    clip_fraction        | 0.0113      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0682     |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0837      |\n",
      "|    n_updates            | 2150        |\n",
      "|    policy_gradient_loss | -0.000789   |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=441000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 441000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=441500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 441500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=442000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 442000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 16701    |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=442500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 442500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00081033574 |\n",
      "|    clip_fraction        | 0.00786       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0609       |\n",
      "|    explained_variance   | 0.445         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0294        |\n",
      "|    n_updates            | 2160          |\n",
      "|    policy_gradient_loss | -0.000323     |\n",
      "|    value_loss           | 0.123         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=443000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 443000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 443500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 444000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 217      |\n",
      "|    time_elapsed    | 16779    |\n",
      "|    total_timesteps | 444416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=444500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 444500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018352134 |\n",
      "|    clip_fraction        | 0.00415      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.054       |\n",
      "|    explained_variance   | 0.439        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0973       |\n",
      "|    n_updates            | 2170         |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    value_loss           | 0.133        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=445000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 445000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=445500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 445500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=446000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 446000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 16857    |\n",
      "|    total_timesteps | 446464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=446500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 446500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026000692 |\n",
      "|    clip_fraction        | 0.0108       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0593      |\n",
      "|    explained_variance   | 0.456        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.046        |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=447000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 447000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=447500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 447500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 448000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=448500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 448500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 219      |\n",
      "|    time_elapsed    | 16941    |\n",
      "|    total_timesteps | 448512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=449000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 449000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012962089 |\n",
      "|    clip_fraction        | 0.00542      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0686      |\n",
      "|    explained_variance   | 0.449        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.06         |\n",
      "|    n_updates            | 2190         |\n",
      "|    policy_gradient_loss | -0.000487    |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=449500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 449500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 17018    |\n",
      "|    total_timesteps | 450560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=451000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 451000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00086748326 |\n",
      "|    clip_fraction        | 0.00854       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0808       |\n",
      "|    explained_variance   | 0.438         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0569        |\n",
      "|    n_updates            | 2200          |\n",
      "|    policy_gradient_loss | -0.000516     |\n",
      "|    value_loss           | 0.128         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=451500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 451500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=452000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 452000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=452500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 452500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 221      |\n",
      "|    time_elapsed    | 17096    |\n",
      "|    total_timesteps | 452608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=453000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 453000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001042136 |\n",
      "|    clip_fraction        | 0.00937     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0926     |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.131       |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | -0.000429   |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=453500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 453500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=454000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 454000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=454500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 454500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 17174    |\n",
      "|    total_timesteps | 454656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=455000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 455000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004168945 |\n",
      "|    clip_fraction        | 0.0143      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0975     |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0734      |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.00116    |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=455500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 455500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 456000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=456500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 456500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 223      |\n",
      "|    time_elapsed    | 17252    |\n",
      "|    total_timesteps | 456704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=457000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 457000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030888566 |\n",
      "|    clip_fraction        | 0.0191       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0907      |\n",
      "|    explained_variance   | 0.465        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0376       |\n",
      "|    n_updates            | 2230         |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=457500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 457500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=458000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 458000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=458500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 458500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 17330    |\n",
      "|    total_timesteps | 458752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=459000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 459000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063431635 |\n",
      "|    clip_fraction        | 0.00923      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0562      |\n",
      "|    explained_variance   | 0.452        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0464       |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.000913    |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=459500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 459500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 225      |\n",
      "|    time_elapsed    | 17408    |\n",
      "|    total_timesteps | 460800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=461000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 461000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09917505 |\n",
      "|    clip_fraction        | 0.0203     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.131     |\n",
      "|    explained_variance   | 0.435      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0605     |\n",
      "|    n_updates            | 2250       |\n",
      "|    policy_gradient_loss | -0.000414  |\n",
      "|    value_loss           | 0.127      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=461500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 461500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=462000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 462000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=462500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 462500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 17486    |\n",
      "|    total_timesteps | 462848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 463000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057189064 |\n",
      "|    clip_fraction        | 0.0644       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.4         |\n",
      "|    explained_variance   | 0.456        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0462       |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | 0.00105      |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=463500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 463500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 464000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=464500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 464500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 227      |\n",
      "|    time_elapsed    | 17564    |\n",
      "|    total_timesteps | 464896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=465000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 465000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062520746 |\n",
      "|    clip_fraction        | 0.0728       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.468       |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0323       |\n",
      "|    n_updates            | 2270         |\n",
      "|    policy_gradient_loss | 0.00161      |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=465500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 465500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=466000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 466000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=466500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 466500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 17642    |\n",
      "|    total_timesteps | 466944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=467000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 467000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028340358 |\n",
      "|    clip_fraction        | 0.0398       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.417       |\n",
      "|    explained_variance   | 0.461        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0405       |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | 0.000846     |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=467500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 467500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 468000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=468500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 468500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 229      |\n",
      "|    time_elapsed    | 17720    |\n",
      "|    total_timesteps | 468992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=469000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 469000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019840226 |\n",
      "|    clip_fraction        | 0.0452       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.386       |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0588       |\n",
      "|    n_updates            | 2290         |\n",
      "|    policy_gradient_loss | 0.00112      |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=469500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 469500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=471000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 471000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 17803    |\n",
      "|    total_timesteps | 471040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=471500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 471500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001493209 |\n",
      "|    clip_fraction        | 0.0388      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0239      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.000598   |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=472000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 472000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=472500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 472500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=473000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 473000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 231      |\n",
      "|    time_elapsed    | 17881    |\n",
      "|    total_timesteps | 473088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=473500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 473500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030110804 |\n",
      "|    clip_fraction        | 0.0197       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.352       |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0728       |\n",
      "|    n_updates            | 2310         |\n",
      "|    policy_gradient_loss | 0.000482     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=474000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 474000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=474500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 474500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=475000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 475000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 17959    |\n",
      "|    total_timesteps | 475136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=475500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 475500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033893406 |\n",
      "|    clip_fraction        | 0.0322       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.261       |\n",
      "|    explained_variance   | 0.443        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0383       |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    value_loss           | 0.129        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=476000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 476000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=476500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 476500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=477000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 477000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 233      |\n",
      "|    time_elapsed    | 18037    |\n",
      "|    total_timesteps | 477184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=477500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 477500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004845985 |\n",
      "|    clip_fraction        | 0.0363      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.288      |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0563      |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | -0.00295    |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=478000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 478000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=478500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 478500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=479000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 479000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 18115    |\n",
      "|    total_timesteps | 479232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=479500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 479500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016048804 |\n",
      "|    clip_fraction        | 0.0244       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.262       |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0523       |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.00139     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 480000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 480500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=481000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 481000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 235      |\n",
      "|    time_elapsed    | 18194    |\n",
      "|    total_timesteps | 481280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=481500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 481500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001846683 |\n",
      "|    clip_fraction        | 0.014       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.257      |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0755      |\n",
      "|    n_updates            | 2350        |\n",
      "|    policy_gradient_loss | 0.000151    |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=482000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 482000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=482500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 482500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 483000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 18272    |\n",
      "|    total_timesteps | 483328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 483500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029693278 |\n",
      "|    clip_fraction        | 0.0251       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.273       |\n",
      "|    explained_variance   | 0.457        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0887       |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.000754    |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=484000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 484000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=484500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 484500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=485000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 485000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 237      |\n",
      "|    time_elapsed    | 18350    |\n",
      "|    total_timesteps | 485376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=485500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 485500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002481917 |\n",
      "|    clip_fraction        | 0.015       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.23       |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0422      |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.000151   |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=486000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 486000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=486500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 486500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=487000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 487000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 18428    |\n",
      "|    total_timesteps | 487424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=487500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 487500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039626174 |\n",
      "|    clip_fraction        | 0.0324       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.254       |\n",
      "|    explained_variance   | 0.432        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0475       |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -0.00278     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=488000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 488000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=488500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 488500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=489000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 489000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 239      |\n",
      "|    time_elapsed    | 18506    |\n",
      "|    total_timesteps | 489472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=489500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 489500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012868952 |\n",
      "|    clip_fraction        | 0.0195       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.204       |\n",
      "|    explained_variance   | 0.449        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0744       |\n",
      "|    n_updates            | 2390         |\n",
      "|    policy_gradient_loss | -0.000958    |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 490000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 490500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=491000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 491000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=491500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 491500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 18590    |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 492000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010605098 |\n",
      "|    clip_fraction        | 0.013        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.199       |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0397       |\n",
      "|    n_updates            | 2400         |\n",
      "|    policy_gradient_loss | -0.000594    |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=492500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 492500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=493000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 493000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=493500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 493500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 241      |\n",
      "|    time_elapsed    | 18668    |\n",
      "|    total_timesteps | 493568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=494000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 494000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013488034 |\n",
      "|    clip_fraction        | 0.0209       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.228       |\n",
      "|    explained_variance   | 0.451        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0722       |\n",
      "|    n_updates            | 2410         |\n",
      "|    policy_gradient_loss | -0.000949    |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=494500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 494500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=495000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 495000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=495500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 495500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 18746    |\n",
      "|    total_timesteps | 495616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 496000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085554615 |\n",
      "|    clip_fraction        | 0.0163        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.265        |\n",
      "|    explained_variance   | 0.451         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0465        |\n",
      "|    n_updates            | 2420          |\n",
      "|    policy_gradient_loss | -0.000267     |\n",
      "|    value_loss           | 0.126         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=496500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 496500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=497000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 497000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=497500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 497500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 243      |\n",
      "|    time_elapsed    | 18824    |\n",
      "|    total_timesteps | 497664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=498000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 498000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020034043 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.254       |\n",
      "|    explained_variance   | 0.449        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0522       |\n",
      "|    n_updates            | 2430         |\n",
      "|    policy_gradient_loss | -0.000529    |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=498500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 498500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=499000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 499000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=499500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 499500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 18902    |\n",
      "|    total_timesteps | 499712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 500000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033605474 |\n",
      "|    clip_fraction        | 0.039        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.245       |\n",
      "|    explained_variance   | 0.431        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0484       |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.0042      |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=500500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=501000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 501000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=501500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 501500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 245      |\n",
      "|    time_elapsed    | 18980    |\n",
      "|    total_timesteps | 501760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=502000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 502000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002107753 |\n",
      "|    clip_fraction        | 0.012       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.233      |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0481      |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | -0.000564   |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=502500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 502500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=503000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 503000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=503500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 503500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 19058    |\n",
      "|    total_timesteps | 503808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007919975 |\n",
      "|    clip_fraction        | 0.0805      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.356      |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0692      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=504500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 504500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=505000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 505000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=505500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 505500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 247      |\n",
      "|    time_elapsed    | 19136    |\n",
      "|    total_timesteps | 505856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=506000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 506000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037468215 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.3         |\n",
      "|    explained_variance   | 0.427        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0341       |\n",
      "|    n_updates            | 2470         |\n",
      "|    policy_gradient_loss | -0.00438     |\n",
      "|    value_loss           | 0.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=506500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 506500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=507000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 507000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=507500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 507500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 19215    |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=508000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 508000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033252728 |\n",
      "|    clip_fraction        | 0.0349       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.285       |\n",
      "|    explained_variance   | 0.451        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0629       |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=508500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 508500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=509000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 509000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=509500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 509500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 249      |\n",
      "|    time_elapsed    | 19293    |\n",
      "|    total_timesteps | 509952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 510000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002111126 |\n",
      "|    clip_fraction        | 0.0249      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.241      |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 2490        |\n",
      "|    policy_gradient_loss | -0.00124    |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=510500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 510500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=511000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 511000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=511500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 511500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 19377    |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 512500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015792511 |\n",
      "|    clip_fraction        | 0.0244       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.247       |\n",
      "|    explained_variance   | 0.444        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0312       |\n",
      "|    n_updates            | 2500         |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=513000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 513000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=513500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 513500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=514000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 514000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 251      |\n",
      "|    time_elapsed    | 19455    |\n",
      "|    total_timesteps | 514048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=514500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 514500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031582196 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.238       |\n",
      "|    explained_variance   | 0.466        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0507       |\n",
      "|    n_updates            | 2510         |\n",
      "|    policy_gradient_loss | -0.00175     |\n",
      "|    value_loss           | 0.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=515000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 515000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=515500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 515500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=516000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 516000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 19533    |\n",
      "|    total_timesteps | 516096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=516500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 516500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000924431 |\n",
      "|    clip_fraction        | 0.00713     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.208      |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0643      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.000235   |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=517000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 517000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=517500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 517500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=518000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 518000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 253      |\n",
      "|    time_elapsed    | 19611    |\n",
      "|    total_timesteps | 518144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=518500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 518500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015290566 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.197       |\n",
      "|    explained_variance   | 0.451        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0768       |\n",
      "|    n_updates            | 2530         |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    value_loss           | 0.125        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=519000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 519000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=519500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 519500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 520000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 19689    |\n",
      "|    total_timesteps | 520192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 520500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015182003 |\n",
      "|    clip_fraction        | 0.0193       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.2         |\n",
      "|    explained_variance   | 0.44         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0226       |\n",
      "|    n_updates            | 2540         |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=521000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 521000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=521500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 521500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=522000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 522000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 255      |\n",
      "|    time_elapsed    | 19767    |\n",
      "|    total_timesteps | 522240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=522500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 522500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008189563 |\n",
      "|    clip_fraction        | 0.0153       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.197       |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0386       |\n",
      "|    n_updates            | 2550         |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=523000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 523000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=523500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 523500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 524000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 19846    |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 524500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027384255 |\n",
      "|    clip_fraction        | 0.0228       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.218       |\n",
      "|    explained_variance   | 0.443        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.069        |\n",
      "|    n_updates            | 2560         |\n",
      "|    policy_gradient_loss | -0.000739    |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=525000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 525000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=525500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 525500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=526000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 526000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 257      |\n",
      "|    time_elapsed    | 19924    |\n",
      "|    total_timesteps | 526336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=526500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 526500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001316207 |\n",
      "|    clip_fraction        | 0.0146      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.202      |\n",
      "|    explained_variance   | 0.441       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.065       |\n",
      "|    n_updates            | 2570        |\n",
      "|    policy_gradient_loss | -0.001      |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=527000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 527000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=527500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 527500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 528000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 20002    |\n",
      "|    total_timesteps | 528384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 528500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002634803 |\n",
      "|    clip_fraction        | 0.0257      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.237      |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0275      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.00169    |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=529000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 529000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=529500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 529500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 530000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 259      |\n",
      "|    time_elapsed    | 20080    |\n",
      "|    total_timesteps | 530432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 530500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017999638 |\n",
      "|    clip_fraction        | 0.0108       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.216       |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0628       |\n",
      "|    n_updates            | 2590         |\n",
      "|    policy_gradient_loss | -0.000358    |\n",
      "|    value_loss           | 0.129        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=531000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 531000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=531500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 531500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=532000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 532000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 20158    |\n",
      "|    total_timesteps | 532480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=532500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 532500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017744862 |\n",
      "|    clip_fraction        | 0.0211       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.208       |\n",
      "|    explained_variance   | 0.445        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0381       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    value_loss           | 0.125        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=533000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 533000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=533500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 533500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=534000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 534000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=534500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 534500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 261      |\n",
      "|    time_elapsed    | 20242    |\n",
      "|    total_timesteps | 534528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=535000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 535000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063085477 |\n",
      "|    clip_fraction        | 0.0365       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.283       |\n",
      "|    explained_variance   | 0.471        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0323       |\n",
      "|    n_updates            | 2610         |\n",
      "|    policy_gradient_loss | -0.00218     |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=535500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 535500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=536000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 536000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=536500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 536500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 20320    |\n",
      "|    total_timesteps | 536576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=537000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 537000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008916587 |\n",
      "|    clip_fraction        | 0.0679      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.323      |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0674      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=537500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 537500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=538000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 538000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=538500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 538500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 263      |\n",
      "|    time_elapsed    | 20398    |\n",
      "|    total_timesteps | 538624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=539000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 539000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012398808 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.349      |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0314      |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=539500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 539500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 20477    |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=541000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 541000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010695564 |\n",
      "|    clip_fraction        | 0.0747      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.306      |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00237    |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.00962    |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=541500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 541500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=542000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 542000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=542500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 542500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 265      |\n",
      "|    time_elapsed    | 20555    |\n",
      "|    total_timesteps | 542720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=543000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 543000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051369728 |\n",
      "|    clip_fraction        | 0.029        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.258       |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0611       |\n",
      "|    n_updates            | 2650         |\n",
      "|    policy_gradient_loss | -0.00184     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=543500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 543500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 544000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 544500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 20633    |\n",
      "|    total_timesteps | 544768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=545000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 545000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058401856 |\n",
      "|    clip_fraction        | 0.0287       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.25        |\n",
      "|    explained_variance   | 0.441        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0458       |\n",
      "|    n_updates            | 2660         |\n",
      "|    policy_gradient_loss | -0.000841    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=545500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 545500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=546000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 546000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=546500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 546500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 267      |\n",
      "|    time_elapsed    | 20712    |\n",
      "|    total_timesteps | 546816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=547000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 547000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019417665 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.162       |\n",
      "|    explained_variance   | 0.433        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.108        |\n",
      "|    n_updates            | 2670         |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    value_loss           | 0.13         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=547500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 547500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=548000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 548000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=548500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 548500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 20791    |\n",
      "|    total_timesteps | 548864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=549000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 549000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011270046 |\n",
      "|    clip_fraction        | 0.0178       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.141       |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0294       |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | -0.00157     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=549500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 549500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 269      |\n",
      "|    time_elapsed    | 20869    |\n",
      "|    total_timesteps | 550912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=551000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 551000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025605434 |\n",
      "|    clip_fraction        | 0.0154       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.171       |\n",
      "|    explained_variance   | 0.451        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0962       |\n",
      "|    n_updates            | 2690         |\n",
      "|    policy_gradient_loss | -0.000754    |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=551500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 551500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=552000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 552000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=552500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 552500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 20948    |\n",
      "|    total_timesteps | 552960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=553000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 553000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004943867 |\n",
      "|    clip_fraction        | 0.0108       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0562       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.000971    |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=553500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 553500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=554000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 554000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=554500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 554500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=555000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 555000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 271      |\n",
      "|    time_elapsed    | 21032    |\n",
      "|    total_timesteps | 555008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=555500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 555500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005289414 |\n",
      "|    clip_fraction        | 0.00518      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.126       |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.065        |\n",
      "|    n_updates            | 2710         |\n",
      "|    policy_gradient_loss | -0.000457    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=556000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 556000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=556500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 556500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=557000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 557000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 21111    |\n",
      "|    total_timesteps | 557056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=557500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 557500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027403882 |\n",
      "|    clip_fraction        | 0.00342       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.108        |\n",
      "|    explained_variance   | 0.449         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0196        |\n",
      "|    n_updates            | 2720          |\n",
      "|    policy_gradient_loss | 6.06e-05      |\n",
      "|    value_loss           | 0.122         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=558000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 558000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=558500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 558500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=559000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 559000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 273      |\n",
      "|    time_elapsed    | 21189    |\n",
      "|    total_timesteps | 559104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=559500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 559500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000645041 |\n",
      "|    clip_fraction        | 0.00684     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.107      |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0794      |\n",
      "|    n_updates            | 2730        |\n",
      "|    policy_gradient_loss | -0.000488   |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 560000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 560500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=561000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 561000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 21268    |\n",
      "|    total_timesteps | 561152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=561500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 561500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032243263 |\n",
      "|    clip_fraction        | 0.0286       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.141       |\n",
      "|    explained_variance   | 0.433        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0197       |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=562000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 562000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=562500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 562500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=563000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 563000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 275      |\n",
      "|    time_elapsed    | 21346    |\n",
      "|    total_timesteps | 563200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=563500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 563500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033185692 |\n",
      "|    clip_fraction        | 0.015        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.123       |\n",
      "|    explained_variance   | 0.425        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0754       |\n",
      "|    n_updates            | 2750         |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=564000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 564000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 564500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=565000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 565000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 21425    |\n",
      "|    total_timesteps | 565248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=565500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 565500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028811172 |\n",
      "|    clip_fraction        | 0.00425      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.122       |\n",
      "|    explained_variance   | 0.465        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0335       |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.000404    |\n",
      "|    value_loss           | 0.119        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=566000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 566000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=566500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 566500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=567000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 567000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 277      |\n",
      "|    time_elapsed    | 21503    |\n",
      "|    total_timesteps | 567296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=567500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 567500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00041831535 |\n",
      "|    clip_fraction        | 0.00801       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.11         |\n",
      "|    explained_variance   | 0.449         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0407        |\n",
      "|    n_updates            | 2770          |\n",
      "|    policy_gradient_loss | -0.000358     |\n",
      "|    value_loss           | 0.123         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=568000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 568000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=568500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 568500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=569000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 569000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 21582    |\n",
      "|    total_timesteps | 569344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=569500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 569500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00081920257 |\n",
      "|    clip_fraction        | 0.00688       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.124        |\n",
      "|    explained_variance   | 0.448         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.031         |\n",
      "|    n_updates            | 2780          |\n",
      "|    policy_gradient_loss | -0.00062      |\n",
      "|    value_loss           | 0.128         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 570000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 570500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=571000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 571000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 279      |\n",
      "|    time_elapsed    | 21661    |\n",
      "|    total_timesteps | 571392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=571500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 571500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00072362024 |\n",
      "|    clip_fraction        | 0.0133        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.133        |\n",
      "|    explained_variance   | 0.446         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0687        |\n",
      "|    n_updates            | 2790          |\n",
      "|    policy_gradient_loss | -0.000291     |\n",
      "|    value_loss           | 0.123         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=572000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 572000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=572500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 572500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=573000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 573000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 21739    |\n",
      "|    total_timesteps | 573440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=573500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 573500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069385367 |\n",
      "|    clip_fraction        | 0.0334       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.191       |\n",
      "|    explained_variance   | 0.452        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0308       |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.00339     |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=574000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 574000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=574500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 574500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=575000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 575000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 281      |\n",
      "|    time_elapsed    | 21818    |\n",
      "|    total_timesteps | 575488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=575500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 575500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00066555967 |\n",
      "|    clip_fraction        | 0.00361       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.155        |\n",
      "|    explained_variance   | 0.455         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0869        |\n",
      "|    n_updates            | 2810          |\n",
      "|    policy_gradient_loss | 7.97e-05      |\n",
      "|    value_loss           | 0.119         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 576000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=576500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 576500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=577000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 577000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=577500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 577500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 21902    |\n",
      "|    total_timesteps | 577536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=578000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 578000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009808159 |\n",
      "|    clip_fraction        | 0.00996      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.161       |\n",
      "|    explained_variance   | 0.452        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0779       |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -5.47e-05    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=578500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 578500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=579000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 579000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=579500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 579500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 283      |\n",
      "|    time_elapsed    | 21981    |\n",
      "|    total_timesteps | 579584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 580000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025281808 |\n",
      "|    clip_fraction        | 0.0303       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.214       |\n",
      "|    explained_variance   | 0.457        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0246       |\n",
      "|    n_updates            | 2830         |\n",
      "|    policy_gradient_loss | -0.000745    |\n",
      "|    value_loss           | 0.118        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=580500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 580500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=581000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 581000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=581500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 581500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 22059    |\n",
      "|    total_timesteps | 581632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=582000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 582000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025232676 |\n",
      "|    clip_fraction        | 0.0198       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.162       |\n",
      "|    explained_variance   | 0.445        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0569       |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.000828    |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=582500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 582500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=583000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 583000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=583500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 583500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 285      |\n",
      "|    time_elapsed    | 22138    |\n",
      "|    total_timesteps | 583680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 584000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003801422 |\n",
      "|    clip_fraction        | 0.0321      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.144      |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.051       |\n",
      "|    n_updates            | 2850        |\n",
      "|    policy_gradient_loss | -0.00159    |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=584500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 584500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=585000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 585000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=585500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 585500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 22216    |\n",
      "|    total_timesteps | 585728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=586000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 586000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008800245 |\n",
      "|    clip_fraction        | 0.0841      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.212      |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0676      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=586500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 586500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=587000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 587000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=587500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 587500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 287      |\n",
      "|    time_elapsed    | 22294    |\n",
      "|    total_timesteps | 587776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=588000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 588000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065060966 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.151       |\n",
      "|    explained_variance   | 0.466        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0441       |\n",
      "|    n_updates            | 2870         |\n",
      "|    policy_gradient_loss | -0.00581     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=588500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 588500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=589000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 589000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=589500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 589500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 22373    |\n",
      "|    total_timesteps | 589824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 590000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039396617 |\n",
      "|    clip_fraction        | 0.0242       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.101       |\n",
      "|    explained_variance   | 0.459        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0343       |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.00208     |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=590500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 590500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=591000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 591000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=591500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 591500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 289      |\n",
      "|    time_elapsed    | 22451    |\n",
      "|    total_timesteps | 591872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 592000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030242587 |\n",
      "|    clip_fraction        | 0.0103       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0894      |\n",
      "|    explained_variance   | 0.43         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0329       |\n",
      "|    n_updates            | 2890         |\n",
      "|    policy_gradient_loss | -0.000869    |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=592500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 592500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=593000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 593000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=593500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 593500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 22530    |\n",
      "|    total_timesteps | 593920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=594000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 594000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019116366 |\n",
      "|    clip_fraction        | 0.00645      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.08        |\n",
      "|    explained_variance   | 0.452        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0382       |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.000728    |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=594500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 594500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=595000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 595000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=595500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 595500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 291      |\n",
      "|    time_elapsed    | 22609    |\n",
      "|    total_timesteps | 595968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=596000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 596000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00095287175 |\n",
      "|    clip_fraction        | 0.00679       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0872       |\n",
      "|    explained_variance   | 0.449         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.022         |\n",
      "|    n_updates            | 2910          |\n",
      "|    policy_gradient_loss | -0.000167     |\n",
      "|    value_loss           | 0.121         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=596500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 596500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=597000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 597000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=597500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 597500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=598000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 598000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 22693    |\n",
      "|    total_timesteps | 598016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=598500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 598500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004999747 |\n",
      "|    clip_fraction        | 0.00361      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0956      |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0883       |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.000116    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=599000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 599000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=599500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 599500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 600000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 293      |\n",
      "|    time_elapsed    | 22771    |\n",
      "|    total_timesteps | 600064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 600500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016463042 |\n",
      "|    clip_fraction        | 0.00898      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0951      |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0585       |\n",
      "|    n_updates            | 2930         |\n",
      "|    policy_gradient_loss | -0.000641    |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=601000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 601000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=601500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 601500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=602000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 602000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 22850    |\n",
      "|    total_timesteps | 602112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=602500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 602500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010679646 |\n",
      "|    clip_fraction        | 0.00474      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.089       |\n",
      "|    explained_variance   | 0.449        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0879       |\n",
      "|    n_updates            | 2940         |\n",
      "|    policy_gradient_loss | -4.98e-05    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=603000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 603000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=603500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 603500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 604000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 295      |\n",
      "|    time_elapsed    | 22928    |\n",
      "|    total_timesteps | 604160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 604500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005646674 |\n",
      "|    clip_fraction        | 0.00527      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0864      |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0601       |\n",
      "|    n_updates            | 2950         |\n",
      "|    policy_gradient_loss | -0.000295    |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=605000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 605000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=605500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 605500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=606000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 606000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 23007    |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=606500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 606500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012974415 |\n",
      "|    clip_fraction        | 0.00781      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0791      |\n",
      "|    explained_variance   | 0.453        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0517       |\n",
      "|    n_updates            | 2960         |\n",
      "|    policy_gradient_loss | -0.000508    |\n",
      "|    value_loss           | 0.125        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=607000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 607000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=607500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 607500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=608000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 608000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 297      |\n",
      "|    time_elapsed    | 23086    |\n",
      "|    total_timesteps | 608256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=608500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 608500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005159377 |\n",
      "|    clip_fraction        | 0.00474      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0836      |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0961       |\n",
      "|    n_updates            | 2970         |\n",
      "|    policy_gradient_loss | -0.000412    |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=609000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 609000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=609500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 609500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 610000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 23165    |\n",
      "|    total_timesteps | 610304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 610500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038568597 |\n",
      "|    clip_fraction        | 0.00288       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0807       |\n",
      "|    explained_variance   | 0.447         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0439        |\n",
      "|    n_updates            | 2980          |\n",
      "|    policy_gradient_loss | -0.000283     |\n",
      "|    value_loss           | 0.127         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=611000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 611000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=611500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 611500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=612000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 612000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 299      |\n",
      "|    time_elapsed    | 23243    |\n",
      "|    total_timesteps | 612352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=612500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 612500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011557194 |\n",
      "|    clip_fraction        | 0.00547      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0668      |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0747       |\n",
      "|    n_updates            | 2990         |\n",
      "|    policy_gradient_loss | -0.000278    |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=613000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 613000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=613500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 613500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=614000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 614000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 23322    |\n",
      "|    total_timesteps | 614400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=614500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 614500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011177243 |\n",
      "|    clip_fraction        | 0.0115       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0613      |\n",
      "|    explained_variance   | 0.439        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0978       |\n",
      "|    n_updates            | 3000         |\n",
      "|    policy_gradient_loss | -0.000647    |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=615000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 615000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=615500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 615500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=616000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 616000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 301      |\n",
      "|    time_elapsed    | 23401    |\n",
      "|    total_timesteps | 616448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=616500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 616500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011409692 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.246      |\n",
      "|    explained_variance   | 0.376       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0635      |\n",
      "|    n_updates            | 3010        |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=617000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 617000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=617500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 617500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=618000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 618000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.09    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 302      |\n",
      "|    time_elapsed    | 23479    |\n",
      "|    total_timesteps | 618496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=618500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 618500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013903275 |\n",
      "|    clip_fraction        | 0.0923      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.271      |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0219      |\n",
      "|    n_updates            | 3020        |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=619000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 619000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=619500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 619500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 303      |\n",
      "|    time_elapsed    | 23564    |\n",
      "|    total_timesteps | 620544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=621000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 621000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012991531 |\n",
      "|    clip_fraction        | 0.0693      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.17       |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0469      |\n",
      "|    n_updates            | 3030        |\n",
      "|    policy_gradient_loss | -0.0086     |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=621500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 621500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=622000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 622000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=622500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 622500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 304      |\n",
      "|    time_elapsed    | 23643    |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=623000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 623000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070089875 |\n",
      "|    clip_fraction        | 0.0243       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0608      |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0392       |\n",
      "|    n_updates            | 3040         |\n",
      "|    policy_gradient_loss | -0.00248     |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=623500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 623500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 624000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 624500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 305      |\n",
      "|    time_elapsed    | 23721    |\n",
      "|    total_timesteps | 624640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=625000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 625000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037093842 |\n",
      "|    clip_fraction        | 0.00723      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0493      |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0576       |\n",
      "|    n_updates            | 3050         |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=625500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 625500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=626000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 626000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=626500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 626500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 306      |\n",
      "|    time_elapsed    | 23800    |\n",
      "|    total_timesteps | 626688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=627000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 627000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00052341993 |\n",
      "|    clip_fraction        | 0.00327       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0504       |\n",
      "|    explained_variance   | 0.447         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0605        |\n",
      "|    n_updates            | 3060          |\n",
      "|    policy_gradient_loss | -0.000572     |\n",
      "|    value_loss           | 0.124         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=627500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 627500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=628000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 628000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=628500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 628500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 307      |\n",
      "|    time_elapsed    | 23879    |\n",
      "|    total_timesteps | 628736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=629000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 629000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043796623 |\n",
      "|    clip_fraction        | 0.00249       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0459       |\n",
      "|    explained_variance   | 0.443         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.1           |\n",
      "|    n_updates            | 3070          |\n",
      "|    policy_gradient_loss | -0.000107     |\n",
      "|    value_loss           | 0.13          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=629500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 629500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 308      |\n",
      "|    time_elapsed    | 23957    |\n",
      "|    total_timesteps | 630784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=631000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 631000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016485284 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0572      |\n",
      "|    explained_variance   | 0.441        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0808       |\n",
      "|    n_updates            | 3080         |\n",
      "|    policy_gradient_loss | -0.000778    |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=631500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 631500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=632000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 632000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=632500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 632500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 309      |\n",
      "|    time_elapsed    | 24036    |\n",
      "|    total_timesteps | 632832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=633000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 633000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012697973 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0621      |\n",
      "|    explained_variance   | 0.438        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0605       |\n",
      "|    n_updates            | 3090         |\n",
      "|    policy_gradient_loss | -0.000589    |\n",
      "|    value_loss           | 0.131        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=633500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 633500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=634000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 634000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=634500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 634500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 310      |\n",
      "|    time_elapsed    | 24115    |\n",
      "|    total_timesteps | 634880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=635000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 635000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049242293 |\n",
      "|    clip_fraction        | 0.0387       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0847      |\n",
      "|    explained_variance   | 0.454        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0477       |\n",
      "|    n_updates            | 3100         |\n",
      "|    policy_gradient_loss | -0.0032      |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=635500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 635500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=636000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 636000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=636500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 636500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 311      |\n",
      "|    time_elapsed    | 24193    |\n",
      "|    total_timesteps | 636928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=637000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 637000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010865174 |\n",
      "|    clip_fraction        | 0.0839      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.274      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.064       |\n",
      "|    n_updates            | 3110        |\n",
      "|    policy_gradient_loss | -0.00711    |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=637500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 637500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=638000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 638000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=638500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 638500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 312      |\n",
      "|    time_elapsed    | 24272    |\n",
      "|    total_timesteps | 638976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=639000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 639000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010195175 |\n",
      "|    clip_fraction        | 0.0494      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.114      |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0106      |\n",
      "|    n_updates            | 3120        |\n",
      "|    policy_gradient_loss | -0.00711    |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=639500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 639500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=641000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 641000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 313      |\n",
      "|    time_elapsed    | 24357    |\n",
      "|    total_timesteps | 641024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=641500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 641500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008095948 |\n",
      "|    clip_fraction        | 0.0667      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.244      |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0448      |\n",
      "|    n_updates            | 3130        |\n",
      "|    policy_gradient_loss | -0.00559    |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=642000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 642000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=642500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 642500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=643000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 643000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 314      |\n",
      "|    time_elapsed    | 24435    |\n",
      "|    total_timesteps | 643072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=643500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 643500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025401125 |\n",
      "|    clip_fraction        | 0.0221       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0904      |\n",
      "|    explained_variance   | 0.445        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0346       |\n",
      "|    n_updates            | 3140         |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=644000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 644000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=644500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 644500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 645000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 315      |\n",
      "|    time_elapsed    | 24514    |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 645500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022696746 |\n",
      "|    clip_fraction        | 0.00767      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0475      |\n",
      "|    explained_variance   | 0.445        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0633       |\n",
      "|    n_updates            | 3150         |\n",
      "|    policy_gradient_loss | -0.000739    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=646000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 646000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=646500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 646500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=647000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 647000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 316      |\n",
      "|    time_elapsed    | 24593    |\n",
      "|    total_timesteps | 647168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=647500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 647500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003374068 |\n",
      "|    clip_fraction        | 0.0241      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.09       |\n",
      "|    explained_variance   | 0.407       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0303      |\n",
      "|    n_updates            | 3160        |\n",
      "|    policy_gradient_loss | -0.00288    |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=648000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 648000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=648500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 648500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=649000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 649000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 317      |\n",
      "|    time_elapsed    | 24672    |\n",
      "|    total_timesteps | 649216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=649500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 649500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020004401 |\n",
      "|    clip_fraction        | 0.0179       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0816      |\n",
      "|    explained_variance   | 0.459        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0612       |\n",
      "|    n_updates            | 3170         |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    value_loss           | 0.13         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 650000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 650500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=651000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 651000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 318      |\n",
      "|    time_elapsed    | 24751    |\n",
      "|    total_timesteps | 651264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=651500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 651500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059386725 |\n",
      "|    clip_fraction        | 0.0599       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.23        |\n",
      "|    explained_variance   | 0.418        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0686       |\n",
      "|    n_updates            | 3180         |\n",
      "|    policy_gradient_loss | -0.0044      |\n",
      "|    value_loss           | 0.129        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=652000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 652000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=652500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 652500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=653000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 653000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 319      |\n",
      "|    time_elapsed    | 24829    |\n",
      "|    total_timesteps | 653312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=653500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 653500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00383303 |\n",
      "|    clip_fraction        | 0.0206     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0972    |\n",
      "|    explained_variance   | 0.465      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0857     |\n",
      "|    n_updates            | 3190       |\n",
      "|    policy_gradient_loss | -0.00188   |\n",
      "|    value_loss           | 0.127      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=654000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 654000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=654500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 654500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=655000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 655000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 320      |\n",
      "|    time_elapsed    | 24908    |\n",
      "|    total_timesteps | 655360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=655500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 655500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008100577 |\n",
      "|    clip_fraction        | 0.0325      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0888     |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0679      |\n",
      "|    n_updates            | 3200        |\n",
      "|    policy_gradient_loss | -0.00405    |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=656000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 656000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=656500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 656500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=657000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 657000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 321      |\n",
      "|    time_elapsed    | 24987    |\n",
      "|    total_timesteps | 657408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=657500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 657500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00414743 |\n",
      "|    clip_fraction        | 0.0351     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0848    |\n",
      "|    explained_variance   | 0.397      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0526     |\n",
      "|    n_updates            | 3210       |\n",
      "|    policy_gradient_loss | -0.00873   |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=658000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 658000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=658500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 658500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=659000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 659000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 322      |\n",
      "|    time_elapsed    | 25066    |\n",
      "|    total_timesteps | 659456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=659500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 659500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010855754 |\n",
      "|    clip_fraction        | 0.0179       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0846      |\n",
      "|    explained_variance   | 0.443        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.049        |\n",
      "|    n_updates            | 3220         |\n",
      "|    policy_gradient_loss | -0.000731    |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 660000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 660500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=661000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 661000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=661500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 661500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 323      |\n",
      "|    time_elapsed    | 25150    |\n",
      "|    total_timesteps | 661504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=662000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 662000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042626285 |\n",
      "|    clip_fraction        | 0.0647       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.249       |\n",
      "|    explained_variance   | 0.426        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0787       |\n",
      "|    n_updates            | 3230         |\n",
      "|    policy_gradient_loss | -0.00393     |\n",
      "|    value_loss           | 0.125        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=662500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 662500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=663000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 663000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=663500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 663500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 324      |\n",
      "|    time_elapsed    | 25229    |\n",
      "|    total_timesteps | 663552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=664000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 664000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013651053 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.123       |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0671       |\n",
      "|    n_updates            | 3240         |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=664500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 664500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 665000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 665500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 325      |\n",
      "|    time_elapsed    | 25308    |\n",
      "|    total_timesteps | 665600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=666000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 666000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033316757 |\n",
      "|    clip_fraction        | 0.0461       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.253       |\n",
      "|    explained_variance   | 0.437        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0407       |\n",
      "|    n_updates            | 3250         |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=666500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 666500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=667000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 667000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=667500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 667500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 326      |\n",
      "|    time_elapsed    | 25387    |\n",
      "|    total_timesteps | 667648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=668000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 668000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002034846 |\n",
      "|    clip_fraction        | 0.0284      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.14       |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0266      |\n",
      "|    n_updates            | 3260        |\n",
      "|    policy_gradient_loss | -0.00273    |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=668500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 668500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=669000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 669000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=669500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 669500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 327      |\n",
      "|    time_elapsed    | 25466    |\n",
      "|    total_timesteps | 669696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 670000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004677771 |\n",
      "|    clip_fraction        | 0.0576      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.251      |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0373      |\n",
      "|    n_updates            | 3270        |\n",
      "|    policy_gradient_loss | -0.00585    |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=670500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 670500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=671000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 671000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=671500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 671500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 328      |\n",
      "|    time_elapsed    | 25545    |\n",
      "|    total_timesteps | 671744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 672000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057240175 |\n",
      "|    clip_fraction        | 0.0406       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.184       |\n",
      "|    explained_variance   | 0.425        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0547       |\n",
      "|    n_updates            | 3280         |\n",
      "|    policy_gradient_loss | -0.00287     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=672500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 672500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=673000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 673000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=673500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 673500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 329      |\n",
      "|    time_elapsed    | 25624    |\n",
      "|    total_timesteps | 673792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=674000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 674000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009824457 |\n",
      "|    clip_fraction        | 0.0151       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.137       |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0496       |\n",
      "|    n_updates            | 3290         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    value_loss           | 0.116        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=674500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 674500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=675000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 675000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=675500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 675500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 330      |\n",
      "|    time_elapsed    | 25703    |\n",
      "|    total_timesteps | 675840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=676000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 676000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00076637365 |\n",
      "|    clip_fraction        | 0.00937       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0591       |\n",
      "|    explained_variance   | 0.457         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0204        |\n",
      "|    n_updates            | 3300          |\n",
      "|    policy_gradient_loss | -0.00131      |\n",
      "|    value_loss           | 0.118         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=676500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 676500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=677000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 677000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=677500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 677500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 331      |\n",
      "|    time_elapsed    | 25781    |\n",
      "|    total_timesteps | 677888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=678000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 678000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035939985 |\n",
      "|    clip_fraction        | 0.0406       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.141       |\n",
      "|    explained_variance   | 0.387        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0692       |\n",
      "|    n_updates            | 3310         |\n",
      "|    policy_gradient_loss | -0.00311     |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=678500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 678500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=679000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 679000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=679500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 679500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 332      |\n",
      "|    time_elapsed    | 25860    |\n",
      "|    total_timesteps | 679936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 680000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002500676 |\n",
      "|    clip_fraction        | 0.0279      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.195      |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.111       |\n",
      "|    n_updates            | 3320        |\n",
      "|    policy_gradient_loss | -0.00112    |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=680500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 680500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=681000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 681000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=681500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 681500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 333      |\n",
      "|    time_elapsed    | 25939    |\n",
      "|    total_timesteps | 681984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=682000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 682000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010663866 |\n",
      "|    clip_fraction        | 0.00854      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0798      |\n",
      "|    explained_variance   | 0.452        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0475       |\n",
      "|    n_updates            | 3330         |\n",
      "|    policy_gradient_loss | -0.000563    |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=682500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 682500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=683000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 683000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=683500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 683500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=684000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 684000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 334      |\n",
      "|    time_elapsed    | 26024    |\n",
      "|    total_timesteps | 684032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=684500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 684500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015679763 |\n",
      "|    clip_fraction        | 0.0157       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0934      |\n",
      "|    explained_variance   | 0.437        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0227       |\n",
      "|    n_updates            | 3340         |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=685000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 685000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=685500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 685500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=686000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 686000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 335      |\n",
      "|    time_elapsed    | 26102    |\n",
      "|    total_timesteps | 686080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=686500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 686500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017073331 |\n",
      "|    clip_fraction        | 0.0232       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.444        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0491       |\n",
      "|    n_updates            | 3350         |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    value_loss           | 0.125        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=687000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 687000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=687500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 687500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=688000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 688000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 336      |\n",
      "|    time_elapsed    | 26181    |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=688500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 688500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014559116 |\n",
      "|    clip_fraction        | 0.0129       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0761      |\n",
      "|    explained_variance   | 0.432        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0435       |\n",
      "|    n_updates            | 3360         |\n",
      "|    policy_gradient_loss | -0.00217     |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=689000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 689000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=689500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 689500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 690000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 337      |\n",
      "|    time_elapsed    | 26260    |\n",
      "|    total_timesteps | 690176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 690500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006566931 |\n",
      "|    clip_fraction        | 0.0124       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.103       |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.113        |\n",
      "|    n_updates            | 3370         |\n",
      "|    policy_gradient_loss | -0.000463    |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=691000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 691000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=691500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 691500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=692000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 692000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 338      |\n",
      "|    time_elapsed    | 26339    |\n",
      "|    total_timesteps | 692224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=692500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 692500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006843979 |\n",
      "|    clip_fraction        | 0.00396      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0407      |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0546       |\n",
      "|    n_updates            | 3380         |\n",
      "|    policy_gradient_loss | -0.000384    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=693000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 693000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=693500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 693500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=694000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 694000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 339      |\n",
      "|    time_elapsed    | 26418    |\n",
      "|    total_timesteps | 694272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=694500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 694500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011576947 |\n",
      "|    clip_fraction        | 0.00625      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0337      |\n",
      "|    explained_variance   | 0.454        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0857       |\n",
      "|    n_updates            | 3390         |\n",
      "|    policy_gradient_loss | -0.000524    |\n",
      "|    value_loss           | 0.117        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=695000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 695000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=695500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 695500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=696000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 696000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 340      |\n",
      "|    time_elapsed    | 26496    |\n",
      "|    total_timesteps | 696320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=696500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 696500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043600582 |\n",
      "|    clip_fraction        | 0.00273       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0298       |\n",
      "|    explained_variance   | 0.451         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0505        |\n",
      "|    n_updates            | 3400          |\n",
      "|    policy_gradient_loss | -0.000227     |\n",
      "|    value_loss           | 0.126         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=697000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 697000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=697500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 697500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=698000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 698000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 341      |\n",
      "|    time_elapsed    | 26575    |\n",
      "|    total_timesteps | 698368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=698500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 698500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024668179 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.11        |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.144        |\n",
      "|    n_updates            | 3410         |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=699000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 699000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=699500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 699500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 342      |\n",
      "|    time_elapsed    | 26654    |\n",
      "|    total_timesteps | 700416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 700500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034190207 |\n",
      "|    clip_fraction        | 0.0293       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.146       |\n",
      "|    explained_variance   | 0.44         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0943       |\n",
      "|    n_updates            | 3420         |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    value_loss           | 0.131        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=701000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 701000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=701500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 701500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=702000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 702000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 343      |\n",
      "|    time_elapsed    | 26733    |\n",
      "|    total_timesteps | 702464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=702500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 702500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002650064 |\n",
      "|    clip_fraction        | 0.0507      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.277      |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0632      |\n",
      "|    n_updates            | 3430        |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=703000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 703000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=703500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 703500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 704000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=704500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 704500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 344      |\n",
      "|    time_elapsed    | 26818    |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=705000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 705000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006733995 |\n",
      "|    clip_fraction        | 0.062       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.177      |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0377      |\n",
      "|    n_updates            | 3440        |\n",
      "|    policy_gradient_loss | -0.00662    |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=705500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 705500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=706000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 706000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=706500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 706500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 345      |\n",
      "|    time_elapsed    | 26897    |\n",
      "|    total_timesteps | 706560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=707000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 707000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001255291 |\n",
      "|    clip_fraction        | 0.0113      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0471     |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.044       |\n",
      "|    n_updates            | 3450        |\n",
      "|    policy_gradient_loss | -0.000479   |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=707500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 707500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=708000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 708000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=708500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 708500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 346      |\n",
      "|    time_elapsed    | 26976    |\n",
      "|    total_timesteps | 708608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=709000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 709000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034136153 |\n",
      "|    clip_fraction        | 0.0455       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.166       |\n",
      "|    explained_variance   | 0.441        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0526       |\n",
      "|    n_updates            | 3460         |\n",
      "|    policy_gradient_loss | -0.00185     |\n",
      "|    value_loss           | 0.129        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=709500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 709500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 347      |\n",
      "|    time_elapsed    | 27054    |\n",
      "|    total_timesteps | 710656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=711000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 711000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004628947 |\n",
      "|    clip_fraction        | 0.0336      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.12       |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0343      |\n",
      "|    n_updates            | 3470        |\n",
      "|    policy_gradient_loss | -0.00113    |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=711500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 711500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=712000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 712000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=712500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 712500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 348      |\n",
      "|    time_elapsed    | 27133    |\n",
      "|    total_timesteps | 712704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=713000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 713000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020568552 |\n",
      "|    clip_fraction        | 0.0269       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.436        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0297       |\n",
      "|    n_updates            | 3480         |\n",
      "|    policy_gradient_loss | -0.000687    |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=713500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 713500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=714000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 714000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=714500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 714500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 349      |\n",
      "|    time_elapsed    | 27212    |\n",
      "|    total_timesteps | 714752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=715000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 715000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009539723 |\n",
      "|    clip_fraction        | 0.0675      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.156      |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0532      |\n",
      "|    n_updates            | 3490        |\n",
      "|    policy_gradient_loss | -0.00905    |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=715500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 715500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=716000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 716000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=716500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 716500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 350      |\n",
      "|    time_elapsed    | 27291    |\n",
      "|    total_timesteps | 716800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=717000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 717000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010545693 |\n",
      "|    clip_fraction        | 0.0729      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.11       |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0855      |\n",
      "|    n_updates            | 3500        |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=717500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 717500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=718000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 718000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=718500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 718500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 351      |\n",
      "|    time_elapsed    | 27370    |\n",
      "|    total_timesteps | 718848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=719000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 719000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056027733 |\n",
      "|    clip_fraction        | 0.0258       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0668      |\n",
      "|    explained_variance   | 0.484        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.043        |\n",
      "|    n_updates            | 3510         |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    value_loss           | 0.118        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=719500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 719500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 352      |\n",
      "|    time_elapsed    | 27449    |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=721000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 721000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017356128 |\n",
      "|    clip_fraction        | 0.0193       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0925      |\n",
      "|    explained_variance   | 0.457        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0696       |\n",
      "|    n_updates            | 3520         |\n",
      "|    policy_gradient_loss | -0.000817    |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=721500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 721500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=722000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 722000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=722500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 722500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 353      |\n",
      "|    time_elapsed    | 27528    |\n",
      "|    total_timesteps | 722944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=723000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 723000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012786447 |\n",
      "|    clip_fraction        | 0.0243       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.102       |\n",
      "|    explained_variance   | 0.456        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0219       |\n",
      "|    n_updates            | 3530         |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    value_loss           | 0.125        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=723500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 723500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=724000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 724000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=724500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 724500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 354      |\n",
      "|    time_elapsed    | 27606    |\n",
      "|    total_timesteps | 724992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 725000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023656231 |\n",
      "|    clip_fraction        | 0.0132       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0553      |\n",
      "|    explained_variance   | 0.428        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0621       |\n",
      "|    n_updates            | 3540         |\n",
      "|    policy_gradient_loss | -0.000924    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=725500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 725500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=726000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 726000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=726500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 726500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=727000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 727000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 355      |\n",
      "|    time_elapsed    | 27691    |\n",
      "|    total_timesteps | 727040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=727500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 727500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016743453 |\n",
      "|    clip_fraction        | 0.0196       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0729      |\n",
      "|    explained_variance   | 0.432        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.047        |\n",
      "|    n_updates            | 3550         |\n",
      "|    policy_gradient_loss | -0.000497    |\n",
      "|    value_loss           | 0.132        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=728000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 728000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=728500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 728500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=729000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 729000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 356      |\n",
      "|    time_elapsed    | 27770    |\n",
      "|    total_timesteps | 729088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=729500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 729500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009362858 |\n",
      "|    clip_fraction        | 0.0601      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.131      |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0336      |\n",
      "|    n_updates            | 3560        |\n",
      "|    policy_gradient_loss | -0.00826    |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 730000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=730500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 730500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=731000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 731000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 357      |\n",
      "|    time_elapsed    | 27849    |\n",
      "|    total_timesteps | 731136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=731500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 731500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005020544 |\n",
      "|    clip_fraction        | 0.00762     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0451     |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0589      |\n",
      "|    n_updates            | 3570        |\n",
      "|    policy_gradient_loss | -0.00118    |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=732000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 732000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=732500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 732500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=733000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 733000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 358      |\n",
      "|    time_elapsed    | 27927    |\n",
      "|    total_timesteps | 733184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=733500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 733500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046753808 |\n",
      "|    clip_fraction        | 0.00381       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0467       |\n",
      "|    explained_variance   | 0.438         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.072         |\n",
      "|    n_updates            | 3580          |\n",
      "|    policy_gradient_loss | -0.000308     |\n",
      "|    value_loss           | 0.124         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=734000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 734000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=734500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 734500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=735000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 735000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 359      |\n",
      "|    time_elapsed    | 28006    |\n",
      "|    total_timesteps | 735232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=735500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 735500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00074642873 |\n",
      "|    clip_fraction        | 0.0063        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0576       |\n",
      "|    explained_variance   | 0.443         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0275        |\n",
      "|    n_updates            | 3590          |\n",
      "|    policy_gradient_loss | -4.97e-05     |\n",
      "|    value_loss           | 0.13          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 736000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=736500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 736500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=737000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 737000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 360      |\n",
      "|    time_elapsed    | 28085    |\n",
      "|    total_timesteps | 737280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=737500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 737500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008140911 |\n",
      "|    clip_fraction        | 0.0416      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.162      |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.052       |\n",
      "|    n_updates            | 3600        |\n",
      "|    policy_gradient_loss | -0.00343    |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=738000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 738000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=738500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 738500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=739000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 739000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 361      |\n",
      "|    time_elapsed    | 28164    |\n",
      "|    total_timesteps | 739328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=739500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 739500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068359235 |\n",
      "|    clip_fraction        | 0.0742       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.156       |\n",
      "|    explained_variance   | 0.46         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0131       |\n",
      "|    n_updates            | 3610         |\n",
      "|    policy_gradient_loss | -0.00486     |\n",
      "|    value_loss           | 0.133        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=741000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 741000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 362      |\n",
      "|    time_elapsed    | 28243    |\n",
      "|    total_timesteps | 741376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=741500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 741500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029896135 |\n",
      "|    clip_fraction        | 0.0235       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0652      |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0636       |\n",
      "|    n_updates            | 3620         |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=742000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 742000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=742500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 742500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=743000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 743000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 363      |\n",
      "|    time_elapsed    | 28322    |\n",
      "|    total_timesteps | 743424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=743500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 743500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012763015 |\n",
      "|    clip_fraction        | 0.00757      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0568      |\n",
      "|    explained_variance   | 0.459        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0523       |\n",
      "|    n_updates            | 3630         |\n",
      "|    policy_gradient_loss | -0.000759    |\n",
      "|    value_loss           | 0.13         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=744000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 744000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=744500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 744500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 745000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 364      |\n",
      "|    time_elapsed    | 28401    |\n",
      "|    total_timesteps | 745472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 100       |\n",
      "|    mean_reward          | -8        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 745500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0083551 |\n",
      "|    clip_fraction        | 0.0303    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0774   |\n",
      "|    explained_variance   | 0.432     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0458    |\n",
      "|    n_updates            | 3640      |\n",
      "|    policy_gradient_loss | -0.0039   |\n",
      "|    value_loss           | 0.123     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=746000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 746000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=746500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 746500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=747000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 747000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=747500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 747500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 365      |\n",
      "|    time_elapsed    | 28485    |\n",
      "|    total_timesteps | 747520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=748000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 748000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016007983 |\n",
      "|    clip_fraction        | 0.0163       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0563      |\n",
      "|    explained_variance   | 0.449        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0599       |\n",
      "|    n_updates            | 3650         |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=748500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 748500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=749000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 749000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=749500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 749500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 366      |\n",
      "|    time_elapsed    | 28564    |\n",
      "|    total_timesteps | 749568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 750000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00056736154 |\n",
      "|    clip_fraction        | 0.00293       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0439       |\n",
      "|    explained_variance   | 0.449         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0995        |\n",
      "|    n_updates            | 3660          |\n",
      "|    policy_gradient_loss | 3.77e-05      |\n",
      "|    value_loss           | 0.122         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=750500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 750500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=751000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 751000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=751500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 751500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 367      |\n",
      "|    time_elapsed    | 28643    |\n",
      "|    total_timesteps | 751616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 752000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000583365 |\n",
      "|    clip_fraction        | 0.0061      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0647     |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0663      |\n",
      "|    n_updates            | 3670        |\n",
      "|    policy_gradient_loss | -7.49e-05   |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=752500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 752500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=753000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 753000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=753500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 753500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 368      |\n",
      "|    time_elapsed    | 28722    |\n",
      "|    total_timesteps | 753664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=754000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 754000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019392008 |\n",
      "|    clip_fraction        | 0.0103       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0651      |\n",
      "|    explained_variance   | 0.453        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0526       |\n",
      "|    n_updates            | 3680         |\n",
      "|    policy_gradient_loss | -0.000196    |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=754500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 754500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=755000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 755000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=755500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 755500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 369      |\n",
      "|    time_elapsed    | 28801    |\n",
      "|    total_timesteps | 755712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=756000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 756000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011852148 |\n",
      "|    clip_fraction        | 0.00527      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0463      |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0371       |\n",
      "|    n_updates            | 3690         |\n",
      "|    policy_gradient_loss | -0.000191    |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=756500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 756500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=757000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 757000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=757500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 757500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 370      |\n",
      "|    time_elapsed    | 28879    |\n",
      "|    total_timesteps | 757760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=758000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 758000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00082218193 |\n",
      "|    clip_fraction        | 0.00459       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0411       |\n",
      "|    explained_variance   | 0.442         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0255        |\n",
      "|    n_updates            | 3700          |\n",
      "|    policy_gradient_loss | -0.000118     |\n",
      "|    value_loss           | 0.125         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=758500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 758500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=759000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 759000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=759500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 759500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 371      |\n",
      "|    time_elapsed    | 28958    |\n",
      "|    total_timesteps | 759808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 760000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029961308 |\n",
      "|    clip_fraction        | 0.00962      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0425      |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0326       |\n",
      "|    n_updates            | 3710         |\n",
      "|    policy_gradient_loss | -0.000535    |\n",
      "|    value_loss           | 0.134        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=760500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 760500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=761000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 761000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=761500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 761500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 372      |\n",
      "|    time_elapsed    | 29037    |\n",
      "|    total_timesteps | 761856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=762000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 762000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019191143 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0451      |\n",
      "|    explained_variance   | 0.46         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0668       |\n",
      "|    n_updates            | 3720         |\n",
      "|    policy_gradient_loss | -0.000891    |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=762500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 762500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=763000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 763000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=763500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 763500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 373      |\n",
      "|    time_elapsed    | 29116    |\n",
      "|    total_timesteps | 763904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=764000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 764000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029236656 |\n",
      "|    clip_fraction        | 0.00986      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0475      |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0642       |\n",
      "|    n_updates            | 3730         |\n",
      "|    policy_gradient_loss | -0.00042     |\n",
      "|    value_loss           | 0.131        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=764500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 764500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=765000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 765000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=765500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 765500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 374      |\n",
      "|    time_elapsed    | 29195    |\n",
      "|    total_timesteps | 765952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=766000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 766000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011772185 |\n",
      "|    clip_fraction        | 0.00449      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.035       |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0725       |\n",
      "|    n_updates            | 3740         |\n",
      "|    policy_gradient_loss | -0.000234    |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=766500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 766500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=767000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 767000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=767500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 767500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 768000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 375      |\n",
      "|    time_elapsed    | 29280    |\n",
      "|    total_timesteps | 768000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=768500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 768500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014071336 |\n",
      "|    clip_fraction        | 0.00884      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.039       |\n",
      "|    explained_variance   | 0.424        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0842       |\n",
      "|    n_updates            | 3750         |\n",
      "|    policy_gradient_loss | -0.000477    |\n",
      "|    value_loss           | 0.135        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=769000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 769000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=769500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 769500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 770000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 376      |\n",
      "|    time_elapsed    | 29359    |\n",
      "|    total_timesteps | 770048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 770500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052854912 |\n",
      "|    clip_fraction        | 0.0762       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.164       |\n",
      "|    explained_variance   | 0.467        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0343       |\n",
      "|    n_updates            | 3760         |\n",
      "|    policy_gradient_loss | -0.00365     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=771000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 771000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=771500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 771500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=772000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 772000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 377      |\n",
      "|    time_elapsed    | 29437    |\n",
      "|    total_timesteps | 772096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=772500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 772500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015250734 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.271      |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0305      |\n",
      "|    n_updates            | 3770        |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=773000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 773000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=773500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 773500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=774000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 774000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.11    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 378      |\n",
      "|    time_elapsed    | 29517    |\n",
      "|    total_timesteps | 774144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=774500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 774500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014667075 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.324      |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0284      |\n",
      "|    n_updates            | 3780        |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=775000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 775000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=775500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 775500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=776000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 776000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.14    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 379      |\n",
      "|    time_elapsed    | 29596    |\n",
      "|    total_timesteps | 776192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=776500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 776500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010637874 |\n",
      "|    clip_fraction        | 0.0705      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.162      |\n",
      "|    explained_variance   | 0.383       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0843      |\n",
      "|    n_updates            | 3790        |\n",
      "|    policy_gradient_loss | -0.0096     |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=777000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 777000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=777500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 777500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=778000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 778000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 380      |\n",
      "|    time_elapsed    | 29675    |\n",
      "|    total_timesteps | 778240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=778500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 778500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004602324 |\n",
      "|    clip_fraction        | 0.0186      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0567     |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0789      |\n",
      "|    n_updates            | 3800        |\n",
      "|    policy_gradient_loss | -0.00232    |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=779000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 779000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=779500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 779500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 780000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 381      |\n",
      "|    time_elapsed    | 29753    |\n",
      "|    total_timesteps | 780288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 780500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010003373 |\n",
      "|    clip_fraction        | 0.00532      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0377      |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0943       |\n",
      "|    n_updates            | 3810         |\n",
      "|    policy_gradient_loss | -0.000253    |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=781000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 781000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=781500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 781500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=782000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 782000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.12    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 382      |\n",
      "|    time_elapsed    | 29832    |\n",
      "|    total_timesteps | 782336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=782500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 782500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004366095 |\n",
      "|    clip_fraction        | 0.004        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0363      |\n",
      "|    explained_variance   | 0.449        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0504       |\n",
      "|    n_updates            | 3820         |\n",
      "|    policy_gradient_loss | -0.000107    |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=783000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 783000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=783500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 783500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 784000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 383      |\n",
      "|    time_elapsed    | 29911    |\n",
      "|    total_timesteps | 784384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=784500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 784500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010923921 |\n",
      "|    clip_fraction        | 0.00391      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0377      |\n",
      "|    explained_variance   | 0.441        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0658       |\n",
      "|    n_updates            | 3830         |\n",
      "|    policy_gradient_loss | -0.000127    |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=785000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 785000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=785500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 785500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 786000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 384      |\n",
      "|    time_elapsed    | 29990    |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 786500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032135638 |\n",
      "|    clip_fraction        | 0.00483       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0451       |\n",
      "|    explained_variance   | 0.45          |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0483        |\n",
      "|    n_updates            | 3840          |\n",
      "|    policy_gradient_loss | -7.26e-05     |\n",
      "|    value_loss           | 0.128         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=787000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 787000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=787500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 787500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=788000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 788000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 385      |\n",
      "|    time_elapsed    | 30069    |\n",
      "|    total_timesteps | 788480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=788500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 788500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003769058 |\n",
      "|    clip_fraction        | 0.00347      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0429      |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0553       |\n",
      "|    n_updates            | 3850         |\n",
      "|    policy_gradient_loss | -0.000187    |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=789000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 789000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=789500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 789500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 386      |\n",
      "|    time_elapsed    | 30154    |\n",
      "|    total_timesteps | 790528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=791000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 791000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010991159 |\n",
      "|    clip_fraction        | 0.0207       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0987      |\n",
      "|    explained_variance   | 0.451        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0293       |\n",
      "|    n_updates            | 3860         |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 0.125        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=791500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 791500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=792000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 792000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=792500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 792500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 387      |\n",
      "|    time_elapsed    | 30233    |\n",
      "|    total_timesteps | 792576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=793000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 793000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085571595 |\n",
      "|    clip_fraction        | 0.00586       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0522       |\n",
      "|    explained_variance   | 0.446         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0553        |\n",
      "|    n_updates            | 3870          |\n",
      "|    policy_gradient_loss | -0.000373     |\n",
      "|    value_loss           | 0.121         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=793500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 793500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=794000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 794000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=794500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 794500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 388      |\n",
      "|    time_elapsed    | 30311    |\n",
      "|    total_timesteps | 794624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=795000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 795000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006193303 |\n",
      "|    clip_fraction        | 0.00747      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0451      |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0625       |\n",
      "|    n_updates            | 3880         |\n",
      "|    policy_gradient_loss | -0.00033     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=795500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 795500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=796000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 796000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=796500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 796500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 389      |\n",
      "|    time_elapsed    | 30390    |\n",
      "|    total_timesteps | 796672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=797000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 797000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00082464854 |\n",
      "|    clip_fraction        | 0.00439       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0456       |\n",
      "|    explained_variance   | 0.454         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.04          |\n",
      "|    n_updates            | 3890          |\n",
      "|    policy_gradient_loss | -0.000489     |\n",
      "|    value_loss           | 0.123         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=797500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 797500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=798000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 798000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=798500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 798500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 390      |\n",
      "|    time_elapsed    | 30468    |\n",
      "|    total_timesteps | 798720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=799000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 799000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020429012 |\n",
      "|    clip_fraction        | 0.021        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0882      |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0454       |\n",
      "|    n_updates            | 3900         |\n",
      "|    policy_gradient_loss | -0.000829    |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=799500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 799500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 391      |\n",
      "|    time_elapsed    | 30546    |\n",
      "|    total_timesteps | 800768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=801000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 801000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023428125 |\n",
      "|    clip_fraction        | 0.00679      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0412      |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0734       |\n",
      "|    n_updates            | 3910         |\n",
      "|    policy_gradient_loss | -0.000573    |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=801500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 801500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=802000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 802000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=802500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 802500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 392      |\n",
      "|    time_elapsed    | 30625    |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=803000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 803000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022480913 |\n",
      "|    clip_fraction        | 0.00254       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0419       |\n",
      "|    explained_variance   | 0.45          |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0543        |\n",
      "|    n_updates            | 3920          |\n",
      "|    policy_gradient_loss | -6.88e-06     |\n",
      "|    value_loss           | 0.125         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=803500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 803500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=804000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 804000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=804500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 804500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 393      |\n",
      "|    time_elapsed    | 30703    |\n",
      "|    total_timesteps | 804864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=805000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 805000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017003642 |\n",
      "|    clip_fraction        | 0.00322       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0354       |\n",
      "|    explained_variance   | 0.449         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0744        |\n",
      "|    n_updates            | 3930          |\n",
      "|    policy_gradient_loss | -0.000263     |\n",
      "|    value_loss           | 0.122         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=805500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 805500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 806000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 806500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 394      |\n",
      "|    time_elapsed    | 30782    |\n",
      "|    total_timesteps | 806912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=807000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 807000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00080664933 |\n",
      "|    clip_fraction        | 0.00352       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0436       |\n",
      "|    explained_variance   | 0.452         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0862        |\n",
      "|    n_updates            | 3940          |\n",
      "|    policy_gradient_loss | -0.000408     |\n",
      "|    value_loss           | 0.125         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=807500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 807500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=808000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 808000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=808500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 808500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 395      |\n",
      "|    time_elapsed    | 30860    |\n",
      "|    total_timesteps | 808960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=809000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 809000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004380869 |\n",
      "|    clip_fraction        | 0.00254      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0501      |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0815       |\n",
      "|    n_updates            | 3950         |\n",
      "|    policy_gradient_loss | -0.000223    |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=809500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 809500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=811000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 811000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 396      |\n",
      "|    time_elapsed    | 30944    |\n",
      "|    total_timesteps | 811008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=811500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 811500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011857382 |\n",
      "|    clip_fraction        | 0.00732      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.058       |\n",
      "|    explained_variance   | 0.431        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0518       |\n",
      "|    n_updates            | 3960         |\n",
      "|    policy_gradient_loss | -0.000676    |\n",
      "|    value_loss           | 0.13         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=812000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 812000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=812500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 812500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=813000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 813000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 397      |\n",
      "|    time_elapsed    | 31023    |\n",
      "|    total_timesteps | 813056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=813500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 813500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005037649 |\n",
      "|    clip_fraction        | 0.0208      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0826     |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0265      |\n",
      "|    n_updates            | 3970        |\n",
      "|    policy_gradient_loss | -0.00106    |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=814000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 814000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=814500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 814500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=815000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 815000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 398      |\n",
      "|    time_elapsed    | 31101    |\n",
      "|    total_timesteps | 815104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=815500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 815500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046316017 |\n",
      "|    clip_fraction        | 0.0251       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0759      |\n",
      "|    explained_variance   | 0.474        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0691       |\n",
      "|    n_updates            | 3980         |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=816000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 816000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=816500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 816500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=817000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 817000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 399      |\n",
      "|    time_elapsed    | 31179    |\n",
      "|    total_timesteps | 817152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=817500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 817500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034305782 |\n",
      "|    clip_fraction        | 0.0268       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0831      |\n",
      "|    explained_variance   | 0.468        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0522       |\n",
      "|    n_updates            | 3990         |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    value_loss           | 0.119        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=818000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 818000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=818500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 818500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=819000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 819000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 400      |\n",
      "|    time_elapsed    | 31258    |\n",
      "|    total_timesteps | 819200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=819500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 819500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019697065 |\n",
      "|    clip_fraction        | 0.00581      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0653      |\n",
      "|    explained_variance   | 0.439        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0619       |\n",
      "|    n_updates            | 4000         |\n",
      "|    policy_gradient_loss | -0.00053     |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=820500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=821000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 821000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 401      |\n",
      "|    time_elapsed    | 31337    |\n",
      "|    total_timesteps | 821248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=821500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 821500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00072992704 |\n",
      "|    clip_fraction        | 0.00884       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0708       |\n",
      "|    explained_variance   | 0.456         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0499        |\n",
      "|    n_updates            | 4010          |\n",
      "|    policy_gradient_loss | -6.69e-05     |\n",
      "|    value_loss           | 0.124         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=822000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 822000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=822500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 822500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=823000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 823000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 402      |\n",
      "|    time_elapsed    | 31415    |\n",
      "|    total_timesteps | 823296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=823500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 823500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015789573 |\n",
      "|    clip_fraction        | 0.00522      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0695      |\n",
      "|    explained_variance   | 0.449        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0749       |\n",
      "|    n_updates            | 4020         |\n",
      "|    policy_gradient_loss | 0.000254     |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=824000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 824000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=824500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 824500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=825000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 825000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 403      |\n",
      "|    time_elapsed    | 31494    |\n",
      "|    total_timesteps | 825344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=825500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 825500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014942721 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0818      |\n",
      "|    explained_variance   | 0.453        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0781       |\n",
      "|    n_updates            | 4030         |\n",
      "|    policy_gradient_loss | -0.000744    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=826000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 826000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 826500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=827000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 827000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 404      |\n",
      "|    time_elapsed    | 31580    |\n",
      "|    total_timesteps | 827392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=827500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 827500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0121028535 |\n",
      "|    clip_fraction        | 0.0366       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.144       |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0783       |\n",
      "|    n_updates            | 4040         |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    value_loss           | 0.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=828000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 828000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=828500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 828500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=829000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 829000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 405      |\n",
      "|    time_elapsed    | 31659    |\n",
      "|    total_timesteps | 829440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=829500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 829500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008858483 |\n",
      "|    clip_fraction        | 0.00854      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0698      |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0526       |\n",
      "|    n_updates            | 4050         |\n",
      "|    policy_gradient_loss | -0.000506    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 830000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 830500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=831000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 831000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 406      |\n",
      "|    time_elapsed    | 31738    |\n",
      "|    total_timesteps | 831488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=831500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 831500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002525302 |\n",
      "|    clip_fraction        | 0.00386      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0649      |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0491       |\n",
      "|    n_updates            | 4060         |\n",
      "|    policy_gradient_loss | -0.000219    |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=832000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 832000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=832500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 832500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=833000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 833000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=833500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 833500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 407      |\n",
      "|    time_elapsed    | 31822    |\n",
      "|    total_timesteps | 833536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=834000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 834000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017447656 |\n",
      "|    clip_fraction        | 0.0223      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0768     |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0543      |\n",
      "|    n_updates            | 4070        |\n",
      "|    policy_gradient_loss | -0.0024     |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=834500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 834500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=835000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 835000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=835500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 835500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 408      |\n",
      "|    time_elapsed    | 31900    |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=836000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 836000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002464648 |\n",
      "|    clip_fraction        | 0.00415     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0704     |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0435      |\n",
      "|    n_updates            | 4080        |\n",
      "|    policy_gradient_loss | -0.000791   |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=836500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 836500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=837000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 837000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=837500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 837500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 409      |\n",
      "|    time_elapsed    | 31979    |\n",
      "|    total_timesteps | 837632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=838000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 838000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012140095 |\n",
      "|    clip_fraction        | 0.00698      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.104       |\n",
      "|    explained_variance   | 0.451        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0316       |\n",
      "|    n_updates            | 4090         |\n",
      "|    policy_gradient_loss | -0.000418    |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=838500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 838500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=839000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 839000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=839500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 839500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 410      |\n",
      "|    time_elapsed    | 32057    |\n",
      "|    total_timesteps | 839680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 840000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007782414 |\n",
      "|    clip_fraction        | 0.00786      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.101       |\n",
      "|    explained_variance   | 0.451        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.107        |\n",
      "|    n_updates            | 4100         |\n",
      "|    policy_gradient_loss | -0.000405    |\n",
      "|    value_loss           | 0.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=840500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 840500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=841000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 841000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=841500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 841500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 411      |\n",
      "|    time_elapsed    | 32136    |\n",
      "|    total_timesteps | 841728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=842000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 842000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00057217246 |\n",
      "|    clip_fraction        | 0.00845       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.093        |\n",
      "|    explained_variance   | 0.447         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0406        |\n",
      "|    n_updates            | 4110          |\n",
      "|    policy_gradient_loss | -0.000485     |\n",
      "|    value_loss           | 0.128         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=842500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 842500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=843000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 843000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=843500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 843500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 412      |\n",
      "|    time_elapsed    | 32214    |\n",
      "|    total_timesteps | 843776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=844000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 844000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020331414 |\n",
      "|    clip_fraction        | 0.012        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.113       |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0442       |\n",
      "|    n_updates            | 4120         |\n",
      "|    policy_gradient_loss | -0.00044     |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=844500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 844500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=845000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 845000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=845500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 845500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 413      |\n",
      "|    time_elapsed    | 32293    |\n",
      "|    total_timesteps | 845824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=846000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 846000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00074416667 |\n",
      "|    clip_fraction        | 0.00498       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.106        |\n",
      "|    explained_variance   | 0.449         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0637        |\n",
      "|    n_updates            | 4130          |\n",
      "|    policy_gradient_loss | -1.31e-05     |\n",
      "|    value_loss           | 0.126         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=846500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 846500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=847000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 847000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=847500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 847500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 414      |\n",
      "|    time_elapsed    | 32371    |\n",
      "|    total_timesteps | 847872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=848000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 848000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036372016 |\n",
      "|    clip_fraction        | 0.0326       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.131       |\n",
      "|    explained_variance   | 0.445        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0221       |\n",
      "|    n_updates            | 4140         |\n",
      "|    policy_gradient_loss | -0.00488     |\n",
      "|    value_loss           | 0.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=848500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 848500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=849000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 849000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=849500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 849500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 415      |\n",
      "|    time_elapsed    | 32450    |\n",
      "|    total_timesteps | 849920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 850000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019351635 |\n",
      "|    clip_fraction        | 0.0105       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.103       |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0679       |\n",
      "|    n_updates            | 4150         |\n",
      "|    policy_gradient_loss | -0.000762    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=850500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 850500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=851000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 851000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=851500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 851500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 416      |\n",
      "|    time_elapsed    | 32528    |\n",
      "|    total_timesteps | 851968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=852000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 852000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012084533 |\n",
      "|    clip_fraction        | 0.00879      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.105       |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0542       |\n",
      "|    n_updates            | 4160         |\n",
      "|    policy_gradient_loss | -0.000436    |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=852500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 852500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=853000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 853000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=853500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 853500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=854000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 854000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 417      |\n",
      "|    time_elapsed    | 32613    |\n",
      "|    total_timesteps | 854016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=854500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 854500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033961865 |\n",
      "|    clip_fraction        | 0.027        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.123       |\n",
      "|    explained_variance   | 0.418        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0163       |\n",
      "|    n_updates            | 4170         |\n",
      "|    policy_gradient_loss | -0.00289     |\n",
      "|    value_loss           | 0.131        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=855000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 855000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=855500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 855500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=856000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 856000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 418      |\n",
      "|    time_elapsed    | 32691    |\n",
      "|    total_timesteps | 856064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=856500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 856500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034345335 |\n",
      "|    clip_fraction        | 0.0179       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.088       |\n",
      "|    explained_variance   | 0.444        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0264       |\n",
      "|    n_updates            | 4180         |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=857000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 857000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=857500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 857500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=858000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 858000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 419      |\n",
      "|    time_elapsed    | 32770    |\n",
      "|    total_timesteps | 858112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=858500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 858500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00325868 |\n",
      "|    clip_fraction        | 0.024      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.113     |\n",
      "|    explained_variance   | 0.438      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0657     |\n",
      "|    n_updates            | 4190       |\n",
      "|    policy_gradient_loss | -0.00224   |\n",
      "|    value_loss           | 0.13       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=859000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 859000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=859500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 859500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 860000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 420      |\n",
      "|    time_elapsed    | 32848    |\n",
      "|    total_timesteps | 860160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 860500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011824319 |\n",
      "|    clip_fraction        | 0.0153      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0869     |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 4200        |\n",
      "|    policy_gradient_loss | -0.00188    |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=861000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 861000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=861500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 861500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=862000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 862000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 421      |\n",
      "|    time_elapsed    | 32927    |\n",
      "|    total_timesteps | 862208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=862500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 862500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043281863 |\n",
      "|    clip_fraction        | 0.00669       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.097        |\n",
      "|    explained_variance   | 0.447         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0644        |\n",
      "|    n_updates            | 4210          |\n",
      "|    policy_gradient_loss | -0.000288     |\n",
      "|    value_loss           | 0.125         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=863000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 863000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=863500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 863500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=864000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 864000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 422      |\n",
      "|    time_elapsed    | 33006    |\n",
      "|    total_timesteps | 864256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=864500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 864500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030948764 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.116       |\n",
      "|    explained_variance   | 0.433        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0356       |\n",
      "|    n_updates            | 4220         |\n",
      "|    policy_gradient_loss | -0.00184     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=865000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 865000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=865500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 865500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 866000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 423      |\n",
      "|    time_elapsed    | 33084    |\n",
      "|    total_timesteps | 866304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 866500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002511328 |\n",
      "|    clip_fraction        | 0.0216      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0997     |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0528      |\n",
      "|    n_updates            | 4230        |\n",
      "|    policy_gradient_loss | -0.00263    |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=867000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 867000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=867500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 867500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=868000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 868000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 424      |\n",
      "|    time_elapsed    | 33163    |\n",
      "|    total_timesteps | 868352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=868500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 868500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031025303 |\n",
      "|    clip_fraction        | 0.0115       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0833      |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0669       |\n",
      "|    n_updates            | 4240         |\n",
      "|    policy_gradient_loss | -0.000996    |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=869000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 869000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=869500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 869500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 870000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 425      |\n",
      "|    time_elapsed    | 33241    |\n",
      "|    total_timesteps | 870400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 870500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038728112 |\n",
      "|    clip_fraction        | 0.00498       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0805       |\n",
      "|    explained_variance   | 0.434         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0625        |\n",
      "|    n_updates            | 4250          |\n",
      "|    policy_gradient_loss | -0.000113     |\n",
      "|    value_loss           | 0.128         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=871000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 871000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=871500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 871500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=872000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 872000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 426      |\n",
      "|    time_elapsed    | 33320    |\n",
      "|    total_timesteps | 872448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=872500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 872500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004645804 |\n",
      "|    clip_fraction        | 0.00977      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0793      |\n",
      "|    explained_variance   | 0.464        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0582       |\n",
      "|    n_updates            | 4260         |\n",
      "|    policy_gradient_loss | -0.000878    |\n",
      "|    value_loss           | 0.118        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=873000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 873000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=873500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 873500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=874000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 874000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 427      |\n",
      "|    time_elapsed    | 33398    |\n",
      "|    total_timesteps | 874496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=874500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 874500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040459714 |\n",
      "|    clip_fraction        | 0.00352       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0843       |\n",
      "|    explained_variance   | 0.451         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0759        |\n",
      "|    n_updates            | 4270          |\n",
      "|    policy_gradient_loss | -7.18e-05     |\n",
      "|    value_loss           | 0.121         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=875000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 875000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=875500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 875500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=876000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 876000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=876500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 876500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 428      |\n",
      "|    time_elapsed    | 33483    |\n",
      "|    total_timesteps | 876544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=877000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 877000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012575282 |\n",
      "|    clip_fraction        | 0.00923      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0829      |\n",
      "|    explained_variance   | 0.452        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0696       |\n",
      "|    n_updates            | 4280         |\n",
      "|    policy_gradient_loss | -0.000699    |\n",
      "|    value_loss           | 0.125        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=877500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 877500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=878000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 878000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=878500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 878500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 429      |\n",
      "|    time_elapsed    | 33562    |\n",
      "|    total_timesteps | 878592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=879000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 879000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019509677 |\n",
      "|    clip_fraction        | 0.002         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0675       |\n",
      "|    explained_variance   | 0.446         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0746        |\n",
      "|    n_updates            | 4290          |\n",
      "|    policy_gradient_loss | -0.000164     |\n",
      "|    value_loss           | 0.123         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=879500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 879500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 430      |\n",
      "|    time_elapsed    | 33640    |\n",
      "|    total_timesteps | 880640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=881000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 881000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037882046 |\n",
      "|    clip_fraction        | 0.0061        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0754       |\n",
      "|    explained_variance   | 0.448         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.066         |\n",
      "|    n_updates            | 4300          |\n",
      "|    policy_gradient_loss | -0.000233     |\n",
      "|    value_loss           | 0.127         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=881500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 881500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=882000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 882000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=882500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 882500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 431      |\n",
      "|    time_elapsed    | 33719    |\n",
      "|    total_timesteps | 882688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=883000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 883000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007844918 |\n",
      "|    clip_fraction        | 0.00317      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0634      |\n",
      "|    explained_variance   | 0.442        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0317       |\n",
      "|    n_updates            | 4310         |\n",
      "|    policy_gradient_loss | 3.1e-05      |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=883500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 883500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=884000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 884000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=884500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 884500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 432      |\n",
      "|    time_elapsed    | 33797    |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=885000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 885000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015631288 |\n",
      "|    clip_fraction        | 0.00791      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0651      |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.088        |\n",
      "|    n_updates            | 4320         |\n",
      "|    policy_gradient_loss | -0.000492    |\n",
      "|    value_loss           | 0.129        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=885500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 885500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=886000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 886000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=886500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 886500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 433      |\n",
      "|    time_elapsed    | 33876    |\n",
      "|    total_timesteps | 886784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=887000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 887000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021304262 |\n",
      "|    clip_fraction        | 0.0126       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0637      |\n",
      "|    explained_variance   | 0.457        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0452       |\n",
      "|    n_updates            | 4330         |\n",
      "|    policy_gradient_loss | -0.000627    |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=887500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 887500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=888000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 888000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=888500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 888500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 434      |\n",
      "|    time_elapsed    | 33954    |\n",
      "|    total_timesteps | 888832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=889000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 889000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014753673 |\n",
      "|    clip_fraction        | 0.00625      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0541      |\n",
      "|    explained_variance   | 0.451        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0534       |\n",
      "|    n_updates            | 4340         |\n",
      "|    policy_gradient_loss | -0.000487    |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=889500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 889500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=890500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 435      |\n",
      "|    time_elapsed    | 34032    |\n",
      "|    total_timesteps | 890880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=891000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 891000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015830264 |\n",
      "|    clip_fraction        | 0.00474      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0572      |\n",
      "|    explained_variance   | 0.449        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0412       |\n",
      "|    n_updates            | 4350         |\n",
      "|    policy_gradient_loss | -0.000137    |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=891500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 891500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=892000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 892000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=892500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 892500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 436      |\n",
      "|    time_elapsed    | 34111    |\n",
      "|    total_timesteps | 892928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=893000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 893000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015630146 |\n",
      "|    clip_fraction        | 0.0064       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0544      |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0573       |\n",
      "|    n_updates            | 4360         |\n",
      "|    policy_gradient_loss | -0.000453    |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=893500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 893500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=894000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 894000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=894500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 894500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 437      |\n",
      "|    time_elapsed    | 34189    |\n",
      "|    total_timesteps | 894976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=895000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 895000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00096098694 |\n",
      "|    clip_fraction        | 0.00679       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.052        |\n",
      "|    explained_variance   | 0.447         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0219        |\n",
      "|    n_updates            | 4370          |\n",
      "|    policy_gradient_loss | -0.000513     |\n",
      "|    value_loss           | 0.122         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=895500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 895500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=896000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 896000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=896500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 896500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=897000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 897000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 438      |\n",
      "|    time_elapsed    | 34274    |\n",
      "|    total_timesteps | 897024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=897500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 897500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009807963 |\n",
      "|    clip_fraction        | 0.0113       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0655      |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0633       |\n",
      "|    n_updates            | 4380         |\n",
      "|    policy_gradient_loss | 9.41e-05     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=898000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 898000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=898500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 898500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=899000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 899000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 439      |\n",
      "|    time_elapsed    | 34352    |\n",
      "|    total_timesteps | 899072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=899500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 899500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008253924 |\n",
      "|    clip_fraction        | 0.0564      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.101      |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0976      |\n",
      "|    n_updates            | 4390        |\n",
      "|    policy_gradient_loss | -0.00679    |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=901000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 901000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 440      |\n",
      "|    time_elapsed    | 34431    |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=901500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 901500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007824878 |\n",
      "|    clip_fraction        | 0.0456      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.108      |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0849      |\n",
      "|    n_updates            | 4400        |\n",
      "|    policy_gradient_loss | -0.00588    |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=902000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 902000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=902500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 902500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=903000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 903000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 441      |\n",
      "|    time_elapsed    | 34509    |\n",
      "|    total_timesteps | 903168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=903500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 903500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003950719 |\n",
      "|    clip_fraction        | 0.0188      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0834     |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0245      |\n",
      "|    n_updates            | 4410        |\n",
      "|    policy_gradient_loss | -0.000822   |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=904000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 904000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=904500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 904500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=905000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 905000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 442      |\n",
      "|    time_elapsed    | 34588    |\n",
      "|    total_timesteps | 905216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=905500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 905500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007928065 |\n",
      "|    clip_fraction        | 0.0064       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0672      |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0681       |\n",
      "|    n_updates            | 4420         |\n",
      "|    policy_gradient_loss | -0.000214    |\n",
      "|    value_loss           | 0.13         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=906000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 906000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=906500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 906500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 907000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 443      |\n",
      "|    time_elapsed    | 34666    |\n",
      "|    total_timesteps | 907264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 907500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037181505 |\n",
      "|    clip_fraction        | 0.00527       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.05         |\n",
      "|    explained_variance   | 0.446         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0996        |\n",
      "|    n_updates            | 4430          |\n",
      "|    policy_gradient_loss | -0.00043      |\n",
      "|    value_loss           | 0.123         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=908000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 908000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=908500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 908500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=909000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 909000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 444      |\n",
      "|    time_elapsed    | 34744    |\n",
      "|    total_timesteps | 909312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=909500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 909500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010473545 |\n",
      "|    clip_fraction        | 0.00557      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0564      |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0606       |\n",
      "|    n_updates            | 4440         |\n",
      "|    policy_gradient_loss | -0.00046     |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 910000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 910500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=911000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 911000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 445      |\n",
      "|    time_elapsed    | 34823    |\n",
      "|    total_timesteps | 911360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=911500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 911500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001142993 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0437      |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0816       |\n",
      "|    n_updates            | 4450         |\n",
      "|    policy_gradient_loss | -0.000234    |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=912000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 912000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=912500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 912500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=913000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 913000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 446      |\n",
      "|    time_elapsed    | 34902    |\n",
      "|    total_timesteps | 913408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=913500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 913500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013744802 |\n",
      "|    clip_fraction        | 0.00215       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0421       |\n",
      "|    explained_variance   | 0.445         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.068         |\n",
      "|    n_updates            | 4460          |\n",
      "|    policy_gradient_loss | -5.73e-05     |\n",
      "|    value_loss           | 0.128         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=914000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 914000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=914500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 914500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=915000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 915000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 447      |\n",
      "|    time_elapsed    | 34980    |\n",
      "|    total_timesteps | 915456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=915500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 915500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00042538866 |\n",
      "|    clip_fraction        | 0.00239       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0405       |\n",
      "|    explained_variance   | 0.451         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0585        |\n",
      "|    n_updates            | 4470          |\n",
      "|    policy_gradient_loss | -0.000238     |\n",
      "|    value_loss           | 0.122         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=916000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 916000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=916500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 916500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=917000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 917000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=917500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 917500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 448      |\n",
      "|    time_elapsed    | 35065    |\n",
      "|    total_timesteps | 917504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=918000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 918000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007475037 |\n",
      "|    clip_fraction        | 0.00366      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0419      |\n",
      "|    explained_variance   | 0.449        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.063        |\n",
      "|    n_updates            | 4480         |\n",
      "|    policy_gradient_loss | -0.000326    |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=918500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 918500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=919000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 919000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=919500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 919500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 449      |\n",
      "|    time_elapsed    | 35143    |\n",
      "|    total_timesteps | 919552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 920000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085316994 |\n",
      "|    clip_fraction        | 0.00317       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0468       |\n",
      "|    explained_variance   | 0.448         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.08          |\n",
      "|    n_updates            | 4490          |\n",
      "|    policy_gradient_loss | -0.000367     |\n",
      "|    value_loss           | 0.123         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=920500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 920500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=921000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 921000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=921500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 921500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 450      |\n",
      "|    time_elapsed    | 35222    |\n",
      "|    total_timesteps | 921600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=922000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 922000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00050113193 |\n",
      "|    clip_fraction        | 0.00342       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.047        |\n",
      "|    explained_variance   | 0.435         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0782        |\n",
      "|    n_updates            | 4500          |\n",
      "|    policy_gradient_loss | -0.000282     |\n",
      "|    value_loss           | 0.128         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=922500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 922500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=923000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 923000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=923500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 923500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 451      |\n",
      "|    time_elapsed    | 35301    |\n",
      "|    total_timesteps | 923648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=924000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 924000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00064028543 |\n",
      "|    clip_fraction        | 0.00728       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0606       |\n",
      "|    explained_variance   | 0.463         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0869        |\n",
      "|    n_updates            | 4510          |\n",
      "|    policy_gradient_loss | -6.03e-05     |\n",
      "|    value_loss           | 0.124         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=924500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 924500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=925000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 925000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=925500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 925500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 452      |\n",
      "|    time_elapsed    | 35379    |\n",
      "|    total_timesteps | 925696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=926000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 926000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007592992 |\n",
      "|    clip_fraction        | 0.055       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.187      |\n",
      "|    explained_variance   | 0.407       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0697      |\n",
      "|    n_updates            | 4520        |\n",
      "|    policy_gradient_loss | -0.00523    |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=926500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 926500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=927000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 927000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=927500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 927500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 453      |\n",
      "|    time_elapsed    | 35458    |\n",
      "|    total_timesteps | 927744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=928000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 928000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009147788 |\n",
      "|    clip_fraction        | 0.0387      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.138      |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0472      |\n",
      "|    n_updates            | 4530        |\n",
      "|    policy_gradient_loss | -0.00543    |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=928500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 928500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=929000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 929000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=929500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 929500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 454      |\n",
      "|    time_elapsed    | 35536    |\n",
      "|    total_timesteps | 929792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 930000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045483103 |\n",
      "|    clip_fraction        | 0.029        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0933      |\n",
      "|    explained_variance   | 0.466        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0537       |\n",
      "|    n_updates            | 4540         |\n",
      "|    policy_gradient_loss | -0.00243     |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=930500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 930500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=931000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 931000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=931500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 931500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 455      |\n",
      "|    time_elapsed    | 35615    |\n",
      "|    total_timesteps | 931840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=932000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 932000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00204532 |\n",
      "|    clip_fraction        | 0.00703    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.066     |\n",
      "|    explained_variance   | 0.454      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0302     |\n",
      "|    n_updates            | 4550       |\n",
      "|    policy_gradient_loss | -0.00109   |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=932500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 932500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=933000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 933000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=933500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 933500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 456      |\n",
      "|    time_elapsed    | 35693    |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=934000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 934000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003676121 |\n",
      "|    clip_fraction        | 0.00625      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0639      |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.122        |\n",
      "|    n_updates            | 4560         |\n",
      "|    policy_gradient_loss | -0.000361    |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=934500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 934500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=935000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 935000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=935500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 935500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 457      |\n",
      "|    time_elapsed    | 35772    |\n",
      "|    total_timesteps | 935936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=936000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 936000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048535079 |\n",
      "|    clip_fraction        | 0.0061        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0731       |\n",
      "|    explained_variance   | 0.443         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0913        |\n",
      "|    n_updates            | 4570          |\n",
      "|    policy_gradient_loss | -0.000471     |\n",
      "|    value_loss           | 0.129         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=936500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 936500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=937000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 937000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=937500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 937500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 458      |\n",
      "|    time_elapsed    | 35850    |\n",
      "|    total_timesteps | 937984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=938000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 938000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024927696 |\n",
      "|    clip_fraction        | 0.00259       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0765       |\n",
      "|    explained_variance   | 0.448         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0775        |\n",
      "|    n_updates            | 4580          |\n",
      "|    policy_gradient_loss | -2.33e-05     |\n",
      "|    value_loss           | 0.122         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=938500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 938500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=939000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 939000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=939500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 939500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 940000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 459      |\n",
      "|    time_elapsed    | 35934    |\n",
      "|    total_timesteps | 940032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 940500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028562674 |\n",
      "|    clip_fraction        | 0.00576       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0926       |\n",
      "|    explained_variance   | 0.449         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0232        |\n",
      "|    n_updates            | 4590          |\n",
      "|    policy_gradient_loss | -6.47e-05     |\n",
      "|    value_loss           | 0.126         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=941000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 941000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=941500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 941500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=942000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 942000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 460      |\n",
      "|    time_elapsed    | 36013    |\n",
      "|    total_timesteps | 942080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=942500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 942500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049111265 |\n",
      "|    clip_fraction        | 0.00557       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0749       |\n",
      "|    explained_variance   | 0.445         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0885        |\n",
      "|    n_updates            | 4600          |\n",
      "|    policy_gradient_loss | -0.000616     |\n",
      "|    value_loss           | 0.124         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=943000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 943000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=943500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 943500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=944000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 944000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 461      |\n",
      "|    time_elapsed    | 36091    |\n",
      "|    total_timesteps | 944128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=944500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 944500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00086210715 |\n",
      "|    clip_fraction        | 0.00884       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0806       |\n",
      "|    explained_variance   | 0.445         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.103         |\n",
      "|    n_updates            | 4610          |\n",
      "|    policy_gradient_loss | -0.00025      |\n",
      "|    value_loss           | 0.129         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=945000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 945000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=945500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 945500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=946000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 946000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 462      |\n",
      "|    time_elapsed    | 36170    |\n",
      "|    total_timesteps | 946176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=946500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 946500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002204669 |\n",
      "|    clip_fraction        | 0.0313      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.125      |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0667      |\n",
      "|    n_updates            | 4620        |\n",
      "|    policy_gradient_loss | -0.000943   |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=947000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 947000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 947500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=948000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 948000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 463      |\n",
      "|    time_elapsed    | 36249    |\n",
      "|    total_timesteps | 948224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=948500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 948500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015132269 |\n",
      "|    clip_fraction        | 0.0178       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0915      |\n",
      "|    explained_variance   | 0.449        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.082        |\n",
      "|    n_updates            | 4630         |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    value_loss           | 0.13         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=949000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 949000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=949500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 949500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 950000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 464      |\n",
      "|    time_elapsed    | 36327    |\n",
      "|    total_timesteps | 950272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 950500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022501219 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.234      |\n",
      "|    explained_variance   | 0.267       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0542      |\n",
      "|    n_updates            | 4640        |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=951000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 951000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=951500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 951500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=952000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 952000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 465      |\n",
      "|    time_elapsed    | 36406    |\n",
      "|    total_timesteps | 952320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=952500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 952500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012516201 |\n",
      "|    clip_fraction        | 0.0302      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.106      |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0827      |\n",
      "|    n_updates            | 4650        |\n",
      "|    policy_gradient_loss | -0.00304    |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=953000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 953000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=953500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 953500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=954000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 954000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 466      |\n",
      "|    time_elapsed    | 36484    |\n",
      "|    total_timesteps | 954368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=954500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 954500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053244773 |\n",
      "|    clip_fraction        | 0.0277       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0997      |\n",
      "|    explained_variance   | 0.465        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0757       |\n",
      "|    n_updates            | 4660         |\n",
      "|    policy_gradient_loss | -0.00192     |\n",
      "|    value_loss           | 0.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=955000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 955000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=955500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 955500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=956000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 956000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 467      |\n",
      "|    time_elapsed    | 36563    |\n",
      "|    total_timesteps | 956416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=956500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 956500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040198285 |\n",
      "|    clip_fraction        | 0.0293       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.11        |\n",
      "|    explained_variance   | 0.454        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.078        |\n",
      "|    n_updates            | 4670         |\n",
      "|    policy_gradient_loss | -0.00258     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=957000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 957000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=957500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 957500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=958000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 958000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 468      |\n",
      "|    time_elapsed    | 36641    |\n",
      "|    total_timesteps | 958464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=958500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 958500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042505795 |\n",
      "|    clip_fraction        | 0.0199       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0825      |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0185       |\n",
      "|    n_updates            | 4680         |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=959000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 959000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=959500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 959500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 469      |\n",
      "|    time_elapsed    | 36726    |\n",
      "|    total_timesteps | 960512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=961000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 961000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005061127 |\n",
      "|    clip_fraction        | 0.00708      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0714      |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0561       |\n",
      "|    n_updates            | 4690         |\n",
      "|    policy_gradient_loss | -0.000449    |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=961500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 961500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=962000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 962000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=962500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 962500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 470      |\n",
      "|    time_elapsed    | 36805    |\n",
      "|    total_timesteps | 962560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=963000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 963000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035532797 |\n",
      "|    clip_fraction        | 0.00415       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0582       |\n",
      "|    explained_variance   | 0.446         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0778        |\n",
      "|    n_updates            | 4700          |\n",
      "|    policy_gradient_loss | -7.56e-05     |\n",
      "|    value_loss           | 0.124         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=963500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 963500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=964000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 964000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=964500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 964500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 471      |\n",
      "|    time_elapsed    | 36883    |\n",
      "|    total_timesteps | 964608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=965000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 965000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004781982 |\n",
      "|    clip_fraction        | 0.0218      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.125      |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.091       |\n",
      "|    n_updates            | 4710        |\n",
      "|    policy_gradient_loss | -0.00153    |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=965500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 965500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=966000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 966000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=966500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 966500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 472      |\n",
      "|    time_elapsed    | 36962    |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=967000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 967000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006313791 |\n",
      "|    clip_fraction        | 0.00635      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0755      |\n",
      "|    explained_variance   | 0.443        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0597       |\n",
      "|    n_updates            | 4720         |\n",
      "|    policy_gradient_loss | -0.000252    |\n",
      "|    value_loss           | 0.125        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=967500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 967500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=968000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 968000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=968500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 968500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 473      |\n",
      "|    time_elapsed    | 37040    |\n",
      "|    total_timesteps | 968704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=969000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 969000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017167443 |\n",
      "|    clip_fraction        | 0.00796      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0637      |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0657       |\n",
      "|    n_updates            | 4730         |\n",
      "|    policy_gradient_loss | -0.000922    |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=969500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 969500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 474      |\n",
      "|    time_elapsed    | 37119    |\n",
      "|    total_timesteps | 970752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=971000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 971000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033736702 |\n",
      "|    clip_fraction        | 0.0267       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.121       |\n",
      "|    explained_variance   | 0.433        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0718       |\n",
      "|    n_updates            | 4740         |\n",
      "|    policy_gradient_loss | -0.0022      |\n",
      "|    value_loss           | 0.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=971500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 971500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=972000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 972000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=972500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 972500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 475      |\n",
      "|    time_elapsed    | 37198    |\n",
      "|    total_timesteps | 972800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=973000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 973000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008264415 |\n",
      "|    clip_fraction        | 0.0434      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.204      |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0306      |\n",
      "|    n_updates            | 4750        |\n",
      "|    policy_gradient_loss | -0.00453    |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=973500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 973500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=974000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 974000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=974500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 974500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 476      |\n",
      "|    time_elapsed    | 37276    |\n",
      "|    total_timesteps | 974848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=975000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 975000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002314286 |\n",
      "|    clip_fraction        | 0.0317      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.149      |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0693      |\n",
      "|    n_updates            | 4760        |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=975500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 975500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=976000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 976000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=976500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 976500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 477      |\n",
      "|    time_elapsed    | 37355    |\n",
      "|    total_timesteps | 976896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=977000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 977000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012563723 |\n",
      "|    clip_fraction        | 0.0114       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | 0.453        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0555       |\n",
      "|    n_updates            | 4770         |\n",
      "|    policy_gradient_loss | -0.000407    |\n",
      "|    value_loss           | 0.119        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=977500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 977500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=978000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 978000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=978500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 978500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 478      |\n",
      "|    time_elapsed    | 37433    |\n",
      "|    total_timesteps | 978944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=979000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 979000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004294847 |\n",
      "|    clip_fraction        | 0.0333      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.159      |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0749      |\n",
      "|    n_updates            | 4780        |\n",
      "|    policy_gradient_loss | -0.00241    |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=979500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 979500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980500   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 479      |\n",
      "|    time_elapsed    | 37512    |\n",
      "|    total_timesteps | 980992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=981000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 981000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032090878 |\n",
      "|    clip_fraction        | 0.0371       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.145       |\n",
      "|    explained_variance   | 0.437        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.092        |\n",
      "|    n_updates            | 4790         |\n",
      "|    policy_gradient_loss | -0.00363     |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=981500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 981500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=982000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 982000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=982500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 982500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=983000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 983000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 480      |\n",
      "|    time_elapsed    | 37596    |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=983500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 983500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016262254 |\n",
      "|    clip_fraction        | 0.0114       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.075       |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.063        |\n",
      "|    n_updates            | 4800         |\n",
      "|    policy_gradient_loss | -0.000696    |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=984000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 984000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=984500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 984500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=985000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 985000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 481      |\n",
      "|    time_elapsed    | 37675    |\n",
      "|    total_timesteps | 985088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=985500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 985500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00097322714 |\n",
      "|    clip_fraction        | 0.00952       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0781       |\n",
      "|    explained_variance   | 0.447         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0499        |\n",
      "|    n_updates            | 4810          |\n",
      "|    policy_gradient_loss | -0.000887     |\n",
      "|    value_loss           | 0.123         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=986000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 986000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=986500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 986500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 987000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 482      |\n",
      "|    time_elapsed    | 37753    |\n",
      "|    total_timesteps | 987136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -8            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 987500        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027988877 |\n",
      "|    clip_fraction        | 0.00356       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0668       |\n",
      "|    explained_variance   | 0.448         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0642        |\n",
      "|    n_updates            | 4820          |\n",
      "|    policy_gradient_loss | -6.14e-05     |\n",
      "|    value_loss           | 0.128         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=988000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 988000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=988500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 988500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=989000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 989000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 483      |\n",
      "|    time_elapsed    | 37832    |\n",
      "|    total_timesteps | 989184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=989500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 989500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024524627 |\n",
      "|    clip_fraction        | 0.0184       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.073       |\n",
      "|    explained_variance   | 0.431        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0558       |\n",
      "|    n_updates            | 4830         |\n",
      "|    policy_gradient_loss | -0.00139     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 990000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 990500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=991000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 991000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 484      |\n",
      "|    time_elapsed    | 37911    |\n",
      "|    total_timesteps | 991232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=991500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 991500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027412727 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.316      |\n",
      "|    explained_variance   | 0.354       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00457    |\n",
      "|    n_updates            | 4840        |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=992000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 992000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=992500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 992500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=993000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 993000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 485      |\n",
      "|    time_elapsed    | 37990    |\n",
      "|    total_timesteps | 993280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=993500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 993500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004259308 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0907     |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.034       |\n",
      "|    n_updates            | 4850        |\n",
      "|    policy_gradient_loss | -0.00376    |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=994000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 994000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=994500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 994500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=995000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 995000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 486      |\n",
      "|    time_elapsed    | 38069    |\n",
      "|    total_timesteps | 995328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=995500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 995500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005080296 |\n",
      "|    clip_fraction        | 0.0171      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.055      |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0423      |\n",
      "|    n_updates            | 4860        |\n",
      "|    policy_gradient_loss | -0.0014     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=996000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 996000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=996500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 996500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=997000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 997000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 487      |\n",
      "|    time_elapsed    | 38148    |\n",
      "|    total_timesteps | 997376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=997500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 997500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00416742 |\n",
      "|    clip_fraction        | 0.00469    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0469    |\n",
      "|    explained_variance   | 0.458      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0289     |\n",
      "|    n_updates            | 4870       |\n",
      "|    policy_gradient_loss | -0.000745  |\n",
      "|    value_loss           | 0.12       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=998000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 998000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=998500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 998500   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=999000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 999000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 488      |\n",
      "|    time_elapsed    | 38227    |\n",
      "|    total_timesteps | 999424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=999500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 999500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002670693 |\n",
      "|    clip_fraction        | 0.00244      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0432      |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0494       |\n",
      "|    n_updates            | 4880         |\n",
      "|    policy_gradient_loss | -2.09e-05    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000500, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000500  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1001000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -8       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1001000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 489      |\n",
      "|    time_elapsed    | 38306    |\n",
      "|    total_timesteps | 1001472  |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = SokobanEnvFixated()\n",
    "test_env = SokobanEnvFixated()\n",
    "\n",
    "# Define the model\n",
    "model = PPO(CnnPolicy, \n",
    "            env, \n",
    "            tensorboard_log=\"./tensorboard_logs/\",\n",
    "            gamma = 0.95, # prioritize future actions\n",
    "            ent_coef = 0.01, # prevent premature convergence\n",
    "            seed = 24,\n",
    "            verbose=1)\n",
    "\n",
    "eval_callback = EvalCallback(test_env, \n",
    "                             best_model_save_path='./models/',\n",
    "                             eval_freq=500,\n",
    "                             deterministic=True,\n",
    "                             render=False)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=1000000,\n",
    "            callback=eval_callback,\n",
    "            tb_log_name=\"log_sokoban_ppo_fixated_env\")\n",
    "env.close()\n",
    "\n",
    "# Run TensorBoard\n",
    "# In the terminal, run: tensorboard --logdir=./03_resource/03_DRL/tensorboard_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:                                                                                                     \t Total: 100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1280x960 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"./models/model_sokoban_ppo_fixated_env_1\")\n",
    "\n",
    "# Create test enviroment\n",
    "test_env = SokobanEnvFixated()\n",
    "\n",
    "# Run the model in the test environment\n",
    "obs = test_env.reset()\n",
    "done = False\n",
    "actions = []\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic = True)\n",
    "    actions.append(int(action))  # Get action from model\n",
    "    obs, reward, done, info = test_env.step(int(action))  # Apply action\n",
    "\n",
    "# Print actions\n",
    "actionDict = {0:\"\", 1:\"\", 2:\"\", 3:\"\", 4:\"\"}\n",
    "print(\"Actions:\", \" \".join(actionDict.get(action) for action in actions), \"\\t Total:\", len(actions))\n",
    "\n",
    "render_state(test_env)\n",
    "test_env.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
